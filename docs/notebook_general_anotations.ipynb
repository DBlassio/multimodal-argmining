{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec69aeb6",
   "metadata": {},
   "source": [
    "# Entender si su explicación es causal o meramente correlacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684b330",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6afef4d8",
   "metadata": {},
   "source": [
    "- ¿Son las justificaciones basadas en causas verdaderas o correlaciones espurias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6763b12",
   "metadata": {},
   "source": [
    "Intervenciones (do)\n",
    "Free Energy Principle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda50d9e",
   "metadata": {},
   "source": [
    "ntervenciones (do(·)) — Explicación detallada\n",
    "1️⃣ Idea central\n",
    "\n",
    "El operador do(X=x) formaliza la idea de modificar una variable de manera deliberada y aislada para estudiar su efecto causal sobre otra variable:\n",
    "\n",
    "Observación tradicional: p(Y|X) → “vemos que cuando X ocurre, Y suele ocurrir” (correlación)\n",
    "\n",
    "Intervención: p(Y | do(X=x)) → “si forzamos que X tome un valor, ¿cómo cambia Y?” (causalidad)\n",
    "\n",
    "Analogía simple:\n",
    "\n",
    "Observación: los paraguas aparecen cuando llueve → correlación.\n",
    "\n",
    "Intervención: quitamos el paraguas deliberadamente y vemos si alguien se moja → causalidad.\n",
    "\n",
    "2️⃣ Por qué es necesario\n",
    "\n",
    "Muchos cambios de predicción en LLMs pueden deberse a sensibilidad a representaciones correlacionales, no a causalidad.\n",
    "\n",
    "Al aplicar do(·) (generar contrafactuales controlados), aislamos la variable causal, eliminando confusores.\n",
    "\n",
    "Ejemplo en NLP:\n",
    "\n",
    "Texto: “Eating carrots improves eyesight.”\n",
    "\n",
    "Predicción modelo: True\n",
    "\n",
    "Contrafactual / intervención (do): “Eating carrots [removed] improves eyesight”\n",
    "\n",
    "Observamos si la predicción cambia: si no cambia → modelo probablemente está usando contexto general (correlacional), no causalidad directa.\n",
    "\n",
    "3️⃣ Tipos de intervenciones en tu proyecto multimodal\n",
    "a) Intervenciones visuales\n",
    "\n",
    "Remover un objeto clave (paraguas, pelota, comida)\n",
    "\n",
    "Blur o reemplazo de objetos\n",
    "\n",
    "Cambiar el contexto irrelevante para validar que el modelo no dependa de correlaciones espurias\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Imagen original: persona sosteniendo paraguas bajo la lluvia\n",
    "\n",
    "Contrafactual: quitar paraguas → predicción debería bajar de “Agree”\n",
    "\n",
    "Contrafactual irrelevante: cambiar fondo de césped → predicción no debería cambiar\n",
    "\n",
    "b) Intervenciones textuales\n",
    "\n",
    "Reemplazar palabras clave que implican causalidad (“because”, “due to”, “causes”)\n",
    "\n",
    "Sustituir sujeto o verbo causal por otro neutro\n",
    "\n",
    "Controlar que no cambie la estructura general de la frase para evitar confusores\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Original: “Umbrellas prevent you from getting wet.”\n",
    "\n",
    "Contrafactual: “Umbrellas [removed] prevent you from getting wet.”\n",
    "\n",
    "Contrafactual neutro: cambiar color del paraguas → predicción no debería cambiar\n",
    "\n",
    "c) Intervenciones multimodales\n",
    "\n",
    "Combinar cambios en texto y visual: quitar objeto y eliminar palabra causal\n",
    "\n",
    "Evalúa si el modelo integra ambas fuentes para inferir causalidad\n",
    "\n",
    "4️⃣ Medición del efecto de do(·)\n",
    "\n",
    "Para cada intervención:\n",
    "\n",
    "Calcular la predicción original del modelo: P(Y|X)\n",
    "\n",
    "Aplicar intervención (do(X=x)) → P(Y | do(X=x))\n",
    "\n",
    "Medir cambio en la predicción:\n",
    "\n",
    "Diferencia en probabilidad / confianza\n",
    "\n",
    "Cambios en ranking de clase (si es clasificación)\n",
    "\n",
    "Interpretación:\n",
    "\n",
    "Cambio significativo → evidencia de sensibilidad causal\n",
    "\n",
    "Cambio mínimo → modelo sigue correlaciones, no causalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ad0e0",
   "metadata": {},
   "source": [
    "CRAB: Fine-grained Causal Reasoning Benchmark\n",
    "\n",
    "Evaluación de LLMs en razonamiento causal explícito e implícito\n",
    "\n",
    "COPA / Choice of Plausible Alternatives\n",
    "\n",
    "Dataset clásico de causal reasoning\n",
    "\n",
    "Counterfactual Datasets:\n",
    "\n",
    "Generan pares de ejemplo/contrafactual\n",
    "\n",
    "Evalúan si el modelo cambia su predicción según evidencia causal\n",
    "\n",
    "Papers multimodales con contrafactuales:\n",
    "\n",
    "E.g., “Visual Counterfactuals for Multimodal Causal Reasoning”\n",
    "\n",
    "Usan imágenes + texto para evaluar sensibilidad causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23041ab0",
   "metadata": {},
   "source": [
    "## PEFT\n",
    "\n",
    "1. Fase prototipo (texto unimodal, limited compute):\n",
    "\n",
    "Empieza con **LoRA** o **BitFit** como baseline. LoRA tiene mejor tradeoff rendimiento/eficiencia; BitFit es ultrarrápido para pruebas.\n",
    "Si usas DeBERTa/BERT, LoRA sobre las proyecciones FFN/attention típicamente funciona genial.\n",
    "\n",
    "2. Multi-task / muchos datasets (claims, premises, stance, persuasiveness):\n",
    "\n",
    "Considera **Adapters** (un adapter por subtarea) y después **AdapterFusion** para combinar conocimiento. Facilita experimentar con múltiples tareas.\n",
    "\n",
    "3. Si vas a usar grandes LLMs multimodales (p.ej. ViT+LLM):\n",
    "\n",
    "Usa LoRA para los bloques del LLM y adapters/LoRA en la parte vision (o adapters en el backbone visual) — es un patrón que funciona en BLIP-style models.\n",
    "Para recursos limitados con models grandes, **QLoRA** es la opción práctica.\n",
    "\n",
    "4. Si priorizas velocidad de experimentación y sharing:\n",
    "\n",
    "Prompt/Prefix Tuning te dejan intercambiar comportamientos rápidamente sin tocar el modelo. Útil para generación multimodal con LLMs instructables."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
