{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec69aeb6",
   "metadata": {},
   "source": [
    "1️⃣ Objetivo del experimento\n",
    "\n",
    "Agregar texto extraído de la imagen como input adicional al modelo existente de tweets (DeBERTa) para ver si mejora la clasificación de stance.\n",
    "\n",
    "Tu baseline actual: Tweet → DeBERTa → Label\n",
    "\n",
    "Experimento: (Tweet + OCR text) → DeBERTa → Label\n",
    "\n",
    "La idea es medir cuánto valor aporta el texto incrustado en las imágenes al F1-Score, comparado con el baseline.\n",
    "\n",
    "2️⃣ Dónde encaja dentro del repo\n",
    "\n",
    "Sí, lo clasificaría como un experimento de multimodal “ligero”, que puedes organizar así:\n",
    "\n",
    "Carpeta/Notebook experimental: experiments/cv_text_ocr/\n",
    "\n",
    "Incluye:\n",
    "\n",
    "Código de extracción de texto (EasyOCR)\n",
    "\n",
    "Pipeline de preprocesamiento (tweet + OCR)\n",
    "\n",
    "Entrenamiento y evaluación usando tu modelo DeBERTa actual\n",
    "\n",
    "Métricas de interés: F1-Score, Precision, Recall\n",
    "\n",
    "Conceptualmente no estás haciendo un modelo “puro” de Computer Vision, porque no estás entrenando embeddings visuales ni redes CNN/ViT todavía. Más bien, estás haciendo un experimento de texto multimodal, donde el “input de la imagen” se transforma en texto.\n",
    "\n",
    "3️⃣ Cómo plantear el pipeline\n",
    "\n",
    "Para cada ejemplo:\n",
    "\n",
    "Tweet → texto\n",
    "\n",
    "Imagen → OCR → texto\n",
    "\n",
    "Combinar los dos textos de manera sencilla (concatenación con un separador claro, ej. [IMG_TEXT])\n",
    "\n",
    "Tokenizar y pasar al modelo DeBERTa\n",
    "\n",
    "Clasificación → Stance\n",
    "\n",
    "Comparar métricas con baseline solo tweet\n",
    "\n",
    "4️⃣ Interpretación de resultados\n",
    "\n",
    "Si el F1-Score mejora: el texto dentro de la imagen aporta señal para stance detection.\n",
    "\n",
    "Si no mejora o empeora:\n",
    "\n",
    "OCR puede estar ruidoso\n",
    "\n",
    "Texto dentro de imagen poco relevante para stance\n",
    "\n",
    "Necesitas procesamiento de texto más avanzado (ej. limpieza, summarization)\n",
    "\n",
    "5️⃣ Posibles extensiones para después\n",
    "\n",
    "Hacer dual encoder: tweet → encoder, OCR text → encoder → fusion\n",
    "\n",
    "Evaluar atención cruzada entre tweet y OCR text\n",
    "\n",
    "Comparar con otros modelos CV que lean texto implícito (SigLIP, BLIP-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
