{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot for the Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stance</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>persuasiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148501065308004357</td>\n",
       "      <td>https://t.co/VQP1FHaWAg</td>\n",
       "      <td>Let's McGyver some Sanity in America!\\n\\nYou a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1103872992537276417</td>\n",
       "      <td>https://t.co/zsyXYSeBkp</td>\n",
       "      <td>A child deserves a chance at life. A child des...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151528583623585794_aug</td>\n",
       "      <td>https://t.co/qSWvDX5MnM</td>\n",
       "      <td>Dear prolifers: girls as young as 10, 11, 12 a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100166844026109953</td>\n",
       "      <td>https://t.co/hxH8tFIHUu</td>\n",
       "      <td>The many States will attempt to amend their co...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021830413550067713</td>\n",
       "      <td>https://t.co/5whvEEtoQR</td>\n",
       "      <td>Every #abortion is wrong, no matter what metho...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                tweet_url  \\\n",
       "0      1148501065308004357  https://t.co/VQP1FHaWAg   \n",
       "1      1103872992537276417  https://t.co/zsyXYSeBkp   \n",
       "2  1151528583623585794_aug  https://t.co/qSWvDX5MnM   \n",
       "3      1100166844026109953  https://t.co/hxH8tFIHUu   \n",
       "4      1021830413550067713  https://t.co/5whvEEtoQR   \n",
       "\n",
       "                                          tweet_text   stance persuasiveness  \\\n",
       "0  Let's McGyver some Sanity in America!\\n\\nYou a...  support             no   \n",
       "1  A child deserves a chance at life. A child des...   oppose             no   \n",
       "2  Dear prolifers: girls as young as 10, 11, 12 a...  support             no   \n",
       "3  The many States will attempt to amend their co...  support             no   \n",
       "4  Every #abortion is wrong, no matter what metho...   oppose            yes   \n",
       "\n",
       "   split  label  persuasiveness_label  \n",
       "0  train      1                     0  \n",
       "1  train      0                     0  \n",
       "2  train      1                     0  \n",
       "3  train      1                     0  \n",
       "4  train      0                     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../data/\"\n",
    "IMG_PATH = \"../data/images\"\n",
    "OUTPUT_DIR = \"../docs/images_report\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train_augmented.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train set stance distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "ax = sns.countplot(data=df_train,x=\"stance\",order=[\"oppose\", \"support\"],\n",
    " palette=\"pastel\",edgecolor=\"black\",linewidth=1.0)\n",
    "ax.set_title(\"Stance Distribution - Train Set\")\n",
    "ax.set_xlabel(\"Stance\")\n",
    "ax.set_ylabel(\"Number of Tweets\")\n",
    "\n",
    "# (Opcional) anotar conteos encima de cada barra\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    ax.annotate(\n",
    "        f\"{int(h)}\",\n",
    "        (p.get_x() + p.get_width() / 2, h),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        xytext=(0, 3),\n",
    "        textcoords=\"offset points\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out_file = os.path.join(OUTPUT_DIR, \"stance_distribution_train_augmented.png\")\n",
    "plt.savefig(out_file, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Model: microsoft/deberta-v3-base\n",
      "BLIP Model: Salesforce/blip-itm-base-coco\n",
      "ALIGN Model: kakaobrain/align-base\n",
      "Common embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Model Names\n",
    "TEXT_MODEL_NAME = \"microsoft/deberta-v3-base\"  # 768 dims\n",
    "BLIP_MODEL_NAME = \"Salesforce/blip-itm-base-coco\"  # 768 dims (vision encoder)\n",
    "ALIGN_MODEL_NAME = \"kakaobrain/align-base\"  # Will project to 768 dims\n",
    "\n",
    "# Text processing\n",
    "MAX_TEXT_LENGTH = 128\n",
    "\n",
    "# Common embedding dimension for fusion\n",
    "COMMON_DIM = 768\n",
    "\n",
    "print(f\"Text Model: {TEXT_MODEL_NAME}\")\n",
    "print(f\"BLIP Model: {BLIP_MODEL_NAME}\")\n",
    "print(f\"ALIGN Model: {ALIGN_MODEL_NAME}\")\n",
    "print(f\"Common embedding dimension: {COMMON_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Epochs: 15\n",
      "Learning rate: 2e-05\n",
      "Weight decay: 0.0001\n",
      "Warmup ratio: 0.1\n",
      "Patience: 5\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 2  \n",
    "\n",
    "# Other\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup ratio: {WARMUP_RATIO}\")\n",
    "print(f\"Patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetBLIP(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + BLIP (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for BLIP vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, img_dir, text_tokenizer, blip_processor, max_length=128):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.blip_processor = blip_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text \n",
    "        text = str(row['tweet_text'])\n",
    "        text_encoding = self.text_tokenizer(text,max_length=self.max_length,padding='max_length',truncation=True,return_tensors='pt')\n",
    "        \n",
    "        # Load Image\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # fallback to grey image\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n",
    "\n",
    "        image_encoding = self.blip_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn_blip(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and processor loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer and processor\n",
    "\n",
    "# Text tokenizer (DeBERTa)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "# BLIP processor\n",
    "blip_processor = BlipProcessor.from_pretrained(BLIP_MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizer and processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 137\n",
      "Dev batches: 13\n",
      "Test batches: 19\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset_blip = MultimodalDatasetBLIP(df_train, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_blip = MultimodalDatasetBLIP(df_dev, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_blip = MultimodalDatasetBLIP(df_test, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_blip = DataLoader(train_dataset_blip,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_blip)\n",
    "\n",
    "dev_loader_blip = DataLoader(dev_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "test_loader_blip = DataLoader(test_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_blip)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_blip)}\")\n",
    "print(f\"Test batches: {len(test_loader_blip)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text input_ids shape: torch.Size([128])\n",
      "Text attention_mask shape: torch.Size([128])\n",
      "Image pixel_values shape: torch.Size([3, 384, 384])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# We test our dataloader\n",
    "sample = train_dataset_blip[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. DeBERTa + BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalBLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and BLIP (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"Salesforce/blip-itm-base-coco\",\n",
    "        num_classes=2,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalBLIP, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: BLIP (only vision part)\n",
    "        blip_full = BlipModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = blip_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = blip_full.config.vision_config.hidden_size  # 768\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers (if needed)\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion layers\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "\n",
    "        # Text embeddings\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision emgeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # BLIP vision encoder returns BaseModelOutputWithPooling\n",
    "        # We can use pooler_output or last_hidden_state[:, 0, :]\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            # Gate decides how much of each modality to use\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion: concat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,398,530\n",
      "  Trainable parameters: 271,398,530\n",
      "\n",
      "Fusion: mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 270,218,114\n",
      "  Trainable parameters: 270,218,114\n",
      "\n",
      "Fusion: weighted_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 270,218,116\n",
      "  Trainable parameters: 270,218,116\n",
      "\n",
      "Fusion: proj_concat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,399,298\n",
      "  Trainable parameters: 271,399,298\n",
      "\n",
      "Fusion: gated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,398,530\n",
      "  Trainable parameters: 271,398,530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all fusion types\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(train_loader_blip))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    num_epochs=15,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    patience=5,\n",
    "    device=DEVICE,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multimodal model with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        best_model: Best model based on dev F1-score\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'dev_loss': [],\n",
    "        'dev_f1': [],\n",
    "        'dev_acc': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dev_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask, pixel_values)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                dev_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_dev_loss = dev_loss / len(dev_loader)\n",
    "        \n",
    "        # Metrics (F1-score binary with pos_label=1)\n",
    "        dev_acc = accuracy_score(all_labels, all_preds)\n",
    "        _, _, dev_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "        \n",
    "        history['dev_loss'].append(avg_dev_loss)\n",
    "        history['dev_f1'].append(dev_f1)\n",
    "        history['dev_acc'].append(dev_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Dev Loss: {avg_dev_loss:.4f}\")\n",
    "        print(f\"  Dev Accuracy: {dev_acc:.4f}\")\n",
    "        print(f\"  Dev F1 (binary, pos=1): {dev_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"    New best F1: {best_f1:.4f}\")\n",
    "            \n",
    "            if save_path:\n",
    "                torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"Training completed. Best Dev F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_model(model, test_loader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with accuracy, F1, precision, recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "        metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"f1\": f1_score(all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"precision\": precision_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"recall\": recall_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"report\": classification_report(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTEST RESULTS\")\n",
    "    print(f\"  Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-score : {metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall   : {metrics['recall']:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(cm)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results_blip = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + BLIP\")\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*80 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_blip_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_blip,\n",
    "        dev_loader=dev_loader_blip,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_blip)\n",
    "    \n",
    "    # Store results\n",
    "    results_blip.append({\n",
    "        'model': 'DeBERTa + BLIP',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results_blip = pd.DataFrame(results_blip)\n",
    "df_results_blip = df_results_blip.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + BLIP\")\n",
    "print(df_results_blip.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_blip.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_blip.csv\"), index=False)\n",
    "print(f\"\\n Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_blip.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeBERTa + ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetALIGN(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + ALIGN (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for ALIGN vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, img_dir, text_tokenizer, align_processor, max_length=128):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.align_processor = align_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text\n",
    "        text = str(row['tweet_text'])\n",
    "        text_encoding = self.text_tokenizer(text,max_length=self.max_length,padding='max_length',truncation=True,return_tensors='pt')\n",
    "\n",
    "    \n",
    "        # Load Image - ALIGN PROCESSOR\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (289, 289), color=(128, 128, 128))\n",
    "        \n",
    "        image_encoding = self.align_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for ALIGN DataLoader (same as BLIP)\n",
    "def collate_fn_align(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ALIGN processor loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ALIGN processor\n",
    "align_processor = AlignProcessor.from_pretrained(ALIGN_MODEL_NAME)\n",
    "\n",
    "print(\" ALIGN processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 137\n",
      "Dev batches: 13\n",
      "Test batches: 19\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset_align = MultimodalDatasetALIGN(df_train, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_align = MultimodalDatasetALIGN(df_dev, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_align = MultimodalDatasetALIGN(df_test, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_align = DataLoader(train_dataset_align,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_align)\n",
    "dev_loader_align = DataLoader(dev_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "test_loader_align = DataLoader(test_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_align)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_align)}\")\n",
    "print(f\"Test batches: {len(test_loader_align)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text input_ids shape: torch.Size([128])\n",
      "Text attention_mask shape: torch.Size([128])\n",
      "Image pixel_values shape: torch.Size([3, 289, 289])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# We test our dataloader\n",
    "sample = train_dataset_align[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalALIGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and ALIGN (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"kakaobrain/align-base\",\n",
    "        num_classes=3,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalALIGN, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: ALIGN (only vision part)\n",
    "        align_full = AlignModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = align_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = align_full.config.vision_config.hidden_dim #640\n",
    "        \n",
    "        print(f\"ALIGN vision dimension: {self.vision_dim}\")\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        # ALIGN vision encoder output needs projection\n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion-specific layers (same as BLIP model)\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        \n",
    "        # Text Embeddings \n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision Embeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # ALIGN vision encoder returns BaseModelOutputWithPooling\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion: concat\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,943,762\n",
      "  Trainable parameters: 247,943,762\n",
      "\n",
      "Fusion: mean\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 246,763,346\n",
      "  Trainable parameters: 246,763,346\n",
      "\n",
      "Fusion: weighted_mean\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 246,763,348\n",
      "  Trainable parameters: 246,763,348\n",
      "\n",
      "Fusion: proj_concat\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,944,530\n",
      "  Trainable parameters: 247,944,530\n",
      "\n",
      "Fusion: gated\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,943,762\n",
      "  Trainable parameters: 247,943,762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sanity check for ALIGN model and fusion strategies\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    batch = next(iter(train_loader_align))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING ALL FUSION STRATEGIES: DeBERTa + ALIGN\n",
      "======================================================================\n",
      "Fusion strategies: ['concat', 'mean', 'weighted_mean', 'proj_concat', 'gated']\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: CONCAT\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb4e6ba051b455e92ceeb5df038ebc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6323\n",
      "  Dev Loss: 0.4504\n",
      "  Dev Accuracy: 0.8200\n",
      "  Dev F1 (binary, pos=1): 0.6897\n",
      "    New best F1: 0.6897\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c30727e2dd4b1480c2f74f1e3f005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3573\n",
      "  Dev Loss: 0.2900\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8497\n",
      "    New best F1: 0.8497\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c9153d7e8b4a2db60372f395de6074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2304\n",
      "  Dev Loss: 0.2778\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8551\n",
      "    New best F1: 0.8551\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d144aabc12e47eda31d2809ea1f38c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1318\n",
      "  Dev Loss: 0.3991\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8296\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ce9c307604afd8da0beebb5419b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0622\n",
      "  Dev Loss: 0.4114\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6dd3a82c2b46c994d028873a597620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0206\n",
      "  Dev Loss: 0.7859\n",
      "  Dev Accuracy: 0.8700\n",
      "  Dev F1 (binary, pos=1): 0.7937\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642e579ed2954adca10e769410873b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0102\n",
      "  Dev Loss: 0.5532\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8671\n",
      "    New best F1: 0.8671\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fe4b5016dd4ef791f424dc7a3d06e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0133\n",
      "  Dev Loss: 0.6751\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8296\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e4a46bf32e48e0be1ec46ccaec9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0042\n",
      "  Dev Loss: 0.7512\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05d6929d1fc4c8e8c33e3805369eb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0066\n",
      "  Dev Loss: 0.6753\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8696\n",
      "    New best F1: 0.8696\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6733b7059aab43f299365ee89fdc3473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0019\n",
      "  Dev Loss: 0.7492\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8593\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91783b1643984618920e3aee5d0e3d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "  Train Loss: 0.0065\n",
      "  Dev Loss: 0.7967\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413e0124c5114ba99ea0abe09c44881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "  Train Loss: 0.0048\n",
      "  Dev Loss: 0.7805\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8700038bf54e078539007b43a8cbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:\n",
      "  Train Loss: 0.0007\n",
      "  Dev Loss: 0.7136\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e2898e1b74533b5a7430c3af710f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:\n",
      "  Train Loss: 0.0006\n",
      "  Dev Loss: 0.7016\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8633\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 15 epochs\n",
      "Training completed. Best Dev F1: 0.8696\n",
      "\n",
      "======================================================================\n",
      "EVALUATING CONCAT ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c475d88ef04625b529d2794cde33ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8400\n",
      "  F1-score : 0.8195\n",
      "  Precision: 0.7365\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[143  39]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: MEAN\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b9943a98df4409b2d86fb3ccb10de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6230\n",
      "  Dev Loss: 0.4651\n",
      "  Dev Accuracy: 0.8700\n",
      "  Dev F1 (binary, pos=1): 0.8000\n",
      "    New best F1: 0.8000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21afb6c6f17493f82e8d43cea23286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3372\n",
      "  Dev Loss: 0.3076\n",
      "  Dev Accuracy: 0.8800\n",
      "  Dev F1 (binary, pos=1): 0.8462\n",
      "    New best F1: 0.8462\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa640ce999a42c78d1be50cf0e5d978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.1875\n",
      "  Dev Loss: 0.2434\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8824\n",
      "    New best F1: 0.8824\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947b3d291384454088ae145e7b42a40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1013\n",
      "  Dev Loss: 0.3748\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8615\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dcb81d6c0b421a8e16d4d1fdbb4f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0635\n",
      "  Dev Loss: 0.3809\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7b17a1ba149b49b568b155973202d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0306\n",
      "  Dev Loss: 0.4160\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "    New best F1: 0.8966\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ecd8a4d684852a37ff5755482dbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0151\n",
      "  Dev Loss: 0.4943\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8652\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594ca698b79f4d1787621f628a098051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0198\n",
      "  Dev Loss: 0.5397\n",
      "  Dev Accuracy: 0.9150\n",
      "  Dev F1 (binary, pos=1): 0.8741\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629522ccaf58404588a1b06bcb7f0c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0128\n",
      "  Dev Loss: 0.6081\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df3eb9b0dd0469ebb9bcfb379ab91cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0119\n",
      "  Dev Loss: 0.5884\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8489\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc970720204b4b1388520a3ba7125327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0103\n",
      "  Dev Loss: 0.6292\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 11 epochs\n",
      "Training completed. Best Dev F1: 0.8966\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MEAN ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9ffb4018b9426ea72d6c80dc1d12d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8567\n",
      "  F1-score : 0.8340\n",
      "  Precision: 0.7660\n",
      "  Recall   : 0.9153\n",
      "\n",
      "Confusion Matrix:\n",
      "[[149  33]\n",
      " [ 10 108]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: WEIGHTED_MEAN\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becafd40baee48f1aa92740dc7c615d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6223\n",
      "  Dev Loss: 0.4865\n",
      "  Dev Accuracy: 0.8050\n",
      "  Dev F1 (binary, pos=1): 0.6355\n",
      "    New best F1: 0.6355\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbc0cc29a0c4d338ac60e8dd0b9dfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3470\n",
      "  Dev Loss: 0.2569\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8594\n",
      "    New best F1: 0.8594\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac1d560a0ed42678a178a8c2ff2b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2172\n",
      "  Dev Loss: 0.2955\n",
      "  Dev Accuracy: 0.8750\n",
      "  Dev F1 (binary, pos=1): 0.8322\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cb8a571f7d47f99dc40432a2627d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1269\n",
      "  Dev Loss: 0.3729\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8462\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb9229f3da141608c8e892bbfdfcad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0815\n",
      "  Dev Loss: 0.4067\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8529\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285bd66b7054714ba5b0a8397c55157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0386\n",
      "  Dev Loss: 0.3855\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8696\n",
      "    New best F1: 0.8696\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2fa7e6bbff4acaa634d1bfee5f5236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0257\n",
      "  Dev Loss: 0.5063\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8550\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262a0016e5aa474b8adf888332a5b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0142\n",
      "  Dev Loss: 0.5189\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8571\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692496b80674832991d2e1c8e7fcd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0136\n",
      "  Dev Loss: 0.6035\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8485\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67368fc8802d4b6ebc815c378605bc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0060\n",
      "  Dev Loss: 0.5532\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbe9a5f6c9a42a89dd42f42a9bf77ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0052\n",
      "  Dev Loss: 0.5578\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 11 epochs\n",
      "Training completed. Best Dev F1: 0.8696\n",
      "\n",
      "======================================================================\n",
      "EVALUATING WEIGHTED_MEAN ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497a2e8e8da2449891252a293701da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8433\n",
      "  F1-score : 0.8240\n",
      "  Precision: 0.7383\n",
      "  Recall   : 0.9322\n",
      "\n",
      "Confusion Matrix:\n",
      "[[143  39]\n",
      " [  8 110]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: PROJ_CONCAT\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5d008919f7494590430df8ac18edab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6641\n",
      "  Dev Loss: 0.5024\n",
      "  Dev Accuracy: 0.8250\n",
      "  Dev F1 (binary, pos=1): 0.7742\n",
      "    New best F1: 0.7742\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aa9000c537412697eadadf0defc4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3096\n",
      "  Dev Loss: 0.3933\n",
      "  Dev Accuracy: 0.8400\n",
      "  Dev F1 (binary, pos=1): 0.7193\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b988380e9443bca67b6d873579ba75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.1624\n",
      "  Dev Loss: 0.3749\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8707\n",
      "    New best F1: 0.8707\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fea40a30ee484da82b61ea5a396b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.0903\n",
      "  Dev Loss: 0.3636\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8788\n",
      "    New best F1: 0.8788\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adea2680b7244f3889a104f7ec488c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0377\n",
      "  Dev Loss: 0.5744\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8507\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcc00f63d4a4ee08da3635a5a4784ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0169\n",
      "  Dev Loss: 0.6069\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3edd61ebc64adcbfce73128eb3bef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0080\n",
      "  Dev Loss: 0.8498\n",
      "  Dev Accuracy: 0.8750\n",
      "  Dev F1 (binary, pos=1): 0.8031\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6795e24400254af09c873567bfb7531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0106\n",
      "  Dev Loss: 0.7178\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8444\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b627a3f92f465497408f2921883bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0032\n",
      "  Dev Loss: 0.7283\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8593\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 9 epochs\n",
      "Training completed. Best Dev F1: 0.8788\n",
      "\n",
      "======================================================================\n",
      "EVALUATING PROJ_CONCAT ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fdb97a4a8b4c2e87dc25c99a31b6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8733\n",
      "  F1-score : 0.8516\n",
      "  Precision: 0.7899\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[153  29]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: GATED\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3693527b1740de829c409c63ac5c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6553\n",
      "  Dev Loss: 0.5548\n",
      "  Dev Accuracy: 0.7400\n",
      "  Dev F1 (binary, pos=1): 0.5185\n",
      "    New best F1: 0.5185\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a111187c6742f28d56bc3e3e85b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3093\n",
      "  Dev Loss: 0.3310\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8372\n",
      "    New best F1: 0.8372\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0f0c67e3e043d1b9d1d1784ff3814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2021\n",
      "  Dev Loss: 0.2287\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8857\n",
      "    New best F1: 0.8857\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbad452f53824976ab9e5faf5427eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1225\n",
      "  Dev Loss: 0.3263\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8633\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b6c52cee14f609b4a382e6dc4b587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0691\n",
      "  Dev Loss: 0.4367\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c440719a124f482ba53e7b8edbf01e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0431\n",
      "  Dev Loss: 0.4549\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24c9ba9fb97457781cb669cf3e3e5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0306\n",
      "  Dev Loss: 0.4843\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8750\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587a569fbc4f48f3bb3f1a95536c697b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0095\n",
      "  Dev Loss: 0.5244\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "    New best F1: 0.8966\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ecc1299e4f408fabe97196603db5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0062\n",
      "  Dev Loss: 0.5433\n",
      "  Dev Accuracy: 0.9150\n",
      "  Dev F1 (binary, pos=1): 0.8794\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2555e3416927404984742c2cb7c2674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0036\n",
      "  Dev Loss: 0.5491\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8873\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75faf588494c2198aad89558207a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0099\n",
      "  Dev Loss: 0.6125\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df684d2e364021a66c352dd867eca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "  Train Loss: 0.0113\n",
      "  Dev Loss: 0.5333\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52347b7b3d8845b389ab4eb91640f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "  Train Loss: 0.0078\n",
      "  Dev Loss: 0.5700\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 13 epochs\n",
      "Training completed. Best Dev F1: 0.8966\n",
      "\n",
      "======================================================================\n",
      "EVALUATING GATED ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a60b642e7f408bb2534960cd0e893e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8533\n",
      "  F1-score : 0.8321\n",
      "  Precision: 0.7569\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[147  35]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "results_align = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + ALIGN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*70 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_align_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_align,\n",
    "        dev_loader=dev_loader_align,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_align)\n",
    "    \n",
    "    # Store results\n",
    "    results_align.append({\n",
    "        'model': 'DeBERTa + ALIGN',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS SUMMARY: DeBERTa + ALIGN\n",
      "         model        fusion  accuracy       f1  precision   recall  best_dev_f1\n",
      "DeBERTa + BLIP   proj_concat  0.873333 0.851562   0.789855 0.923729     0.878788\n",
      "DeBERTa + BLIP          mean  0.856667 0.833977   0.765957 0.915254     0.896552\n",
      "DeBERTa + BLIP         gated  0.853333 0.832061   0.756944 0.923729     0.896552\n",
      "DeBERTa + BLIP weighted_mean  0.843333 0.823970   0.738255 0.932203     0.869565\n",
      "DeBERTa + BLIP        concat  0.840000 0.819549   0.736486 0.923729     0.869565\n",
      "\n",
      "  Results saved to: ../../results/multimodal/blip_align/results_deberta_align.csv\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "df_results_align = pd.DataFrame(results_align)\n",
    "df_results_align = df_results_align.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + ALIGN\")\n",
    "print(df_results_align.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_align.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_align.csv\"), index=False)\n",
    "print(f\"\\n  Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_align.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparation: BLIP vs ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "df_all_results = pd.concat([df_results_blip, df_results_align], ignore_index=True)\n",
    "\n",
    "# Sort by F1-binary\n",
    "df_all_results = df_all_results.sort_values('f1', ascending=False)\n",
    "\n",
    "\n",
    "print(\"GLOBAL RESULTS: ALL MODELS AND FUSION STRATEGIES\")\n",
    "print(df_all_results.to_string(index=False))\n",
    "\n",
    "# Best overall model\n",
    "best_overall = df_all_results.iloc[0]\n",
    "print(f\"\\n BEST OVERALL MODEL \")\n",
    "print(f\"   Model: {best_overall['model']}\")\n",
    "print(f\"   Fusion: {best_overall['fusion'].upper()}\")\n",
    "print(f\"   Test F1-Binary: {best_overall['f1']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "print(f\"   Dev F1 (best): {best_overall['best_dev_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Conclusiones y Recomendaciones\n",
    "\n",
    "**Anlisis:**\n",
    "\n",
    "1. **Mejor Modelo Global:** El modelo con mejor F1-Score binario es el indicado arriba\n",
    "\n",
    "2. **BLIP vs ALIGN:** Comparacin del rendimiento promedio de cada vision encoder\n",
    "\n",
    "3. **Mejor Estrategia de Fusin:** La estrategia que consistentemente da mejores resultados\n",
    "\n",
    "4. **Recomendaciones:**\n",
    "   - Para produccin, usar el mejor modelo global\n",
    "   - Considerar trade-off entre rendimiento y complejidad\n",
    "   - Evaluar tiempo de inferencia si es crtico\n",
    "\n",
    "**Siguientes Pasos:**\n",
    "- Analizar casos donde el modelo falla\n",
    "- Explorar data augmentation para imgenes\n",
    "- Considerar ensembles de los mejores modelos\n",
    "- Fine-tuning con learning rates diferenciados por capa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
