{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aaf41",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques\n",
    "\n",
    "This notebook contains Python implementations of 12 common text preprocessing techniques. \n",
    "Each technique is provided as a standalone function that takes a text string as input and returns the processed string. Users can combine functions in any order to create custom preprocessing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e996245",
   "metadata": {},
   "source": [
    "### Descriptions of Preprocessing Techniques\n",
    "\n",
    "- **Lowercasing**: Converts all text to lowercase to ensure uniformity.\n",
    "- **Uppercasing**: Converts all text to uppercase, useful in some tokenization scenarios.\n",
    "- **Removing Punctuation**: Deletes punctuation marks that may not add value to the model.\n",
    "- **Removing Numbers**: Elimintes digits from the text if they are not informative.\n",
    "- **Removing Extra Whitespace**: Collapses multiple spaces into a single space for clean input.\n",
    "- **Removing Stopwords**: Removes common words (like \"the\", \"and\") that usually carry little meaning.\n",
    "- **Stemming**: Reduces words to their root form (e.g., \"running\" â†’ \"run\").\n",
    "- **Lemmatization**: Converts words to their base form using vocabulary knowledge (e.g., \"better\" â†’ \"good\").\n",
    "- **Removing Special Characters**: Removes symbols and non-alphanumeric characters.\n",
    "- **Expanding Contractions**: Converts contractions to their full forms (e.g., \"don't\" â†’ \"do not\").\n",
    "- **Removing HTML Tags**: Strips HTML markup from text, useful for web-scraped data.\n",
    "- **Removing Non-ASCII Characters**: Deletes characters outside standard ASCII encoding, e.g., emojis or accented letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872d7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import contractions\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ed3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of the sentence\n",
    "sentence = \"Don't worry! Visit my website at https://diegoblassio.com <b>now</b> for 100% free resources ðŸ˜Š.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a9e2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"don't worry! visit my website at https://diegoblassio.com <b>now</b> for 100% free resources ðŸ˜Š.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Lowercasing\n",
    "def lowercasing(text=\"\"):\n",
    "    return text.lower()\n",
    "\n",
    "lowercasing(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20ef30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DON'T WORRY! VISIT MY WEBSITE AT HTTPS://DIEGOBLASSIO.COM <B>NOW</B> FOR 100% FREE RESOURCES ðŸ˜Š.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Uppercasing\n",
    "def uppercasing(text=\"\"):\n",
    "    return text.upper()\n",
    "\n",
    "uppercasing(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2425f9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dont worry Visit my website at httpsdiegoblassiocom bnowb for 100 free resources ðŸ˜Š'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Removing punctuation\n",
    "def remove_punctuation(text=\"\"):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "remove_punctuation(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2421970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry! Visit my website at https://diegoblassio.com <b>now</b> for % free resources ðŸ˜Š.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Removing digits/numbers\n",
    "def remove_numbers(text=\"\"):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "remove_numbers(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662ac8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry! Visit my website at https://diegoblassio.com <b>now</b> for 100% free resources ðŸ˜Š.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Removing extra whitespaces\n",
    "def remove_extra_whitespace(text=\"\"):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "remove_extra_whitespace(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc6718e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worry! Visit website https://diegoblassio.com <b>now</b> 100% free resources ðŸ˜Š.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Removing stopwords\n",
    "def remove_stopwords(text=\"\"):\n",
    "    tokens = text.split()\n",
    "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "remove_stopwords(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76251139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"don't worry! visit my websit at https://diegoblassio.com <b>now</b> for 100% free resourc ðŸ˜Š.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Stemming\n",
    "def stemming(text=\"\"):\n",
    "    tokens = text.split()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "stemming(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d4f090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry! Visit my website at https://diegoblassio.com <b>now</b> for 100% free resource ðŸ˜Š.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Lemmatization\n",
    "def lemmatization(text=\"\"):\n",
    "    tokens = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "lemmatization(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd52858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dont worry Visit my website at httpsdiegoblassiocom bnowb for 100 free resources '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Removing special characters (non-alphanumeric)\n",
    "def remove_special_characters(text=\"\"):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "\n",
    "remove_special_characters(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4759f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not worry! Visit my website at https://diegoblassio.com <b>now</b> for 100% free resources ðŸ˜Š.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Expanding contractions (e.g., \"don't\" -> \"do not\")\n",
    "def expand_contractions(text=\"\"):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "expand_contractions(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4ea7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry! Visit my website at https://diegoblassio.com now for 100% free resources ðŸ˜Š.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. Removing HTML tags\n",
    "def remove_html_tags(text=\"\"):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "remove_html_tags(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d438d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry! Visit my website at https://diegoblassio.com <b>now</b> for 100% free resources .\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12. Removing non-ASCII characters\n",
    "def remove_non_ascii(text=\"\"):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "remove_non_ascii(text=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07dc332",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latest_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlatest_dev\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'latest_dev' is not defined"
     ]
    }
   ],
   "source": [
    "latest_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512b630",
   "metadata": {},
   "source": [
    "In our work, we opted not to apply traditional text preprocessing techniques, such as stopword removal, stemming, or lemmatization, before feeding the data into our models. Recent studies have shown that pre-trained transformer models, including BERT and RoBERTa, are robust to raw textual input and are capable of learning contextual representations without extensive preprocessing. Applying aggressive preprocessing can even remove information relevant to the model, potentially reducing performance. Therefore, we maintained the original text structure, relying on the tokenization mechanisms of the pre-trained models to preserve semantic and syntactic information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
