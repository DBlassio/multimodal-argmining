{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a84cff6",
   "metadata": {},
   "source": [
    "### Multimodal Model: Early Fusion (CLIP + DeBERTa)\n",
    "\n",
    "This notebook establishes our baseline multimodal model for stance classification.\n",
    "\n",
    "Architecture:\n",
    "  - Text Branch:  DeBERTa\n",
    "  - Image Branch: CLIP\n",
    "  - Fusion:       Early Fusion (concatenate embeddings)\n",
    "  - Classifier:   MLP\n",
    "\n",
    "Strategy:\n",
    "  - Tested with our data augmentated. (Previous notebooks)\n",
    "  - No gating mechanism (all images used)\n",
    "  - Simple concatenation fusion\n",
    "  - Standard hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276ab224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  42\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    CLIPModel, CLIPProcessor,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# --- Optional: avoid HF tokenizers fork warnings + improve stability\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Determinism (may slightly reduce speed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Stronger determinism (PyTorch >= 1.8)\n",
    "    # Note: some ops may throw if non-deterministic; if that happens, set to False.\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Deterministic algorithms not fully enabled: {e}\")\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Device configuration\n",
    "# -------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Seed:  {SEED}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cf6b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train label distribution:\n",
      "\n",
      " Stance: \n",
      " Oppose: 1095\n",
      " Support: 1095\n",
      "\n",
      "\n",
      "  Persuasiveness \n",
      " No: 1548\n",
      " Yes: 642\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stance</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>persuasiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148501065308004357</td>\n",
       "      <td>https://t.co/VQP1FHaWAg</td>\n",
       "      <td>Let's McGyver some Sanity in America!\\n\\nYou a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1103872992537276417</td>\n",
       "      <td>https://t.co/zsyXYSeBkp</td>\n",
       "      <td>A child deserves a chance at life. A child des...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151528583623585794_aug</td>\n",
       "      <td>https://t.co/qSWvDX5MnM</td>\n",
       "      <td>Dear prolifers: girls as young as 10, 11, 12 a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100166844026109953</td>\n",
       "      <td>https://t.co/hxH8tFIHUu</td>\n",
       "      <td>The many States will attempt to amend their co...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021830413550067713</td>\n",
       "      <td>https://t.co/5whvEEtoQR</td>\n",
       "      <td>Every #abortion is wrong, no matter what metho...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                tweet_url  \\\n",
       "0      1148501065308004357  https://t.co/VQP1FHaWAg   \n",
       "1      1103872992537276417  https://t.co/zsyXYSeBkp   \n",
       "2  1151528583623585794_aug  https://t.co/qSWvDX5MnM   \n",
       "3      1100166844026109953  https://t.co/hxH8tFIHUu   \n",
       "4      1021830413550067713  https://t.co/5whvEEtoQR   \n",
       "\n",
       "                                          tweet_text   stance persuasiveness  \\\n",
       "0  Let's McGyver some Sanity in America!\\n\\nYou a...  support             no   \n",
       "1  A child deserves a chance at life. A child des...   oppose             no   \n",
       "2  Dear prolifers: girls as young as 10, 11, 12 a...  support             no   \n",
       "3  The many States will attempt to amend their co...  support             no   \n",
       "4  Every #abortion is wrong, no matter what metho...   oppose            yes   \n",
       "\n",
       "   split  label  persuasiveness_label  \n",
       "0  train      1                     0  \n",
       "1  train      0                     0  \n",
       "2  train      1                     0  \n",
       "3  train      1                     0  \n",
       "4  train      0                     1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../../data/\"\n",
    "IMG_PATH = \"../../data/images\"\n",
    "OUTPUT_DIR = \"../../results/multimodal/baseline_multimodal/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train_augmented.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "\n",
    "print(f\"\\n Train label distribution:\")\n",
    "print(f\"\\n Stance: \\n Oppose: {(df_train['label']==0).sum()}\\n Support: {(df_train['label']==1).sum()}\")\n",
    "print(f\"\\n\\n  Persuasiveness \\n No: {(df_train['persuasiveness_label']==0).sum()}\\n Yes: {(df_train['persuasiveness_label']==1).sum()}\")\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c180ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "TEXT_MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "VISION_MODEL_NAME = \"openai/clip-vit-base-patch32\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1a98a3-a727-4f06-8501-dcfad8ee334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  TEXT_MODEL_NAME:  microsoft/deberta-v3-base\n",
      "  VISION_MODEL_NAME: openai/clip-vit-base-patch32\n",
      "  BATCH_SIZE: 16\n",
      "  NUM_EPOCHS: 15\n",
      "  LR: 2e-05\n",
      "  WD: 0.0001\n",
      "  WARMUP_RATIO: 0.1\n",
      "  PATIENCE: 5\n",
      "  MAX_TEXT_LENGTH: 105\n",
      "  NUM_WORKERS: 0\n"
     ]
    }
   ],
   "source": [
    " # Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Text preprocessing\n",
    "MAX_TEXT_LENGTH = 105\n",
    "\n",
    "# Other\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(\"Config:\")\n",
    "print(\"  TEXT_MODEL_NAME: \", TEXT_MODEL_NAME)\n",
    "print(\"  VISION_MODEL_NAME:\", VISION_MODEL_NAME)\n",
    "print(\"  BATCH_SIZE:\", BATCH_SIZE)\n",
    "print(\"  NUM_EPOCHS:\", NUM_EPOCHS)\n",
    "print(\"  LR:\", LEARNING_RATE)\n",
    "print(\"  WD:\", WEIGHT_DECAY)\n",
    "print(\"  WARMUP_RATIO:\", WARMUP_RATIO)\n",
    "print(\"  PATIENCE:\", PATIENCE)\n",
    "print(\"  MAX_TEXT_LENGTH:\", MAX_TEXT_LENGTH)\n",
    "print(\"  NUM_WORKERS:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb7101",
   "metadata": {},
   "source": [
    "###  Multimodal Dataset\n",
    "We create a MultimodalDataset that will return:\n",
    "- tokenized text (input_ids, attention_mask)\n",
    "- image (Pixel Values PIL)\n",
    "- label (stance)\n",
    "\n",
    "\n",
    "We will handle corrupted images safely (Grey image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc5973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetCLIP(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for CLIP + DeBERTa multimodal learning.\n",
    "    Returns tokenized text + processed image + label.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, img_dir: str, tokenizer, processor, max_length: int = 128):\n",
    "\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor \n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Class distribution\n",
    "        self.class_counts = self.df['label'].value_counts().to_dict()\n",
    "        self.num_samples = len(self.df)\n",
    "        print(f\"  Dataset created: {self.num_samples} samples\")\n",
    "        print(f\"    - Class 0 (oppose):  {self.class_counts.get(0, 0)}\")\n",
    "        print(f\"    - Class 1 (support): {self.class_counts.get(1, 0)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load Image\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # fallback to grey image\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(0, 0, 0))\n",
    "\n",
    "        # Text\n",
    "        text = str(row['tweet_text'])\n",
    "        encoding = self.tokenizer(text,add_special_tokens=True,\n",
    "                                  max_length=self.max_length,padding='max_length',\n",
    "                                  truncation=True,return_tensors='pt')\n",
    "\n",
    "        # Label\n",
    "        label = row['label']\n",
    "\n",
    "        return {\n",
    "            'pixel_values': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'tweet_id': str(row['tweet_id']),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "    def get_class_weights(self, device):\n",
    "        num_class_0 = self.class_counts.get(0, 0)\n",
    "        num_class_1 = self.class_counts.get(1, 0)\n",
    "        total = self.num_samples\n",
    "        weights = torch.tensor([\n",
    "            total / num_class_0 if num_class_0 > 0 else 1.0,\n",
    "            total / num_class_1 if num_class_1 > 0 else 1.0], dtype=torch.float32).to(device)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39606909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn_clip(batch, processor):\n",
    "    images = [item['pixel_values'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "\n",
    "    # Process images\n",
    "    processed = processor(images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    return {\n",
    "        'pixel_values': processed['pixel_values'],  # tensor [B, 3, 224, 224]\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f148dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and Processor loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and Processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "clip_processor = CLIPProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "print(\"Tokenizer and Processor loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be66b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset created: 2190 samples\n",
      "    - Class 0 (oppose):  1095\n",
      "    - Class 1 (support): 1095\n",
      "  Dataset created: 200 samples\n",
      "    - Class 0 (oppose):  127\n",
      "    - Class 1 (support): 73\n",
      "  Dataset created: 300 samples\n",
      "    - Class 0 (oppose):  182\n",
      "    - Class 1 (support): 118\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MultimodalDatasetCLIP(df_train, IMG_PATH, tokenizer, clip_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset   = MultimodalDatasetCLIP(df_dev, IMG_PATH, tokenizer, clip_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset  = MultimodalDatasetCLIP(df_test, IMG_PATH, tokenizer, clip_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, collate_fn=lambda batch: collate_fn_clip(batch, clip_processor))\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                        pin_memory=PIN_MEMORY, collate_fn=lambda batch: collate_fn_clip(batch, clip_processor))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                         pin_memory=PIN_MEMORY, collate_fn=lambda batch: collate_fn_clip(batch, clip_processor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe4a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels'])\n",
      "pixel_values: torch.Size([16, 3, 224, 224])\n",
      "input_ids: torch.Size([16, 105])\n",
      "attention_mask: torch.Size([16, 105])\n",
      "labels: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"pixel_values:\", batch[\"pixel_values\"].shape)\n",
    "print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "print(\"labels:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69a3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalBaseline(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"openai/clip-vit-base-patch32\",\n",
    "        num_classes=2,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=True,\n",
    "        fusion_type=\"mean\",\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fusion_type = fusion_type\n",
    "\n",
    "\n",
    "        # TEXT ENCODER\n",
    "        print(f\"Loading TEXT encoder: {text_model_name}\")\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_hidden = self.text_encoder.config.hidden_size  # e.g., 768\n",
    "\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "            print(\"Text encoder FROZEN\")\n",
    "\n",
    "\n",
    "        # VISION ENCODER (CLIP)\n",
    "        print(f\"Loading VISION encoder: {vision_model_name}\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(vision_model_name)\n",
    "        self.vision_model = self.clip_model.vision_model\n",
    "        self.clip_hidden = self.clip_model.config.projection_dim  # 512\n",
    "\n",
    "        if freeze_vision:\n",
    "            for p in self.vision_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            # Optionally unfreeze last layer\n",
    "            for name, param in self.vision_model.named_parameters():\n",
    "                if \"encoder.layers.11\" in name:\n",
    "                    param.requires_grad = True\n",
    "            print(\"CLIP encoder partially frozen (last layer trainable)\")\n",
    "\n",
    "        # Projections\n",
    "        \n",
    "        # Project vision embeddings to text_hidden (needed for mean & gated & proj_concat)\n",
    "        self.vision_proj = nn.Linear(self.clip_hidden, self.text_hidden)\n",
    "\n",
    "        # Gated fusion\n",
    "        if fusion_type == \"gated\":\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(self.text_hidden * 2, self.text_hidden),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        # ----------------\n",
    "        # CLASSIFIER\n",
    "        # ----------------\n",
    "        if fusion_type == \"concat\":\n",
    "            fused_dim = self.text_hidden + self.clip_hidden\n",
    "        elif fusion_type == \"mean\":\n",
    "            fused_dim = self.text_hidden\n",
    "        elif fusion_type == \"gated\":\n",
    "            fused_dim = self.text_hidden\n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            fused_dim = self.text_hidden * 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion_type: {fusion_type}\")\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fused_dim, num_classes))\n",
    "\n",
    "        print(f\"MultimodalBaseline initialized | Fusion={fusion_type} | fused_dim={fused_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images=None, mode=\"multimodal\"):\n",
    " \n",
    "        # TEXT EMBEDDINGS\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_emb = text_out.last_hidden_state[:, 0, :]  # CLS token [B, text_hidden]\n",
    "\n",
    "\n",
    "        # VISION EMBEDDINGS\n",
    "        if mode == \"multimodal\" and images is not None:\n",
    "            vision_feat = self.vision_model(pixel_values=images)\n",
    "            cls_embedding = vision_feat.last_hidden_state[:, 0, :]  # CLS token\n",
    "            vision_emb = self.clip_model.visual_projection(cls_embedding)  # [B, clip_hidden]\n",
    "        else:\n",
    "            vision_emb = torch.zeros(text_emb.size(0), self.clip_hidden, device=text_emb.device)\n",
    "\n",
    "       \n",
    "        # EARLY FUSION\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "        elif self.fusion_type == \"mean\":\n",
    "            vision_emb_proj = self.vision_proj(vision_emb)  # [B, text_hidden]\n",
    "            fused = (text_emb + vision_emb_proj) / 2\n",
    "        elif self.fusion_type == \"gated\":\n",
    "            vision_emb_proj = self.vision_proj(vision_emb)  # [B, text_hidden]\n",
    "            gate_input = torch.cat([text_emb, vision_emb_proj], dim=1)  # [B, 2*text_hidden]\n",
    "            gate = self.gate(gate_input)  # [B, text_hidden], sigmoid outputs 0-1\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb_proj  # [B, text_hidden]\n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            fused = torch.cat([text_emb, self.vision_proj(vision_emb)], dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion_type: {self.fusion_type}\")\n",
    "\n",
    "        # CLASSIFIER\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b48d97",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81476ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    num_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    mode=\"multimodal\",\n",
    "    patience=3,\n",
    "    device=DEVICE,\n",
    "    save_path=None\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -------------------------\n",
    "    # CLASS WEIGHTS (IMBALANCE)\n",
    "    # -------------------------\n",
    "    class_weights = train_loader.dataset.get_class_weights(device)\n",
    "    print(\n",
    "        f\"Class weights: \"\n",
    "        f\"oppose={class_weights[0]:.3f}, \"\n",
    "        f\"support={class_weights[1]:.3f}\"\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # -------------------------\n",
    "    # OPTIMIZER (PARAM GROUPS)\n",
    "    # -------------------------\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\n",
    "                \"params\": model.text_encoder.parameters(),\n",
    "                \"lr\": learning_rate\n",
    "            },\n",
    "            {\n",
    "                \"params\": model.vision_model.parameters(),\n",
    "                \"lr\": learning_rate * 0.5\n",
    "            },\n",
    "            {\n",
    "                \"params\": model.classifier.parameters(),\n",
    "                \"lr\": learning_rate * 2\n",
    "            }\n",
    "        ],\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # SCHEDULER\n",
    "    # -------------------------\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    print(f\"Total training steps: {num_training_steps}\")\n",
    "    print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # HISTORY\n",
    "    # -------------------------\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"dev_loss\": [],\n",
    "        \"dev_f1\": [],\n",
    "        \"learning_rates\": []\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # EARLY STOPPING\n",
    "    # -------------------------\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # =========================\n",
    "    # TRAINING LOOP\n",
    "    # =========================\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        # -------- TRAIN --------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            if mode == \"text_only\":\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, images=None, mode=\"text_only\")\n",
    "            else:\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images, mode=\"multimodal\")\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_f1 = f1_score(train_labels,train_preds,average=\"binary\",pos_label=1,zero_division=0)\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()\n",
    "        dev_loss = 0.0\n",
    "        dev_preds, dev_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader, desc=\"Validation\", leave=False):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                images = batch[\"pixel_values\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                if mode == \"text_only\":\n",
    "                    logits = model(input_ids=input_ids, attention_mask=attention_mask, images=None, mode=\"text_only\")\n",
    "                else:\n",
    "                    logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images, mode=\"multimodal\")\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                dev_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                dev_preds.extend(preds.cpu().numpy())\n",
    "                dev_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        dev_loss /= len(dev_loader.dataset)\n",
    "        dev_f1 = f1_score(dev_labels,dev_preds,average=\"binary\",pos_label=1,zero_division=0)\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "        history[\"dev_loss\"].append(dev_loss)\n",
    "        history[\"dev_f1\"].append(dev_f1)\n",
    "        history[\"learning_rates\"].append(current_lr)\n",
    "\n",
    "        print(f\"TRAIN LOSS: {train_loss:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"DEV   LOSS: {dev_loss:.4f} | F1: {dev_f1:.4f}\")\n",
    "        print(f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "        # -------- EARLY STOPPING --------\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"  New best DEV F1: {best_f1:.4f}\")\n",
    "\n",
    "            if save_path is not None:\n",
    "                torch.save(best_model_state, save_path)\n",
    "                print(f\"   Saved best checkpoint: {save_path}\")\n",
    "\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"   No improvement ({epochs_without_improvement}/{patience})\")\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"  Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is None and save_path is not None and os.path.exists(save_path):\n",
    "        print(\"[WARN] best_model_state is None; loading from disk checkpoint.\")\n",
    "        best_model_state = torch.load(save_path, map_location=\"cpu\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"\\nBest DEV F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a99955",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d40764-88ef-4191-b91a-44ae0e25b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_argmax(model, dataloader, mode=\"multimodal\", device=DEVICE, verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", disable=not verbose):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            if mode == \"text_only\":\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, images=None, mode=\"text_only\")\n",
    "            else:\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images, mode=\"multimodal\")\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36497673-150f-4c95-a748-896cfe75d6fe",
   "metadata": {},
   "source": [
    "## Trainning & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a003cb9-644f-4bd4-9fef-01c6e7a72a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING (replication run) — DeBERTa + CLIP | fusion=mean | freeze_vision=True\n",
      "Loading TEXT encoder: microsoft/deberta-v3-base\n",
      "Loading VISION encoder: openai/clip-vit-base-patch32\n",
      "MultimodalBaseline initialized | Fusion=mean | fused_dim=768\n",
      "Class weights: oppose=2.000, support=2.000\n",
      "Total training steps: 2055\n",
      "Warmup steps: 205\n",
      "\n",
      "============================================================\n",
      "Epoch 1/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7176aabe4dea4678ad46e2f2b7b264e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.6008 | F1: 0.5071\n",
      "DEV   LOSS: 0.4879 | F1: 0.6486\n",
      "LR: 1.34e-05\n",
      "  New best DEV F1: 0.6486\n",
      "   Saved best checkpoint: ../../results/multimodal/baseline_multimodal/best_deberta_clip_mean.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 2/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335001f86df745ba8e1b72dcf429aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.2753 | F1: 0.8938\n",
      "DEV   LOSS: 0.2836 | F1: 0.8372\n",
      "LR: 1.93e-05\n",
      "  New best DEV F1: 0.8372\n",
      "   Saved best checkpoint: ../../results/multimodal/baseline_multimodal/best_deberta_clip_mean.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 3/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f20d2bf7a024707a6aa7e50829ce489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.1661 | F1: 0.9444\n",
      "DEV   LOSS: 0.2596 | F1: 0.8741\n",
      "LR: 1.78e-05\n",
      "  New best DEV F1: 0.8741\n",
      "   Saved best checkpoint: ../../results/multimodal/baseline_multimodal/best_deberta_clip_mean.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 4/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c28a19528e04b949ee3b43ca3e907a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0631 | F1: 0.9804\n",
      "DEV   LOSS: 0.4011 | F1: 0.8676\n",
      "LR: 1.63e-05\n",
      "   No improvement (1/5)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c532c15cb424fd3bff998981a4e47d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0200 | F1: 0.9936\n",
      "DEV   LOSS: 0.4407 | F1: 0.8857\n",
      "LR: 1.48e-05\n",
      "  New best DEV F1: 0.8857\n",
      "   Saved best checkpoint: ../../results/multimodal/baseline_multimodal/best_deberta_clip_mean.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 6/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c011d33ef2044624a5a90594ea9bc76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0136 | F1: 0.9973\n",
      "DEV   LOSS: 0.3669 | F1: 0.9167\n",
      "LR: 1.33e-05\n",
      "  New best DEV F1: 0.9167\n",
      "   Saved best checkpoint: ../../results/multimodal/baseline_multimodal/best_deberta_clip_mean.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 7/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad9b2a193dd4389a38e39d9b6757c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0055 | F1: 0.9991\n",
      "DEV   LOSS: 0.4667 | F1: 0.8794\n",
      "LR: 1.18e-05\n",
      "   No improvement (1/5)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc2f380c16740c282bfa938805be119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0060 | F1: 0.9982\n",
      "DEV   LOSS: 0.5885 | F1: 0.8333\n",
      "LR: 1.04e-05\n",
      "   No improvement (2/5)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4fdcda459a401786b746b9b1466d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0012 | F1: 0.9995\n",
      "DEV   LOSS: 0.4901 | F1: 0.8777\n",
      "LR: 8.89e-06\n",
      "   No improvement (3/5)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b38c180108474aa83f99b1cb53b248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0026 | F1: 0.9991\n",
      "DEV   LOSS: 0.5368 | F1: 0.8696\n",
      "LR: 7.41e-06\n",
      "   No improvement (4/5)\n",
      "\n",
      "============================================================\n",
      "Epoch 11/15\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a8a0f994a41de82605413811fd838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.0012 | F1: 0.9991\n",
      "DEV   LOSS: 0.7076 | F1: 0.8031\n",
      "LR: 5.92e-06\n",
      "   No improvement (5/5)\n",
      "  Early stopping triggered.\n",
      "\n",
      "Best DEV F1: 0.9167\n",
      "\n",
      "EVALUATION ON TEST (argmax):\n",
      "F1: 0.8178 | P: 0.7285 | R: 0.9322 | Acc: 0.8367\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING (replication run) — DeBERTa + CLIP | fusion=mean | freeze_vision=True\")\n",
    "\n",
    "fusion = \"mean\"\n",
    "model = MultimodalBaseline(\n",
    "    text_model_name=TEXT_MODEL_NAME,\n",
    "    vision_model_name=VISION_MODEL_NAME,\n",
    "    num_classes=2,\n",
    "    freeze_text=False,\n",
    "    freeze_vision=False,\n",
    "    fusion_type=fusion,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt_path = os.path.join(OUTPUT_DIR, f\"best_deberta_clip_{fusion}.pth\")\n",
    "\n",
    "model, history = train_multimodal_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    mode=\"multimodal\",\n",
    "    patience=PATIENCE,\n",
    "    device=DEVICE,\n",
    "    save_path=ckpt_path\n",
    ")\n",
    "\n",
    "print(\"\\nEVALUATION ON TEST (argmax):\")\n",
    "test_res = evaluate_argmax(model, test_loader, mode=\"multimodal\", device=DEVICE, verbose=False)\n",
    "print(f\"F1: {test_res['f1']:.4f} | P: {test_res['precision']:.4f} | R: {test_res['recall']:.4f} | Acc: {test_res['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09603694",
   "metadata": {},
   "source": [
    "Best Model:\n",
    "\n",
    "- Fusion Mean\n",
    "\n",
    "F1-Score:\n",
    "\n",
    "- 84.29 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
