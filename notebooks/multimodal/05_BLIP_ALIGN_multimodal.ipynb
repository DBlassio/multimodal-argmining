{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Stance Classification: DeBERTa + BLIP/ALIGN\n",
    "\n",
    "Este notebook explora diferentes arquitecturas de **Early Fusion** para clasificación de stance multimodal.\n",
    "\n",
    "## Objetivos:\n",
    "1. **Sección 1:** DeBERTa (texto) + BLIP (imagen) con múltiples estrategias de fusión\n",
    "2. **Sección 2:** DeBERTa (texto) + ALIGN (imagen) con múltiples estrategias de fusión\n",
    "\n",
    "## Estrategias de Early Fusion:\n",
    "- Concatenación simple\n",
    "- Mean/Average\n",
    "- Weighted Average (learnable weights)\n",
    "- Projection + Concatenation\n",
    "- Gated Fusion\n",
    "\n",
    "**Métrica principal:** F1-Score Binario (pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel,BlipProcessor,BlipModel,AlignProcessor,AlignModel,get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../../data/\"\n",
    "IMG_PATH = \"../../data/images\"\n",
    "OUTPUT_DIR = \"../../results/multimodal/baseline_multimodal/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train_augmented.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "\n",
    "print(f\"\\n Train label distribution:\")\n",
    "print(f\"\\n Stance: \\n Oppose: {(df_train['label']==0).sum()}\\n Support: {(df_train['label']==1).sum()}\")\n",
    "print(f\"\\n\\n  Persuasiveness \\n No: {(df_train['persuasiveness_label']==0).sum()}\\n Yes: {(df_train['persuasiveness_label']==1).sum()}\")\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names\n",
    "TEXT_MODEL_NAME = \"microsoft/deberta-v3-base\"  # 768 dims\n",
    "BLIP_MODEL_NAME = \"Salesforce/blip-itm-base-coco\"  # 768 dims (vision encoder)\n",
    "ALIGN_MODEL_NAME = \"kakaobrain/align-base\"  # Will project to 768 dims\n",
    "\n",
    "# Text processing\n",
    "MAX_TEXT_LENGTH = 128\n",
    "\n",
    "# Common embedding dimension for fusion\n",
    "COMMON_DIM = 768\n",
    "\n",
    "print(f\"Text Model: {TEXT_MODEL_NAME}\")\n",
    "print(f\"BLIP Model: {BLIP_MODEL_NAME}\")\n",
    "print(f\"ALIGN Model: {ALIGN_MODEL_NAME}\")\n",
    "print(f\"Common embedding dimension: {COMMON_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 2  \n",
    "\n",
    "# Other\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup ratio: {WARMUP_RATIO}\")\n",
    "print(f\"Patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetBLIP(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + BLIP (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for BLIP vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, df, img_path, text_tokenizer, blip_processor, max_length=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_path = img_path\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.blip_processor = blip_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text \n",
    "        text = str(row['text'])\n",
    "        text_encoding = self.text_tokenizer(text,max_length=self.max_length,padding='max_length',truncation=True,return_tensors='pt')\n",
    "        \n",
    "        # Image\n",
    "        img_file = os.path.join(self.img_path, row['image'])\n",
    "        try:\n",
    "            image = Image.open(img_file).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # fallback to grey image\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n",
    "\n",
    "        image_encoding = self.blip_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn_blip(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer and processor\n",
    "\n",
    "# Text tokenizer (DeBERTa)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "# BLIP processor\n",
    "blip_processor = BlipProcessor.from_pretrained(BLIP_MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizer and processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_blip = MultimodalDatasetBLIP(df_train, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_blip = MultimodalDatasetBLIP(df_dev, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_blip = MultimodalDatasetBLIP(df_test, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_blip = DataLoader(train_dataset_blip,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_blip)\n",
    "\n",
    "dev_loader_blip = DataLoader(dev_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "test_loader_blip = DataLoader(test_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_blip)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_blip)}\")\n",
    "print(f\"Test batches: {len(test_loader_blip)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test our dataloader\n",
    "sample = train_dataset_blip[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. DeBERTa + BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalBLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and BLIP (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"Salesforce/blip-itm-base-coco\",\n",
    "        num_classes=2,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalBLIP, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: BLIP (only vision part)\n",
    "        blip_full = BlipModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = blip_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = blip_full.config.vision_config.hidden_size  # 768\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers (if needed)\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion layers\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "\n",
    "        # Text embeddings\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision emgeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # BLIP vision encoder returns BaseModelOutputWithPooling\n",
    "        # We can use pooler_output or last_hidden_state[:, 0, :]\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            # Gate decides how much of each modality to use\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all fusion types\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(train_loader_blip))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    num_epochs=15,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    patience=5,\n",
    "    device=DEVICE,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multimodal model with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        best_model: Best model based on dev F1-score\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'dev_loss': [],\n",
    "        'dev_f1': [],\n",
    "        'dev_acc': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dev_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask, pixel_values)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                dev_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_dev_loss = dev_loss / len(dev_loader)\n",
    "        \n",
    "        # Metrics (F1-score binary with pos_label=1)\n",
    "        dev_acc = accuracy_score(all_labels, all_preds)\n",
    "        _, _, dev_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "        \n",
    "        history['dev_loss'].append(avg_dev_loss)\n",
    "        history['dev_f1'].append(dev_f1)\n",
    "        history['dev_acc'].append(dev_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Dev Loss: {avg_dev_loss:.4f}\")\n",
    "        print(f\"  Dev Accuracy: {dev_acc:.4f}\")\n",
    "        print(f\"  Dev F1 (binary, pos=1): {dev_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"    New best F1: {best_f1:.4f}\")\n",
    "            \n",
    "            if save_path:\n",
    "                torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"Training completed. Best Dev F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_model(model, test_loader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with accuracy, F1, precision, recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "        metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"f1\": f1_score(all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"precision\": precision_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"recall\": recall_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"report\": classification_report(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTEST RESULTS\")\n",
    "    print(f\"  Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-score : {metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall   : {metrics['recall']:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(cm)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results_blip = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + BLIP\")\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*80 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_blip_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_blip,\n",
    "        dev_loader=dev_loader_blip,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_blip)\n",
    "    \n",
    "    # Store results\n",
    "    results_blip.append({\n",
    "        'model': 'DeBERTa + BLIP',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results_blip = pd.DataFrame(results_blip)\n",
    "df_results_blip = df_results_blip.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + BLIP\")\n",
    "print(df_results_blip.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_blip.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_blip.csv\"), index=False)\n",
    "print(f\"\\n✓ Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_blip.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeBERTa + ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetALIGN(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + ALIGN (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for ALIGN vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, df, img_path, text_tokenizer, align_processor, max_length=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_path = img_path\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.align_processor = align_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text\n",
    "        text = str(row['text'])\n",
    "        text_encoding = self.text_tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Image - ALIGN processor\n",
    "        img_file = os.path.join(self.img_path, row['image'])\n",
    "        try:\n",
    "            image = Image.open(img_file).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (289, 289), color=(128, 128, 128))\n",
    "        \n",
    "        image_encoding = self.align_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for ALIGN DataLoader (same as BLIP)\n",
    "def collate_fn_align(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGN processor\n",
    "align_processor = AlignProcessor.from_pretrained(ALIGN_MODEL_NAME)\n",
    "\n",
    "print(\" ALIGN processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_align = MultimodalDatasetALIGN(df_train, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_align = MultimodalDatasetALIGN(df_dev, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_align = MultimodalDatasetALIGN(df_test, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_align = DataLoader(train_dataset_align,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_align)\n",
    "dev_loader_align = DataLoader(dev_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "test_loader_align = DataLoader(test_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_align)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_align)}\")\n",
    "print(f\"Test batches: {len(test_loader_align)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test our dataloader\n",
    "\n",
    "sample = train_dataset_align[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalALIGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and ALIGN (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"kakaobrain/align-base\",\n",
    "        num_classes=3,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalALIGN, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: ALIGN (only vision part)\n",
    "        align_full = AlignModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = align_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = align_full.config.vision_config.hidden_size  # Typically 640\n",
    "        \n",
    "        print(f\"ALIGN vision dimension: {self.vision_dim}\")\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        # ALIGN vision encoder output needs projection\n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion-specific layers (same as BLIP model)\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        \n",
    "        # Text Embeddings \n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision Embeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # ALIGN vision encoder returns BaseModelOutputWithPooling\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check for ALIGN model and fusion strategies\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    batch = next(iter(train_loader_align))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results_align = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + ALIGN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*70 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_align_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_align,\n",
    "        dev_loader=dev_loader_align,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_align)\n",
    "    \n",
    "    # Store results\n",
    "    results_blip.append({\n",
    "        'model': 'DeBERTa + BLIP',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results_align = pd.DataFrame(results_align)\n",
    "df_results_align = df_results_align.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + ALIGN\")\n",
    "print(df_results_align.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_align.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_align.csv\"), index=False)\n",
    "print(f\"\\n  Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_align.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparation: BLIP vs ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "df_all_results = pd.concat([df_results_blip, df_results_align], ignore_index=True)\n",
    "\n",
    "# Sort by F1-binary\n",
    "df_all_results = df_all_results.sort_values('f1', ascending=False)\n",
    "\n",
    "\n",
    "print(\"GLOBAL RESULTS: ALL MODELS AND FUSION STRATEGIES\")\n",
    "print(df_all_results.to_string(index=False))\n",
    "\n",
    "# Best overall model\n",
    "best_overall = df_all_results.iloc[0]\n",
    "print(f\"\\n BEST OVERALL MODEL \")\n",
    "print(f\"   Model: {best_overall['model']}\")\n",
    "print(f\"   Fusion: {best_overall['fusion'].upper()}\")\n",
    "print(f\"   Test F1-Binary: {best_overall['f1']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "print(f\"   Dev F1 (best): {best_overall['best_dev_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Conclusiones y Recomendaciones\n",
    "\n",
    "**Análisis:**\n",
    "\n",
    "1. **Mejor Modelo Global:** El modelo con mejor F1-Score binario es el indicado arriba\n",
    "\n",
    "2. **BLIP vs ALIGN:** Comparación del rendimiento promedio de cada vision encoder\n",
    "\n",
    "3. **Mejor Estrategia de Fusión:** La estrategia que consistentemente da mejores resultados\n",
    "\n",
    "4. **Recomendaciones:**\n",
    "   - Para producción, usar el mejor modelo global\n",
    "   - Considerar trade-off entre rendimiento y complejidad\n",
    "   - Evaluar tiempo de inferencia si es crítico\n",
    "\n",
    "**Siguientes Pasos:**\n",
    "- Analizar casos donde el modelo falla\n",
    "- Explorar data augmentation para imágenes\n",
    "- Considerar ensembles de los mejores modelos\n",
    "- Fine-tuning con learning rates diferenciados por capa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
