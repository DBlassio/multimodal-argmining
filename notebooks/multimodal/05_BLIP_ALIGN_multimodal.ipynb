{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Stance Classification: DeBERTa + BLIP/ALIGN\r\n",
    "\r\n",
    "This notebook explores different **Early Fusion** architectures for multimodal stance classification.\r\n",
    "\r\n",
    "## Objectives:\r\n",
    "1. **Section 1:** DeBERTa (text) + BLIP (image) with multiple fusion strategies\r\n",
    "2. **Section 2:** DeBERTa (text) + ALIGN (image) with multiple fusion strategies\r\n",
    "\r\n",
    "## Early Fusion Strategies:\r\n",
    "- Simple Concatenation\r\n",
    "- Mean/Average\r\n",
    "- Weighted Average (learnable weights)\r\n",
    "- Projection + Concatenation\r\n",
    "- Gated Fusion\r\n",
    "\r\n",
    "**Main Metric:** F1 - Binary Score (pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel,BlipProcessor,BlipModel,AlignProcessor,AlignModel,get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train label distribution:\n",
      "\n",
      " Stance: \n",
      " Oppose: 1095\n",
      " Support: 1095\n",
      "\n",
      "\n",
      "  Persuasiveness \n",
      " No: 1548\n",
      " Yes: 642\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stance</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>persuasiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148501065308004357</td>\n",
       "      <td>https://t.co/VQP1FHaWAg</td>\n",
       "      <td>Let's McGyver some Sanity in America!\\n\\nYou a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1103872992537276417</td>\n",
       "      <td>https://t.co/zsyXYSeBkp</td>\n",
       "      <td>A child deserves a chance at life. A child des...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151528583623585794_aug</td>\n",
       "      <td>https://t.co/qSWvDX5MnM</td>\n",
       "      <td>Dear prolifers: girls as young as 10, 11, 12 a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100166844026109953</td>\n",
       "      <td>https://t.co/hxH8tFIHUu</td>\n",
       "      <td>The many States will attempt to amend their co...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021830413550067713</td>\n",
       "      <td>https://t.co/5whvEEtoQR</td>\n",
       "      <td>Every #abortion is wrong, no matter what metho...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                tweet_url  \\\n",
       "0      1148501065308004357  https://t.co/VQP1FHaWAg   \n",
       "1      1103872992537276417  https://t.co/zsyXYSeBkp   \n",
       "2  1151528583623585794_aug  https://t.co/qSWvDX5MnM   \n",
       "3      1100166844026109953  https://t.co/hxH8tFIHUu   \n",
       "4      1021830413550067713  https://t.co/5whvEEtoQR   \n",
       "\n",
       "                                          tweet_text   stance persuasiveness  \\\n",
       "0  Let's McGyver some Sanity in America!\\n\\nYou a...  support             no   \n",
       "1  A child deserves a chance at life. A child des...   oppose             no   \n",
       "2  Dear prolifers: girls as young as 10, 11, 12 a...  support             no   \n",
       "3  The many States will attempt to amend their co...  support             no   \n",
       "4  Every #abortion is wrong, no matter what metho...   oppose            yes   \n",
       "\n",
       "   split  label  persuasiveness_label  \n",
       "0  train      1                     0  \n",
       "1  train      0                     0  \n",
       "2  train      1                     0  \n",
       "3  train      1                     0  \n",
       "4  train      0                     1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../../data/\"\n",
    "IMG_PATH = \"../../data/images\"\n",
    "OUTPUT_DIR = \"../../results/multimodal/blip_align/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train_augmented.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "\n",
    "print(f\"\\n Train label distribution:\")\n",
    "print(f\"\\n Stance: \\n Oppose: {(df_train['label']==0).sum()}\\n Support: {(df_train['label']==1).sum()}\")\n",
    "print(f\"\\n\\n  Persuasiveness \\n No: {(df_train['persuasiveness_label']==0).sum()}\\n Yes: {(df_train['persuasiveness_label']==1).sum()}\")\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Model: microsoft/deberta-v3-base\n",
      "BLIP Model: Salesforce/blip-itm-base-coco\n",
      "ALIGN Model: kakaobrain/align-base\n",
      "Common embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Model Names\n",
    "TEXT_MODEL_NAME = \"microsoft/deberta-v3-base\"  # 768 dims\n",
    "BLIP_MODEL_NAME = \"Salesforce/blip-itm-base-coco\"  # 768 dims (vision encoder)\n",
    "ALIGN_MODEL_NAME = \"kakaobrain/align-base\"  # Will project to 768 dims\n",
    "\n",
    "# Text processing\n",
    "MAX_TEXT_LENGTH = 128\n",
    "\n",
    "# Common embedding dimension for fusion\n",
    "COMMON_DIM = 768\n",
    "\n",
    "print(f\"Text Model: {TEXT_MODEL_NAME}\")\n",
    "print(f\"BLIP Model: {BLIP_MODEL_NAME}\")\n",
    "print(f\"ALIGN Model: {ALIGN_MODEL_NAME}\")\n",
    "print(f\"Common embedding dimension: {COMMON_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Epochs: 15\n",
      "Learning rate: 2e-05\n",
      "Weight decay: 0.0001\n",
      "Warmup ratio: 0.1\n",
      "Patience: 5\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 2  \n",
    "\n",
    "# Other\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup ratio: {WARMUP_RATIO}\")\n",
    "print(f\"Patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetBLIP(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + BLIP (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for BLIP vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, img_dir, text_tokenizer, blip_processor, max_length=128):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.blip_processor = blip_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text \n",
    "        text = str(row['tweet_text'])\n",
    "        text_encoding = self.text_tokenizer(text,max_length=self.max_length,padding='max_length',truncation=True,return_tensors='pt')\n",
    "        \n",
    "        # Load Image\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # fallback to grey image\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n",
    "\n",
    "        image_encoding = self.blip_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn_blip(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and processor loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer and processor\n",
    "\n",
    "# Text tokenizer (DeBERTa)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "# BLIP processor\n",
    "blip_processor = BlipProcessor.from_pretrained(BLIP_MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizer and processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 137\n",
      "Dev batches: 13\n",
      "Test batches: 19\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset_blip = MultimodalDatasetBLIP(df_train, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_blip = MultimodalDatasetBLIP(df_dev, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_blip = MultimodalDatasetBLIP(df_test, IMG_PATH, text_tokenizer, blip_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_blip = DataLoader(train_dataset_blip,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_blip)\n",
    "\n",
    "dev_loader_blip = DataLoader(dev_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "test_loader_blip = DataLoader(test_dataset_blip,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_blip)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_blip)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_blip)}\")\n",
    "print(f\"Test batches: {len(test_loader_blip)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text input_ids shape: torch.Size([128])\n",
      "Text attention_mask shape: torch.Size([128])\n",
      "Image pixel_values shape: torch.Size([3, 384, 384])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# We test our dataloader\n",
    "sample = train_dataset_blip[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. DeBERTa + BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalBLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and BLIP (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"Salesforce/blip-itm-base-coco\",\n",
    "        num_classes=2,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalBLIP, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: BLIP (only vision part)\n",
    "        blip_full = BlipModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = blip_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = blip_full.config.vision_config.hidden_size  # 768\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers (if needed)\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion layers\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "\n",
    "        # Text embeddings\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision emgeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # BLIP vision encoder returns BaseModelOutputWithPooling\n",
    "        # We can use pooler_output or last_hidden_state[:, 0, :]\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            # Gate decides how much of each modality to use\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion: concat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,398,530\n",
      "  Trainable parameters: 271,398,530\n",
      "\n",
      "Fusion: mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 270,218,114\n",
      "  Trainable parameters: 270,218,114\n",
      "\n",
      "Fusion: weighted_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 270,218,116\n",
      "  Trainable parameters: 270,218,116\n",
      "\n",
      "Fusion: proj_concat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,399,298\n",
      "  Trainable parameters: 271,399,298\n",
      "\n",
      "Fusion: gated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.attention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.attention.output.dense.bias', 'text_model.encoder.layer.{0...11}.attention.output.dense.weight', 'text_model.encoder.layer.{0...11}.attention.self.key.bias', 'text_model.encoder.layer.{0...11}.attention.self.key.weight', 'text_model.encoder.layer.{0...11}.attention.self.query.bias', 'text_model.encoder.layer.{0...11}.attention.self.query.weight', 'text_model.encoder.layer.{0...11}.attention.self.value.bias', 'text_model.encoder.layer.{0...11}.attention.self.value.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.bias', 'text_model.encoder.layer.{0...11}.crossattention.output.dense.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.key.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.key.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.query.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.query.weight', 'text_model.encoder.layer.{0...11}.crossattention.self.value.bias', 'text_model.encoder.layer.{0...11}.crossattention.self.value.weight', 'text_model.encoder.layer.{0...11}.intermediate.dense.bias', 'text_model.encoder.layer.{0...11}.intermediate.dense.weight', 'text_model.encoder.layer.{0...11}.output.LayerNorm.bias', 'text_model.encoder.layer.{0...11}.output.LayerNorm.weight', 'text_model.encoder.layer.{0...11}.output.dense.bias', 'text_model.encoder.layer.{0...11}.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 271,398,530\n",
      "  Trainable parameters: 271,398,530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all fusion types\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(train_loader_blip))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    num_epochs=15,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    patience=5,\n",
    "    device=DEVICE,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multimodal model with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        best_model: Best model based on dev F1-score\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'dev_loss': [],\n",
    "        'dev_f1': [],\n",
    "        'dev_acc': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dev_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask, pixel_values)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                dev_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_dev_loss = dev_loss / len(dev_loader)\n",
    "        \n",
    "        # Metrics (F1-score binary with pos_label=1)\n",
    "        dev_acc = accuracy_score(all_labels, all_preds)\n",
    "        _, _, dev_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "        \n",
    "        history['dev_loss'].append(avg_dev_loss)\n",
    "        history['dev_f1'].append(dev_f1)\n",
    "        history['dev_acc'].append(dev_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Dev Loss: {avg_dev_loss:.4f}\")\n",
    "        print(f\"  Dev Accuracy: {dev_acc:.4f}\")\n",
    "        print(f\"  Dev F1 (binary, pos=1): {dev_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"    New best F1: {best_f1:.4f}\")\n",
    "            \n",
    "            if save_path:\n",
    "                torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"Training completed. Best Dev F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_model(model, test_loader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with accuracy, F1, precision, recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "        metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"f1\": f1_score(all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0),\n",
    "        \"precision\": precision_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"recall\": recall_score(all_labels, all_preds, pos_label=1, zero_division=0),\n",
    "        \"report\": classification_report(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTEST RESULTS\")\n",
    "    print(f\"  Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-score : {metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall   : {metrics['recall']:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(cm)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results_blip = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + BLIP\")\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*80 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalBLIP(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=BLIP_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_blip_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_blip,\n",
    "        dev_loader=dev_loader_blip,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_blip)\n",
    "    \n",
    "    # Store results\n",
    "    results_blip.append({\n",
    "        'model': 'DeBERTa + BLIP',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results_blip = pd.DataFrame(results_blip)\n",
    "df_results_blip = df_results_blip.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + BLIP\")\n",
    "print(df_results_blip.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_blip.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_blip.csv\"), index=False)\n",
    "print(f\"\\n Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_blip.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeBERTa + ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDatasetALIGN(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeBERTa (text) + ALIGN (vision) multimodal learning.\n",
    "    \n",
    "    Returns:\n",
    "    - Tokenized text (input_ids, attention_mask) for DeBERTa\n",
    "    - Processed image (pixel_values) for ALIGN vision encoder\n",
    "    - Label\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, img_dir, text_tokenizer, align_processor, max_length=128):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.align_processor = align_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Text\n",
    "        text = str(row['tweet_text'])\n",
    "        text_encoding = self.text_tokenizer(text,max_length=self.max_length,padding='max_length',truncation=True,return_tensors='pt')\n",
    "\n",
    "    \n",
    "        # Load Image - ALIGN PROCESSOR\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (289, 289), color=(128, 128, 128))\n",
    "        \n",
    "        image_encoding = self.align_processor(images=image, return_tensors='pt')\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_encoding['pixel_values'].squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for ALIGN DataLoader (same as BLIP)\n",
    "def collate_fn_align(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ALIGN processor loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ALIGN processor\n",
    "align_processor = AlignProcessor.from_pretrained(ALIGN_MODEL_NAME)\n",
    "\n",
    "print(\" ALIGN processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 137\n",
      "Dev batches: 13\n",
      "Test batches: 19\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset_align = MultimodalDatasetALIGN(df_train, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "dev_dataset_align = MultimodalDatasetALIGN(df_dev, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "test_dataset_align = MultimodalDatasetALIGN(df_test, IMG_PATH, text_tokenizer, align_processor, MAX_TEXT_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_align = DataLoader(train_dataset_align,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_align)\n",
    "dev_loader_align = DataLoader(dev_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "test_loader_align = DataLoader(test_dataset_align,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn_align)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader_align)}\")\n",
    "print(f\"Dev batches: {len(dev_loader_align)}\")\n",
    "print(f\"Test batches: {len(test_loader_align)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text input_ids shape: torch.Size([128])\n",
      "Text attention_mask shape: torch.Size([128])\n",
      "Image pixel_values shape: torch.Size([3, 289, 289])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# We test our dataloader\n",
    "sample = train_dataset_align[0]\n",
    "print(f\"Text input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Text attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Image pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalALIGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining DeBERTa (text) and ALIGN (vision).\n",
    "    \n",
    "    Fusion strategies:\n",
    "    - 'concat': Simple concatenation\n",
    "    - 'mean': Average of embeddings\n",
    "    - 'weighted_mean': Learnable weighted average\n",
    "    - 'proj_concat': Project then concatenate\n",
    "    - 'gated': Gated fusion mechanism\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name=\"microsoft/deberta-v3-base\",\n",
    "        vision_model_name=\"kakaobrain/align-base\",\n",
    "        num_classes=3,\n",
    "        fusion_type=\"concat\",\n",
    "        common_dim=768,\n",
    "        dropout=0.1,\n",
    "        freeze_text=False,\n",
    "        freeze_vision=False\n",
    "    ):\n",
    "        super(MultimodalALIGN, self).__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim = common_dim\n",
    "        \n",
    "        # Text encoder: DeBERTa\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "        \n",
    "        # Vision encoder: ALIGN (only vision part)\n",
    "        align_full = AlignModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = align_full.vision_model  # Extract only vision encoder\n",
    "        self.vision_dim = align_full.config.vision_config.hidden_dim #640\n",
    "        \n",
    "        print(f\"ALIGN vision dimension: {self.vision_dim}\")\n",
    "        \n",
    "        # Freeze encoders if specified\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze_vision:\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        if self.text_dim != common_dim:\n",
    "            self.text_projection = nn.Linear(self.text_dim, common_dim)\n",
    "        else:\n",
    "            self.text_projection = nn.Identity()\n",
    "        \n",
    "        # ALIGN vision encoder output needs projection\n",
    "        if self.vision_dim != common_dim:\n",
    "            self.vision_projection = nn.Linear(self.vision_dim, common_dim)\n",
    "        else:\n",
    "            self.vision_projection = nn.Identity()\n",
    "        \n",
    "        # Fusion-specific layers (same as BLIP model)\n",
    "        if fusion_type == \"concat\":\n",
    "            fusion_dim = common_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"mean\":\n",
    "            fusion_dim = common_dim\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"weighted_mean\":\n",
    "            fusion_dim = common_dim\n",
    "            # Learnable weights for weighted average\n",
    "            self.text_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.vision_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        elif fusion_type == \"proj_concat\":\n",
    "            # Project to lower dim, then concatenate\n",
    "            proj_dim = common_dim // 2\n",
    "            self.text_proj = nn.Linear(common_dim, proj_dim)\n",
    "            self.vision_proj = nn.Linear(common_dim, proj_dim)\n",
    "            fusion_dim = proj_dim * 2\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(fusion_dim, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        elif fusion_type == \"gated\":\n",
    "            fusion_dim = common_dim\n",
    "            # Gate mechanism\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(common_dim * 2, common_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_layer = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {fusion_type}\")\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        \n",
    "        # Text Embeddings \n",
    "        text_outputs = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_emb = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_emb = self.text_projection(text_emb)  # Project to common_dim\n",
    "        \n",
    "        # Vision Embeddings\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # ALIGN vision encoder returns BaseModelOutputWithPooling\n",
    "        vision_emb = vision_outputs.pooler_output  # Already pooled\n",
    "        vision_emb = self.vision_projection(vision_emb)  # Project to common_dim\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fused = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"mean\":\n",
    "            fused = (text_emb + vision_emb) / 2\n",
    "        \n",
    "        elif self.fusion_type == \"weighted_mean\":\n",
    "            # Normalize weights\n",
    "            w_text = torch.sigmoid(self.text_weight)\n",
    "            w_vision = torch.sigmoid(self.vision_weight)\n",
    "            total = w_text + w_vision\n",
    "            fused = (w_text / total) * text_emb + (w_vision / total) * vision_emb\n",
    "        \n",
    "        elif self.fusion_type == \"proj_concat\":\n",
    "            text_proj = self.text_proj(text_emb)\n",
    "            vision_proj = self.vision_proj(vision_emb)\n",
    "            fused = torch.cat([text_proj, vision_proj], dim=1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            concat = torch.cat([text_emb, vision_emb], dim=1)\n",
    "            gate = self.gate(concat)\n",
    "            fused = gate * text_emb + (1 - gate) * vision_emb\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion: concat\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,943,762\n",
      "  Trainable parameters: 247,943,762\n",
      "\n",
      "Fusion: mean\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 246,763,346\n",
      "  Trainable parameters: 246,763,346\n",
      "\n",
      "Fusion: weighted_mean\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 246,763,348\n",
      "  Trainable parameters: 246,763,348\n",
      "\n",
      "Fusion: proj_concat\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,944,530\n",
      "  Trainable parameters: 247,944,530\n",
      "\n",
      "Fusion: gated\n",
      "ALIGN vision dimension: 640\n",
      "  Input batch size: 4\n",
      "  Output logits shape: torch.Size([4, 2])\n",
      "  Model parameters: 247,943,762\n",
      "  Trainable parameters: 247,943,762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sanity check for ALIGN model and fusion strategies\n",
    "fusion_types = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "for fusion in fusion_types:\n",
    "    print(f\"Fusion: {fusion}\")\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM).to(DEVICE)\n",
    "    \n",
    "    batch = next(iter(train_loader_align))\n",
    "    input_ids = batch['input_ids'][:4].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'][:4].to(DEVICE)\n",
    "    pixel_values = batch['pixel_values'][:4].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "    \n",
    "    print(f\"  Input batch size: {input_ids.size(0)}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING ALL FUSION STRATEGIES: DeBERTa + ALIGN\n",
      "======================================================================\n",
      "Fusion strategies: ['concat', 'mean', 'weighted_mean', 'proj_concat', 'gated']\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: CONCAT\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb4e6ba051b455e92ceeb5df038ebc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6323\n",
      "  Dev Loss: 0.4504\n",
      "  Dev Accuracy: 0.8200\n",
      "  Dev F1 (binary, pos=1): 0.6897\n",
      "    New best F1: 0.6897\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c30727e2dd4b1480c2f74f1e3f005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3573\n",
      "  Dev Loss: 0.2900\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8497\n",
      "    New best F1: 0.8497\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c9153d7e8b4a2db60372f395de6074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2304\n",
      "  Dev Loss: 0.2778\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8551\n",
      "    New best F1: 0.8551\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d144aabc12e47eda31d2809ea1f38c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1318\n",
      "  Dev Loss: 0.3991\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8296\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ce9c307604afd8da0beebb5419b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0622\n",
      "  Dev Loss: 0.4114\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6dd3a82c2b46c994d028873a597620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0206\n",
      "  Dev Loss: 0.7859\n",
      "  Dev Accuracy: 0.8700\n",
      "  Dev F1 (binary, pos=1): 0.7937\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642e579ed2954adca10e769410873b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0102\n",
      "  Dev Loss: 0.5532\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8671\n",
      "    New best F1: 0.8671\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fe4b5016dd4ef791f424dc7a3d06e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0133\n",
      "  Dev Loss: 0.6751\n",
      "  Dev Accuracy: 0.8850\n",
      "  Dev F1 (binary, pos=1): 0.8296\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e4a46bf32e48e0be1ec46ccaec9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0042\n",
      "  Dev Loss: 0.7512\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05d6929d1fc4c8e8c33e3805369eb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0066\n",
      "  Dev Loss: 0.6753\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8696\n",
      "    New best F1: 0.8696\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6733b7059aab43f299365ee89fdc3473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0019\n",
      "  Dev Loss: 0.7492\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8593\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91783b1643984618920e3aee5d0e3d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "  Train Loss: 0.0065\n",
      "  Dev Loss: 0.7967\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413e0124c5114ba99ea0abe09c44881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "  Train Loss: 0.0048\n",
      "  Dev Loss: 0.7805\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8421\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8700038bf54e078539007b43a8cbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:\n",
      "  Train Loss: 0.0007\n",
      "  Dev Loss: 0.7136\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e2898e1b74533b5a7430c3af710f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:\n",
      "  Train Loss: 0.0006\n",
      "  Dev Loss: 0.7016\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8633\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 15 epochs\n",
      "Training completed. Best Dev F1: 0.8696\n",
      "\n",
      "======================================================================\n",
      "EVALUATING CONCAT ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c475d88ef04625b529d2794cde33ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8400\n",
      "  F1-score : 0.8195\n",
      "  Precision: 0.7365\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[143  39]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: MEAN\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b9943a98df4409b2d86fb3ccb10de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6230\n",
      "  Dev Loss: 0.4651\n",
      "  Dev Accuracy: 0.8700\n",
      "  Dev F1 (binary, pos=1): 0.8000\n",
      "    New best F1: 0.8000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21afb6c6f17493f82e8d43cea23286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3372\n",
      "  Dev Loss: 0.3076\n",
      "  Dev Accuracy: 0.8800\n",
      "  Dev F1 (binary, pos=1): 0.8462\n",
      "    New best F1: 0.8462\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa640ce999a42c78d1be50cf0e5d978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.1875\n",
      "  Dev Loss: 0.2434\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8824\n",
      "    New best F1: 0.8824\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947b3d291384454088ae145e7b42a40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1013\n",
      "  Dev Loss: 0.3748\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8615\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dcb81d6c0b421a8e16d4d1fdbb4f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0635\n",
      "  Dev Loss: 0.3809\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7b17a1ba149b49b568b155973202d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0306\n",
      "  Dev Loss: 0.4160\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "    New best F1: 0.8966\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ecd8a4d684852a37ff5755482dbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0151\n",
      "  Dev Loss: 0.4943\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8652\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594ca698b79f4d1787621f628a098051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0198\n",
      "  Dev Loss: 0.5397\n",
      "  Dev Accuracy: 0.9150\n",
      "  Dev F1 (binary, pos=1): 0.8741\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629522ccaf58404588a1b06bcb7f0c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0128\n",
      "  Dev Loss: 0.6081\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df3eb9b0dd0469ebb9bcfb379ab91cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0119\n",
      "  Dev Loss: 0.5884\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8489\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc970720204b4b1388520a3ba7125327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0103\n",
      "  Dev Loss: 0.6292\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 11 epochs\n",
      "Training completed. Best Dev F1: 0.8966\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MEAN ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9ffb4018b9426ea72d6c80dc1d12d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8567\n",
      "  F1-score : 0.8340\n",
      "  Precision: 0.7660\n",
      "  Recall   : 0.9153\n",
      "\n",
      "Confusion Matrix:\n",
      "[[149  33]\n",
      " [ 10 108]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: WEIGHTED_MEAN\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becafd40baee48f1aa92740dc7c615d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6223\n",
      "  Dev Loss: 0.4865\n",
      "  Dev Accuracy: 0.8050\n",
      "  Dev F1 (binary, pos=1): 0.6355\n",
      "    New best F1: 0.6355\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbc0cc29a0c4d338ac60e8dd0b9dfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3470\n",
      "  Dev Loss: 0.2569\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8594\n",
      "    New best F1: 0.8594\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac1d560a0ed42678a178a8c2ff2b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2172\n",
      "  Dev Loss: 0.2955\n",
      "  Dev Accuracy: 0.8750\n",
      "  Dev F1 (binary, pos=1): 0.8322\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cb8a571f7d47f99dc40432a2627d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1269\n",
      "  Dev Loss: 0.3729\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8462\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb9229f3da141608c8e892bbfdfcad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0815\n",
      "  Dev Loss: 0.4067\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8529\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285bd66b7054714ba5b0a8397c55157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0386\n",
      "  Dev Loss: 0.3855\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8696\n",
      "    New best F1: 0.8696\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2fa7e6bbff4acaa634d1bfee5f5236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0257\n",
      "  Dev Loss: 0.5063\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8550\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262a0016e5aa474b8adf888332a5b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0142\n",
      "  Dev Loss: 0.5189\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8571\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692496b80674832991d2e1c8e7fcd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0136\n",
      "  Dev Loss: 0.6035\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8485\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67368fc8802d4b6ebc815c378605bc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0060\n",
      "  Dev Loss: 0.5532\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbe9a5f6c9a42a89dd42f42a9bf77ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0052\n",
      "  Dev Loss: 0.5578\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8657\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 11 epochs\n",
      "Training completed. Best Dev F1: 0.8696\n",
      "\n",
      "======================================================================\n",
      "EVALUATING WEIGHTED_MEAN ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497a2e8e8da2449891252a293701da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8433\n",
      "  F1-score : 0.8240\n",
      "  Precision: 0.7383\n",
      "  Recall   : 0.9322\n",
      "\n",
      "Confusion Matrix:\n",
      "[[143  39]\n",
      " [  8 110]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: PROJ_CONCAT\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5d008919f7494590430df8ac18edab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6641\n",
      "  Dev Loss: 0.5024\n",
      "  Dev Accuracy: 0.8250\n",
      "  Dev F1 (binary, pos=1): 0.7742\n",
      "    New best F1: 0.7742\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aa9000c537412697eadadf0defc4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3096\n",
      "  Dev Loss: 0.3933\n",
      "  Dev Accuracy: 0.8400\n",
      "  Dev F1 (binary, pos=1): 0.7193\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b988380e9443bca67b6d873579ba75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.1624\n",
      "  Dev Loss: 0.3749\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8707\n",
      "    New best F1: 0.8707\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fea40a30ee484da82b61ea5a396b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.0903\n",
      "  Dev Loss: 0.3636\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8788\n",
      "    New best F1: 0.8788\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adea2680b7244f3889a104f7ec488c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0377\n",
      "  Dev Loss: 0.5744\n",
      "  Dev Accuracy: 0.9000\n",
      "  Dev F1 (binary, pos=1): 0.8507\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcc00f63d4a4ee08da3635a5a4784ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0169\n",
      "  Dev Loss: 0.6069\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3edd61ebc64adcbfce73128eb3bef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0080\n",
      "  Dev Loss: 0.8498\n",
      "  Dev Accuracy: 0.8750\n",
      "  Dev F1 (binary, pos=1): 0.8031\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6795e24400254af09c873567bfb7531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0106\n",
      "  Dev Loss: 0.7178\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8444\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b627a3f92f465497408f2921883bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0032\n",
      "  Dev Loss: 0.7283\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8593\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 9 epochs\n",
      "Training completed. Best Dev F1: 0.8788\n",
      "\n",
      "======================================================================\n",
      "EVALUATING PROJ_CONCAT ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fdb97a4a8b4c2e87dc25c99a31b6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8733\n",
      "  F1-score : 0.8516\n",
      "  Precision: 0.7899\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[153  29]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "FUSION STRATEGY: GATED\n",
      "######################################################################\n",
      "\n",
      "ALIGN vision dimension: 640\n",
      "Starting training for 15 epochs...\n",
      "Total steps: 2055, Warmup steps: 205\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3693527b1740de829c409c63ac5c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train Loss: 0.6553\n",
      "  Dev Loss: 0.5548\n",
      "  Dev Accuracy: 0.7400\n",
      "  Dev F1 (binary, pos=1): 0.5185\n",
      "    New best F1: 0.5185\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a111187c6742f28d56bc3e3e85b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train Loss: 0.3093\n",
      "  Dev Loss: 0.3310\n",
      "  Dev Accuracy: 0.8950\n",
      "  Dev F1 (binary, pos=1): 0.8372\n",
      "    New best F1: 0.8372\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0f0c67e3e043d1b9d1d1784ff3814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train Loss: 0.2021\n",
      "  Dev Loss: 0.2287\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8857\n",
      "    New best F1: 0.8857\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbad452f53824976ab9e5faf5427eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train Loss: 0.1225\n",
      "  Dev Loss: 0.3263\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8633\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b6c52cee14f609b4a382e6dc4b587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train Loss: 0.0691\n",
      "  Dev Loss: 0.4367\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c440719a124f482ba53e7b8edbf01e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train Loss: 0.0431\n",
      "  Dev Loss: 0.4549\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24c9ba9fb97457781cb669cf3e3e5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train Loss: 0.0306\n",
      "  Dev Loss: 0.4843\n",
      "  Dev Accuracy: 0.9100\n",
      "  Dev F1 (binary, pos=1): 0.8750\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587a569fbc4f48f3bb3f1a95536c697b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train Loss: 0.0095\n",
      "  Dev Loss: 0.5244\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "    New best F1: 0.8966\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ecc1299e4f408fabe97196603db5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train Loss: 0.0062\n",
      "  Dev Loss: 0.5433\n",
      "  Dev Accuracy: 0.9150\n",
      "  Dev F1 (binary, pos=1): 0.8794\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2555e3416927404984742c2cb7c2674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train Loss: 0.0036\n",
      "  Dev Loss: 0.5491\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8873\n",
      "  Patience: 2/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75faf588494c2198aad89558207a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train Loss: 0.0099\n",
      "  Dev Loss: 0.6125\n",
      "  Dev Accuracy: 0.9050\n",
      "  Dev F1 (binary, pos=1): 0.8613\n",
      "  Patience: 3/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df684d2e364021a66c352dd867eca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "  Train Loss: 0.0113\n",
      "  Dev Loss: 0.5333\n",
      "  Dev Accuracy: 0.9250\n",
      "  Dev F1 (binary, pos=1): 0.8966\n",
      "  Patience: 4/5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52347b7b3d8845b389ab4eb91640f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "  Train Loss: 0.0078\n",
      "  Dev Loss: 0.5700\n",
      "  Dev Accuracy: 0.9200\n",
      "  Dev F1 (binary, pos=1): 0.8841\n",
      "  Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 13 epochs\n",
      "Training completed. Best Dev F1: 0.8966\n",
      "\n",
      "======================================================================\n",
      "EVALUATING GATED ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a60b642e7f408bb2534960cd0e893e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST RESULTS\n",
      "  Accuracy : 0.8533\n",
      "  F1-score : 0.8321\n",
      "  Precision: 0.7569\n",
      "  Recall   : 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[147  35]\n",
      " [  9 109]]\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "results_align = []\n",
    "\n",
    "fusion_strategies = [\"concat\", \"mean\", \"weighted_mean\", \"proj_concat\", \"gated\"]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL FUSION STRATEGIES: DeBERTa + ALIGN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Fusion strategies: {fusion_strategies}\\n\")\n",
    "\n",
    "for fusion in fusion_strategies:\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"FUSION STRATEGY: {fusion.upper()}\")\n",
    "    print(\"#\"*70 + \"\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalALIGN(\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        vision_model_name=ALIGN_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        fusion_type=fusion,\n",
    "        common_dim=COMMON_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"deberta_align_{fusion}_best.pt\")\n",
    "    trained_model, history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_align,\n",
    "        dev_loader=dev_loader_align,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        patience=PATIENCE,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {fusion.upper()} ON TEST SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    test_metrics = evaluate_multimodal_model(trained_model, test_loader_align)\n",
    "    \n",
    "    # Store results\n",
    "    results_align.append({\n",
    "        'model': 'DeBERTa + ALIGN',\n",
    "        'fusion': fusion,\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'best_dev_f1': max(history['dev_f1'])\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    # Clean up\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS SUMMARY: DeBERTa + ALIGN\n",
      "         model        fusion  accuracy       f1  precision   recall  best_dev_f1\n",
      "DeBERTa + BLIP   proj_concat  0.873333 0.851562   0.789855 0.923729     0.878788\n",
      "DeBERTa + BLIP          mean  0.856667 0.833977   0.765957 0.915254     0.896552\n",
      "DeBERTa + BLIP         gated  0.853333 0.832061   0.756944 0.923729     0.896552\n",
      "DeBERTa + BLIP weighted_mean  0.843333 0.823970   0.738255 0.932203     0.869565\n",
      "DeBERTa + BLIP        concat  0.840000 0.819549   0.736486 0.923729     0.869565\n",
      "\n",
      "  Results saved to: ../../results/multimodal/blip_align/results_deberta_align.csv\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "df_results_align = pd.DataFrame(results_align)\n",
    "df_results_align = df_results_align.sort_values('f1', ascending=False)\n",
    "print(\"RESULTS SUMMARY: DeBERTa + ALIGN\")\n",
    "print(df_results_align.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_align.to_csv(os.path.join(OUTPUT_DIR, \"results_deberta_align.csv\"), index=False)\n",
    "print(f\"\\n  Results saved to: {os.path.join(OUTPUT_DIR, 'results_deberta_align.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparation: BLIP vs ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "df_all_results = pd.concat([df_results_blip, df_results_align], ignore_index=True)\n",
    "\n",
    "# Sort by F1-binary\n",
    "df_all_results = df_all_results.sort_values('f1', ascending=False)\n",
    "\n",
    "\n",
    "print(\"GLOBAL RESULTS: ALL MODELS AND FUSION STRATEGIES\")\n",
    "print(df_all_results.to_string(index=False))\n",
    "\n",
    "# Best overall model\n",
    "best_overall = df_all_results.iloc[0]\n",
    "print(f\"\\n BEST OVERALL MODEL \")\n",
    "print(f\"   Model: {best_overall['model']}\")\n",
    "print(f\"   Fusion: {best_overall['fusion'].upper()}\")\n",
    "print(f\"   Test F1-Binary: {best_overall['f1']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "print(f\"   Dev F1 (best): {best_overall['best_dev_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Conclusiones y Recomendaciones\n",
    "\n",
    "**Anlisis:**\n",
    "\n",
    "1. **Mejor Modelo Global:** El modelo con mejor F1-Score binario es el indicado arriba\n",
    "\n",
    "2. **BLIP vs ALIGN:** Comparacin del rendimiento promedio de cada vision encoder\n",
    "\n",
    "3. **Mejor Estrategia de Fusin:** La estrategia que consistentemente da mejores resultados\n",
    "\n",
    "4. **Recomendaciones:**\n",
    "   - Para produccin, usar el mejor modelo global\n",
    "   - Considerar trade-off entre rendimiento y complejidad\n",
    "   - Evaluar tiempo de inferencia si es crtico\n",
    "\n",
    "**Siguientes Pasos:**\n",
    "- Analizar casos donde el modelo falla\n",
    "- Explorar data augmentation para imgenes\n",
    "- Considerar ensembles de los mejores modelos\n",
    "- Fine-tuning con learning rates diferenciados por capa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
