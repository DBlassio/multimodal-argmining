{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67501c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b38f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model Base\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=2)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e4cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset[\"train\"].column_names)\n",
    "print(dataset[\"train\"][0][\"text\"])\n",
    "print(dataset[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f97530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:20<00:00, 1223.77 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:23<00:00, 1066.00 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [01:13<00:00, 678.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'input_ids': [0, 100, 16425, 38, 3326, 230, 42338, 18024, 12, 975, 25322, 4581, 31, 127, 569, 1400, 142, 9, 70, 5, 6170, 14, 7501, 24, 77, 24, 21, 78, 703, 11, 13025, 4, 38, 67, 1317, 14, 23, 78, 24, 21, 5942, 30, 121, 4, 104, 4, 10102, 114, 24, 655, 1381, 7, 2914, 42, 247, 6, 3891, 145, 10, 2378, 9, 3541, 1687, 22, 10800, 34689, 113, 38, 269, 56, 7, 192, 42, 13, 2185, 49069, 3809, 1589, 49007, 3809, 48709, 133, 6197, 16, 14889, 198, 10, 664, 9004, 4149, 1294, 1440, 27450, 54, 1072, 7, 1532, 960, 79, 64, 59, 301, 4, 96, 1989, 79, 1072, 7, 1056, 69, 39879, 2485, 7, 442, 103, 2345, 9, 6717, 15, 99, 5, 674, 25517, 242, 802, 59, 1402, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization \n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"],padding=\"max_length\",truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "871dc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Lore\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Secuencia de clasificación\n",
    "    r=8,                  # Dimensión baja del adaptador\n",
    "    lora_alpha=32,        # Escala\n",
    "    lora_dropout=0.1,     # Dropout para regularización\n",
    "    target_modules=[\"query\", \"value\"]  # Solo aplicamos LoRA a Q y V en self-attention\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36ab18",
   "metadata": {},
   "source": [
    "### LoRA Configuration Parameters\n",
    "\n",
    "\n",
    "| Parameter | Description | How to Choose / Rule of Thumb |\n",
    "|-----------|-------------|-------------------------------|\n",
    "| **task_type** | Type of task the model is fine-tuned for (e.g., sequence classification, generation). | `\"SEQ_CLS\"` for classification, `\"SEQ_2_SEQ_LM\"` for generation, `\"CAUSAL_LM\"` for causal decoders. |\n",
    "| **r** | Low-rank dimension of the LoRA matrices (controls capacity). | Small models: 4–16; Large models: 16–64. Higher → more expressive but more parameters. |\n",
    "| **lora_alpha** | Scaling factor for the LoRA update \\(W + αBA\\). | Usually 1–4 × `r`. Too low → weak updates; too high → unstable training. |\n",
    "| **lora_dropout** | Dropout applied to the LoRA module for regularization. | Small datasets: 0.1–0.2; Large datasets: 0–0.1. Prevents overfitting. |\n",
    "| **target_modules** | Specifies which layers are modified with LoRA. | Common: `[\"query\", \"value\"]` for attention. Can include feed-forward (`\"dense\"`) or `\"all\"`. |\n",
    "| **fan_in_fan_out** | Adjusts matrix orientation; required for some architectures like GPT. | Usually left as default unless the model needs it. |\n",
    "| **merge_weights** | Whether to merge LoRA weights into the base model after training. | Merge after fine-tuning to reduce memory usage. |\n",
    "| **bias** | Whether LoRA affects biases or only weight matrices. | Usually keep bias unchanged. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA to the Model\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b32d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_magnitude_vector ModuleDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_dropout ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_dropout.default Dropout(p=0.1, inplace=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default Linear(in_features=768, out_features=8, bias=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      ")\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default Linear(in_features=8, out_features=768, bias=False)\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_embedding_A ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_embedding_B ParameterDict()\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_magnitude_vector ModuleDict()\n"
     ]
    }
   ],
   "source": [
    "# Explore each module of our Model\n",
    "for name, module in model_lora.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9271d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRaining Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir columnas que el modelo espera\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78386b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_32608\\1016706193.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ccecff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/4689 01:55 < 25:07:13, 0.05 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32608\\49973641.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2325\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2672\u001b[0m                     )\n\u001b[0;32m   2673\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2674\u001b[1;33m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2676\u001b[0m                     if (\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4019\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4020\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4022\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_items_in_batch\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4110\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4111\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4112\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\peft\\peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"task_ids\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m                 return self.base_model(\n\u001b[0m\u001b[0;32m   1653\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\peft\\tuners\\tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         outputs = self.roberta(\n\u001b[0m\u001b[0;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    863\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\utils\\deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add cross attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
