{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a84cff6",
   "metadata": {},
   "source": [
    "### Fusion Strategies Comparison\n",
    "\n",
    "This experiment compares three multimodal fusion strategies:\n",
    "  1. Early Fusion (concatenate embeddings)\n",
    "  2. Late Fusion (ensemble predictions)\n",
    "  3. Intermediate Fusion (cross-attention)\n",
    "\n",
    "Goal: Determine which fusion strategy works best for stance classification.\n",
    "\n",
    "Research Questions:\n",
    "  - Does allowing modalities to interact improve performance?\n",
    "  - Is the added complexity of cross-attention worth it?\n",
    "  - Can late fusion ensemble provide benefits without retraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276ab224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  42\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel,get_linear_schedule_with_warmup\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Seed:  {SEED}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cf6b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train label distribution:\n",
      "\n",
      " Stance: \n",
      " Oppose: 1095\n",
      " Support: 719\n",
      "\n",
      "\n",
      "  Persuasiveness \n",
      " No: 1285\n",
      " Yes: 529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stance</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>persuasiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148501065308004357</td>\n",
       "      <td>https://t.co/VQP1FHaWAg</td>\n",
       "      <td>Let's McGyver some Sanity in America!\\n\\nYou a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1103872992537276417</td>\n",
       "      <td>https://t.co/zsyXYSeBkp</td>\n",
       "      <td>A child deserves a chance at life. A child des...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151528583623585794</td>\n",
       "      <td>https://t.co/qSWvDX5MnM</td>\n",
       "      <td>Dear prolifers: girls as young as 10, 11, 12 a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100166844026109953</td>\n",
       "      <td>https://t.co/hxH8tFIHUu</td>\n",
       "      <td>The many States will attempt to amend their co...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021830413550067713</td>\n",
       "      <td>https://t.co/5whvEEtoQR</td>\n",
       "      <td>Every #abortion is wrong, no matter what metho...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                tweet_url  \\\n",
       "0  1148501065308004357  https://t.co/VQP1FHaWAg   \n",
       "1  1103872992537276417  https://t.co/zsyXYSeBkp   \n",
       "2  1151528583623585794  https://t.co/qSWvDX5MnM   \n",
       "3  1100166844026109953  https://t.co/hxH8tFIHUu   \n",
       "4  1021830413550067713  https://t.co/5whvEEtoQR   \n",
       "\n",
       "                                          tweet_text   stance persuasiveness  \\\n",
       "0  Let's McGyver some Sanity in America!\\n\\nYou a...  support             no   \n",
       "1  A child deserves a chance at life. A child des...   oppose             no   \n",
       "2  Dear prolifers: girls as young as 10, 11, 12 a...  support             no   \n",
       "3  The many States will attempt to amend their co...  support             no   \n",
       "4  Every #abortion is wrong, no matter what metho...   oppose            yes   \n",
       "\n",
       "   split  label  persuasiveness_label  \n",
       "0  train      1                     0  \n",
       "1  train      0                     0  \n",
       "2  train      1                     0  \n",
       "3  train      1                     0  \n",
       "4  train      0                     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../../../data/\"\n",
    "IMG_PATH = \"../../../data/images\"\n",
    "OUTPUT_DIR = \"../../../results/multimodal/baseline_multimodal/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "\n",
    "print(f\"\\n Train label distribution:\")\n",
    "print(f\"\\n Stance: \\n Oppose: {(df_train['label']==0).sum()}\\n Support: {(df_train['label']==1).sum()}\")\n",
    "print(f\"\\n\\n  Persuasiveness \\n No: {(df_train['persuasiveness_label']==0).sum()}\\n Yes: {(df_train['persuasiveness_label']==1).sum()}\")\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c180ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "TEXT_MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "VISION_MODEL_NAME = \"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e57edd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Image preprocessing\n",
    "IMG_SIZE = 384\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Text preprocessing\n",
    "MAX_TEXT_LENGTH = 105\n",
    "\n",
    "# Other\n",
    "NUM_WORKERS = 1\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb7101",
   "metadata": {},
   "source": [
    "###  Multimodal Dataset\n",
    "We create a MultimodalDataset that will return:\n",
    "- tokenized text (input_ids, attention_mask)\n",
    "- image tensor (transforms applied)\n",
    "- label (stance)\n",
    "\n",
    "We will handle corrupted images safely (blank image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab2f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transforms (no augmentation for baseline)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMG_MEAN, std=IMG_STD)\n",
    "])\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns (image, text, label) for multimodal learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        img_dir: str,\n",
    "        tokenizer,\n",
    "        image_transform,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"  Dataset created: {len(self)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load Image\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.image_transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not load image {img_path}. Using blank image instead.\")\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(0, 0, 0))\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        \n",
    "        # Load Text and Tokenize \n",
    "        text = str(row['tweet_text'])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        # Our Label\n",
    "        label = row['label']\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'tweet_id': str(row['tweet_id']),\n",
    "            'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc0de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: microsoft/deberta-v3-base\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenized\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {TEXT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82312c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset created: 1814 samples\n",
      "  Dataset created: 200 samples\n",
      "  Dataset created: 300 samples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(df_train, IMG_PATH, tokenizer, image_transforms, MAX_TEXT_LENGTH)\n",
    "dev_dataset = MultimodalDataset(df_dev, IMG_PATH, tokenizer, image_transforms, MAX_TEXT_LENGTH)\n",
    "test_dataset = MultimodalDataset(df_test, IMG_PATH, tokenizer, image_transforms, MAX_TEXT_LENGTH)\n",
    "\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "dev_loader = DataLoader(dev_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44a27c",
   "metadata": {},
   "source": [
    "### 1. Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7ffd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"Early Fusion: Concatenate embeddings before classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_model_name, vision_model_name, num_classes=2, \n",
    "                 freeze_encoders=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Vision encoder\n",
    "        self.vision_encoder = models.resnet50(pretrained=True)\n",
    "        self.vision_encoder = nn.Sequential(*list(self.vision_encoder.children())[:-1])\n",
    "        self.vision_dim = 2048\n",
    "        \n",
    "        # Freeze encoders\n",
    "        if freeze_encoders:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion_dim = self.text_dim + self.vision_dim\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.fusion_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout/2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"   Early Fusion initialized: {self.fusion_dim}D â†’ {num_classes}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Text embedding\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_emb = text_out.last_hidden_state[:, 0, :]  # [batch, 768]\n",
    "        \n",
    "        # Image embedding\n",
    "        img_feat = self.vision_encoder(images).squeeze(-1).squeeze(-1)  # [batch, 2048]\n",
    "        \n",
    "        # Concatenate\n",
    "        fused = torch.cat([text_emb, img_feat], dim=1)  # [batch, 2816]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9303b",
   "metadata": {},
   "source": [
    "### 2. Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97deb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Late Fusion: Train separate classifiers, then ensemble predictions.\n",
    "    \n",
    "    Two approaches:\n",
    "      1. Fixed weights: Î± * text_logits + (1-Î±) * image_logits\n",
    "      2. Learned weights: Network learns Î± during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_model_name, vision_model_name, num_classes=2,\n",
    "                 freeze_encoders=True, dropout=0.3, learn_fusion_weight=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text branch\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size\n",
    "        self.text_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.text_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Image branch\n",
    "        self.vision_encoder = models.resnet50(pretrained=True)\n",
    "        self.vision_encoder = nn.Sequential(*list(self.vision_encoder.children())[:-1])\n",
    "        self.vision_dim = 2048\n",
    "        self.vision_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.vision_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Freeze encoders\n",
    "        if freeze_encoders:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Fusion weight\n",
    "        self.learn_fusion_weight = learn_fusion_weight\n",
    "        if learn_fusion_weight:\n",
    "            # Learnable weight Î± âˆˆ [0, 1]\n",
    "            self.fusion_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        else:\n",
    "            # Fixed weight\n",
    "            self.register_buffer('fusion_weight', torch.tensor(0.5))\n",
    "        \n",
    "        print(f\"    Late Fusion initialized\")\n",
    "        print(f\"     - Learn fusion weight: {learn_fusion_weight}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Text logits\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_emb = text_out.last_hidden_state[:, 0, :]\n",
    "        text_logits = self.text_classifier(text_emb)  # [batch, num_classes]\n",
    "        \n",
    "        # Image logits\n",
    "        img_feat = self.vision_encoder(images).squeeze(-1).squeeze(-1)\n",
    "        img_logits = self.vision_classifier(img_feat)  # [batch, num_classes]\n",
    "        \n",
    "        # Weighted fusion\n",
    "        alpha = torch.sigmoid(self.fusion_weight)  # Ensure [0, 1]\n",
    "        fused_logits = alpha * text_logits + (1 - alpha) * img_logits\n",
    "        \n",
    "        return fused_logits\n",
    "    \n",
    "    def get_fusion_weight(self):\n",
    "        if self.learn_fusion_weight:\n",
    "            return torch.sigmoid(self.fusion_weight).item()\n",
    "        else:\n",
    "            return self.fusion_weight.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148febdc",
   "metadata": {},
   "source": [
    "### 3. Intermediate Fusion (Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention module for multimodal fusion.\n",
    "    \n",
    "    Allows text and image to attend to each other.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_dim, image_dim, hidden_dim=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project to same dimension\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Normalization\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Feedforward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, text_emb, image_emb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_emb: [batch, text_dim]\n",
    "            image_emb: [batch, image_dim]\n",
    "        \n",
    "        Returns:\n",
    "            attended_text: [batch, hidden_dim]\n",
    "            attended_image: [batch, hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size = text_emb.size(0)\n",
    "        \n",
    "        # Project to same dimension\n",
    "        text_proj = self.text_proj(text_emb).unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        image_proj = self.image_proj(image_emb).unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Text attends to Image\n",
    "        attended_text, _ = self.cross_attention(\n",
    "            query=text_proj,\n",
    "            key=image_proj,\n",
    "            value=image_proj\n",
    "        )\n",
    "        attended_text = attended_text.squeeze(1)  # [batch, hidden_dim]\n",
    "        attended_text = self.norm(attended_text + text_proj.squeeze(1))\n",
    "        \n",
    "        # Feedforward\n",
    "        attended_text = attended_text + self.ffn(attended_text)\n",
    "        attended_text = self.norm2(attended_text)\n",
    "        \n",
    "        # Image attends to Text (optional, for symmetry)\n",
    "        attended_image, _ = self.cross_attention(\n",
    "            query=image_proj,\n",
    "            key=text_proj,\n",
    "            value=text_proj\n",
    "        )\n",
    "        attended_image = attended_image.squeeze(1)\n",
    "        attended_image = self.norm(attended_image + image_proj.squeeze(1))\n",
    "        attended_image = attended_image + self.ffn(attended_image)\n",
    "        attended_image = self.norm2(attended_image)\n",
    "        \n",
    "        return attended_text, attended_image\n",
    "\n",
    "class IntermediateFusionModel(nn.Module):\n",
    "    \"\"\"Intermediate Fusion: Cross-attention between text and image.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_model_name, vision_model_name, num_classes=2,\n",
    "                 freeze_encoders=True, dropout=0.3, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Vision encoder\n",
    "        self.vision_encoder = models.resnet50(pretrained=True)\n",
    "        self.vision_encoder = nn.Sequential(*list(self.vision_encoder.children())[:-1])\n",
    "        self.vision_dim = 2048\n",
    "        \n",
    "        # Freeze encoders\n",
    "        if freeze_encoders:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.vision_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        self.fusion_dim = 512\n",
    "        self.cross_attention = CrossAttentionFusion(\n",
    "            text_dim=self.text_dim,\n",
    "            image_dim=self.vision_dim,\n",
    "            hidden_dim=self.fusion_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Classifier (takes concatenated attended features)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.fusion_dim * 2, 256),  # *2 because we concat text+image\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout/2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"    Intermediate Fusion initialized\")\n",
    "        print(f\"     - Attention heads: {num_heads}\")\n",
    "        print(f\"     - Hidden dim: {self.fusion_dim}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Text embedding\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_emb = text_out.last_hidden_state[:, 0, :]  # [batch, 768]\n",
    "        \n",
    "        # Image embedding\n",
    "        img_feat = self.vision_encoder(images).squeeze(-1).squeeze(-1)  # [batch, 2048]\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        attended_text, attended_image = self.cross_attention(text_emb, img_feat)\n",
    "        \n",
    "        # Concatenate attended features\n",
    "        fused = torch.cat([attended_text, attended_image], dim=1)  # [batch, 1024]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915207f",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f803e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    model_name=\"model\",\n",
    "    num_epochs=15,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    patience=5,\n",
    "    device=device):\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # History\n",
    "    history = {'train_loss': [], 'dev_f1': [], 'dev_acc': []}\n",
    "    \n",
    "    # Early stopping\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader, desc=\"Validation\", leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label']\n",
    "                \n",
    "                logits = model(input_ids, attention_mask, images)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                dev_preds.extend(preds.cpu().numpy())\n",
    "                dev_labels.extend(labels.numpy())\n",
    "        \n",
    "        dev_acc = accuracy_score(dev_labels, dev_preds)\n",
    "        dev_f1 = f1_score(dev_labels, dev_preds, average='weighted')\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['dev_f1'].append(dev_f1)\n",
    "        history['dev_acc'].append(dev_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Dev Acc: {dev_acc:.4f} | Dev F1: {dev_f1:.4f}\")\n",
    "        \n",
    "        # Check for fusion weight (Late Fusion)\n",
    "        if hasattr(model, 'get_fusion_weight'):\n",
    "            alpha = model.get_fusion_weight()\n",
    "            print(f\"Fusion weight Î±: {alpha:.4f} (text={alpha:.2f}, image={1-alpha:.2f})\")\n",
    "        \n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"  New best F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\n  Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"\\n  Training complete! Best Dev F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, device=device):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label']\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
    "        'y_true': np.array(all_labels),\n",
    "        'y_pred': np.array(all_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Early Fusion\n",
    "results = {}\n",
    "\n",
    "print(\"MODEL 1: EARLY FUSION\")\n",
    "early_model = EarlyFusionModel(\n",
    "    text_model_name=TEXT_MODEL_NAME,\n",
    "    vision_model_name=VISION_MODEL_NAME,\n",
    "    num_classes=2,\n",
    "    freeze_encoders=True)\n",
    "\n",
    "early_model, early_history = train_model(\n",
    "    early_model, train_loader, dev_loader,\n",
    "    model_name=\"Early Fusion\",\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE)\n",
    "\n",
    "early_test_results = evaluate_model(early_model, test_loader)\n",
    "results['Early Fusion'] = early_test_results\n",
    "\n",
    "torch.save(early_model.state_dict(), os.path.join(OUTPUT_DIR, 'early_fusion_best.pth'))\n",
    "print(f\"\\n  Early Fusion Test F1: {early_test_results['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Late Fusion\n",
    "print(\"MODEL 2: LATE FUSION\")\n",
    "\n",
    "late_model = LateFusionModel(\n",
    "    text_model_name=TEXT_MODEL_NAME,\n",
    "    vision_model_name=VISION_MODEL_NAME,\n",
    "    num_classes=2,\n",
    "    freeze_encoders=True,\n",
    "    learn_fusion_weight=True)\n",
    "\n",
    "late_model, late_history = train_model(\n",
    "    late_model, train_loader, dev_loader,\n",
    "    model_name=\"Late Fusion\",\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE)\n",
    "\n",
    "late_test_results = evaluate_model(late_model, test_loader)\n",
    "results['Late Fusion'] = late_test_results\n",
    "\n",
    "# We print final fusion weight\n",
    "final_alpha = late_model.get_fusion_weight()\n",
    "print(f\"\\n  Learned fusion weight: Î±={final_alpha:.4f}\")\n",
    "print(f\"   â†’ Text contribution: {final_alpha*100:.1f}%\")\n",
    "print(f\"   â†’ Image contribution: {(1-final_alpha)*100:.1f}%\")\n",
    "\n",
    "torch.save(late_model.state_dict(), os.path.join(OUTPUT_DIR, 'late_fusion_best.pth'))\n",
    "print(f\"\\nðŸ“Š Late Fusion Test F1: {late_test_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e11eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion\n",
    "print(\"MODEL 3: INTERMEDIATE FUSION (CROSS-ATTENTION)\")\n",
    "\n",
    "intermediate_model = IntermediateFusionModel(\n",
    "    text_model_name=TEXT_MODEL_NAME,\n",
    "    vision_model_name=VISION_MODEL_NAME,\n",
    "    num_classes=2,\n",
    "    freeze_encoders=True,\n",
    "    num_heads=8)\n",
    "\n",
    "intermediate_model, intermediate_history = train_model(\n",
    "    intermediate_model, train_loader, dev_loader,\n",
    "    model_name=\"Intermediate Fusion\",    \n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE)\n",
    "\n",
    "intermediate_test_results = evaluate_model(intermediate_model, test_loader)\n",
    "results['Intermediate Fusion'] = intermediate_test_results\n",
    "\n",
    "torch.save(intermediate_model.state_dict(), os.path.join(OUTPUT_DIR, 'intermediate_fusion_best.pth'))\n",
    "print(f\"\\n  Intermediate Fusion Test F1: {intermediate_test_results['f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
