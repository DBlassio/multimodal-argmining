{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3af112",
   "metadata": {},
   "source": [
    "\n",
    "### Contrastive Persuasiveness Scoring\n",
    "\n",
    "This notebook explores the development of a continuous persuasiveness socrer [0,1] that measures semantic alignment between image and text (tweets). Our dataset includes tweets paired with images, but thoses images can be persuasive or not (support the stance of the final argument of the tweet), our idea is that instead of using all images (with their text) we are going to filter/weight images based on their persuasiveness and then pass it to the final model. In this way, the model will only receive usefull information instead of images that could just add noise (ex. an image full white or ambigious)\n",
    "\n",
    "Research Questions:\n",
    "  1. Can CLIP zero-shot predict persuasiveness without training?\n",
    "  2. Does fine-tuning CLIP improve persuasiveness detection?\n",
    "  3. Does BLIP-2 outperform CLIP for this task?\n",
    "  4. What threshold/weighting should we use in multimodal fusion?\n",
    "\n",
    "Output:\n",
    "  - Persuasiveness score [0, 1] for each (image, text) pair\n",
    "  - Correlation analysis with ground truth labels\n",
    "  - Conclusion and recommendation for multimodal fusion strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f3ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  42\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score,precision_recall_curve, roc_curve,f1_score, accuracy_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer,Blip2Processor, Blip2Model,get_cosine_schedule_with_warmup\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Seed:  {SEED}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d95fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train label distribution:\n",
      "\n",
      " Stance: \n",
      " Oppose: 1095\n",
      " Support: 719\n",
      "\n",
      "\n",
      "  Persuasiveness \n",
      " No: 1285\n",
      " Yes: 529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stance</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>persuasiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148501065308004357</td>\n",
       "      <td>https://t.co/VQP1FHaWAg</td>\n",
       "      <td>Let's McGyver some Sanity in America!\\n\\nYou a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1103872992537276417</td>\n",
       "      <td>https://t.co/zsyXYSeBkp</td>\n",
       "      <td>A child deserves a chance at life. A child des...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151528583623585794</td>\n",
       "      <td>https://t.co/qSWvDX5MnM</td>\n",
       "      <td>Dear prolifers: girls as young as 10, 11, 12 a...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100166844026109953</td>\n",
       "      <td>https://t.co/hxH8tFIHUu</td>\n",
       "      <td>The many States will attempt to amend their co...</td>\n",
       "      <td>support</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021830413550067713</td>\n",
       "      <td>https://t.co/5whvEEtoQR</td>\n",
       "      <td>Every #abortion is wrong, no matter what metho...</td>\n",
       "      <td>oppose</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                tweet_url  \\\n",
       "0  1148501065308004357  https://t.co/VQP1FHaWAg   \n",
       "1  1103872992537276417  https://t.co/zsyXYSeBkp   \n",
       "2  1151528583623585794  https://t.co/qSWvDX5MnM   \n",
       "3  1100166844026109953  https://t.co/hxH8tFIHUu   \n",
       "4  1021830413550067713  https://t.co/5whvEEtoQR   \n",
       "\n",
       "                                          tweet_text   stance persuasiveness  \\\n",
       "0  Let's McGyver some Sanity in America!\\n\\nYou a...  support             no   \n",
       "1  A child deserves a chance at life. A child des...   oppose             no   \n",
       "2  Dear prolifers: girls as young as 10, 11, 12 a...  support             no   \n",
       "3  The many States will attempt to amend their co...  support             no   \n",
       "4  Every #abortion is wrong, no matter what metho...   oppose            yes   \n",
       "\n",
       "   split  label  persuasiveness_label  \n",
       "0  train      1                     0  \n",
       "1  train      0                     0  \n",
       "2  train      1                     0  \n",
       "3  train      1                     0  \n",
       "4  train      0                     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"../../../data/\"\n",
    "IMG_PATH = \"../../../data/images\"\n",
    "OUTPUT_DIR = \"../../../experiments/vision/contrastive_persuasiveness/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(DATA_PATH,\"train.csv\")\n",
    "dev_path   = os.path.join(DATA_PATH,\"dev.csv\")\n",
    "test_path  = os.path.join(DATA_PATH,\"test.csv\")\n",
    "\n",
    "#Load Data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Map labels to ints\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "pers_2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "    df[\"persuasiveness_label\"] = df[\"persuasiveness\"].map(pers_2id)\n",
    "\n",
    "\n",
    "print(f\"\\n Train label distribution:\")\n",
    "print(f\"\\n Stance: \\n Oppose: {(df_train['label']==0).sum()}\\n Support: {(df_train['label']==1).sum()}\")\n",
    "print(f\"\\n\\n  Persuasiveness \\n No: {(df_train['persuasiveness_label']==0).sum()}\\n Yes: {(df_train['persuasiveness_label']==1).sum()}\")\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a0c82",
   "metadata": {},
   "source": [
    "### Multimodal Models\n",
    "\n",
    "Why these models?\n",
    "\n",
    "1. CLIP (Contrastive Language–Image Pretraining)\n",
    "- Architecture: Dual-encoder model (ViT + Text Transformer) trained with a large-scale contrastive loss.\n",
    "- Training: Learned to align images and text by maximizing similarity of paired samples.\n",
    "- Relevance: It provides a strong baseline for pure semantic similarity. It's useful for measuring alignment and detect relevant/irrelevant images based on context.\n",
    "\n",
    "\n",
    "2. BLIP-2 (Bootstrapped Language–Image Pretraining)\n",
    "- Architecture: Vision encoder + Q-Former + Large Language Model (Flan-T5/OPT) that bridges visual features into a reasoning-capable LLM.\n",
    "- Training: Combines contrastive pretraining with instruction tuning, enabling deeper multimodal understanding.\n",
    "- Relevance: Captures fine-grained semantic support, reasoning about whether an image reinforces the meaning or stance of the tweet. This is critical for estimating persuasiveness beyond simple similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2990c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = \"openai/clip-vit-base-patch32\"\n",
    "BLIP2 = \"Salesforce/blip2-flan-t5-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489446a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Hyperparameters and setup\n",
    "BATCH_SIZE = 16  \n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE =  5e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "PATIENCE = 3  \n",
    "NUM_WORKERS = 1  \n",
    "\n",
    "# Contrastive loss\n",
    "TEMPERATURE = 0.07  \n",
    "\n",
    "# Thresholds\n",
    "SIMILARITY_THRESHOLDS = np.arange(0.0, 1.0, 0.05)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We  define our Dataset class\n",
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for contrastive learning that returns (image, text, persuasiveness_label).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dataframe: pd.DataFrame,image_dir: str,processor, tokenizer=None,model_type: str = 'clip'):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, f\"{row['tweet_id']}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "        \n",
    "        # Get text\n",
    "        text = str(row['tweet_text'])\n",
    "        \n",
    "        # Get labels\n",
    "        persuasiveness = row['persuasiveness_label']\n",
    "        stance = row['label']\n",
    "        \n",
    "        # Process based on model type (CLIP or BIP-2)\n",
    "        if self.model_type == 'clip':\n",
    "            # CLIP preprocessing\n",
    "            inputs = self.processor(text=text,images=image,return_tensors=\"pt\",padding=True,truncation=True,max_length=77)\n",
    "            \n",
    "            return {'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'persuasiveness': torch.tensor(persuasiveness, dtype=torch.float32),\n",
    "                'stance': torch.tensor(stance, dtype=torch.long),\n",
    "                'text': text,\n",
    "                'tweet_id': row['tweet_id']}\n",
    "        \n",
    "        elif self.model_type == 'blip2':\n",
    "            # BLIP-2 preprocessing\n",
    "            inputs = self.processor(images=image,text=text,return_tensors=\"pt\",padding=True,truncation=True)\n",
    "            \n",
    "            return {'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "                'input_ids': inputs['input_ids'].squeeze(0) if 'input_ids' in inputs else None,\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0) if 'attention_mask' in inputs else None,\n",
    "                'persuasiveness': torch.tensor(persuasiveness, dtype=torch.float32),\n",
    "                'stance': torch.tensor(stance, dtype=torch.long),\n",
    "                'text': text,\n",
    "                'tweet_id': row['tweet_id']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dbf187",
   "metadata": {},
   "source": [
    "### CLIP Zero-Shot (Baseline)\n",
    "\n",
    "General Strategy: We use CLIP (No training) encode images and texts, and compute cosine similarity between embeddings. Use that similarity as persuasiveness socre [0,1] and evaluate: High similarity -> persuasiveness=1 (yes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9cbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(CLIP).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP)\n",
    "print(f\"Clip model loaded: {CLIP}\")\n",
    "\n",
    "# We create our dataset\n",
    "clip_test_dataset = ImageTextDataset(df_test, IMG_PATH, clip_processor, model_type='clip')\n",
    "clip_test_loader = DataLoader(clip_test_dataset, IMG_PATH, clip_processor, model_type='clip')\n",
    "\n",
    "# Function to compute CLIP similarities\n",
    "def compute_clip_similarities(model, dataloader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_similarities = []\n",
    "    all_labels = []\n",
    "    all_texts = []\n",
    "    all_tweet_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing CLIP similarities\"):\n",
    "            # Move to device\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Get visual embeddings\n",
    "            vision_outputs = model.vision_model(pixel_values=pixel_values)\n",
    "            image_embeds = vision_outputs.pooler_output  # [batch, hidden_dim]\n",
    "\n",
    "            # Get text embeddings\n",
    "            text_outputs = model.text_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            text_embeds = text_outputs.pooler_output # [batch, hidden_dim]\n",
    "            \n",
    "            # Project them to joint space\n",
    "            image_embeds = model.visual_projection(image_embeds)  # [batch, 512]\n",
    "            text_embeds = model.text_projection(text_embeds)      # [batch, 512]\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_embeds = F.normalize(image_embeds, p=2, dim=1)\n",
    "            text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "            \n",
    "            # Compute cosine similarity!\n",
    "            similarities = (image_embeds * text_embeds).sum(dim=1)  # [batch]\n",
    "            \n",
    "            # Since similarities are in [-1, 1], we convert them to [0,1] summing 1d dividing by 2\n",
    "            similarities = (similarities + 1) / 2\n",
    "            \n",
    "            all_similarities.extend(similarities.cpu().numpy())\n",
    "            all_labels.extend(batch['persuasiveness'].numpy())\n",
    "            all_texts.extend(batch['text'])\n",
    "            all_tweet_ids.extend(batch['tweet_id'])\n",
    "    \n",
    "    return (np.array(all_similarities), # similarities 0,1\n",
    "            np.array(all_labels), # persuasiveness labels\n",
    "            all_texts, # tweets\n",
    "            all_tweet_ids) # tweet ids\n",
    "\n",
    "# Compute similarities\n",
    "print(\"\\n Computing CLIP (zero-shot) similarities on TEST set...\")\n",
    "similarities, labels, texts, tweet_ids = compute_clip_similarities(clip_model,clip_test_loader,device)\n",
    "\n",
    "print(f\"\\n Computed {len(similarities)} similarity scores\")\n",
    "print(f\"  Mean similarity: {similarities.mean():.4f}\")\n",
    "print(f\"  Std similarity: {similarities.std():.4f}\")\n",
    "print(f\"  Min similarity: {similarities.min():.4f}\")\n",
    "print(f\"  Max similarity: {similarities.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate CLIP zero-shot as a persuasiveness predictor\n",
    "\n",
    "# continious scores\n",
    "auroc = roc_auc_score(labels, similarities)\n",
    "ap = average_precision_score(labels, similarities) \n",
    "\n",
    "# Pearson and Spearman correlation\n",
    "pearson_corr, pearson_p = pearsonr(similarities, labels)\n",
    "spearman_corr, spearman_p = spearmanr(similarities, labels)\n",
    "\n",
    "print(f\"\\n CLIP Zero-Shot Performance:\")\n",
    "print(f\"  AUROC: {auroc:.4f}\")\n",
    "print(f\"  Average Precision: {ap:.4f}\")\n",
    "print(f\"  Pearson correlation: {pearson_corr:.4f} (p={pearson_p:.2e})\")\n",
    "print(f\"  Spearman correlation: {spearman_corr:.4f} (p={spearman_p:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a670996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now find the optimal threshold for binary classification (persuasiveness yes/no)\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in SIMILARITY_THRESHOLDS:\n",
    "    predictions = (similarities >= threshold).astype(int)\n",
    "    f1 = f1_score(labels, predictions, zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    threshold_results.append({'threshold': threshold,'f1': f1,'accuracy': acc})\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n Optimal threshold: {best_threshold:.2f}\")\n",
    "print(f\"   F1 Score: {best_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1794c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict with our optimal threshold\n",
    "optimal_preds = (similarities >= best_threshold).astype(int)\n",
    "optimal_acc = accuracy_score(labels, optimal_preds)\n",
    "\n",
    "print(f\"\\n CLIP Zero-Shot Performance (binary with threshold={best_threshold:.2f}):\")\n",
    "print(f\"  Accuracy: {optimal_acc:.4f}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Similarity distribution by label\n",
    "axes[0, 0].hist(similarities[labels == 0], bins=30, alpha=0.6, label='Not Persuasive', color='red')\n",
    "axes[0, 0].hist(similarities[labels == 1], bins=30, alpha=0.6, label='Persuasive', color='green')\n",
    "axes[0, 0].axvline(best_threshold, color='black', linestyle='--', label=f'Optimal threshold: {best_threshold:.2f}')\n",
    "axes[0, 0].set_xlabel('CLIP Similarity Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('CLIP Similarity Distribution by Label', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(labels, similarities)\n",
    "axes[0, 1].plot(fpr, tpr, label=f'CLIP (AUROC={auroc:.3f})', linewidth=2)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(labels, similarities)\n",
    "axes[1, 0].plot(recall, precision, linewidth=2, label=f'CLIP (AP={ap:.3f})')\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Precision-Recall Curve', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# F1 vs Threshold\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "axes[1, 1].plot(threshold_df['threshold'], threshold_df['f1'], marker='o', label='F1 Score')\n",
    "axes[1, 1].axvline(best_threshold, color='red', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "axes[1, 1].axhline(best_f1, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Similarity Threshold')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('F1 Score vs Threshold', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'clip_zeroshot_evaluation.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({'tweet_id': tweet_ids,'text': texts,'similarity_score': similarities,'persuasiveness_true': labels,'persuasiveness_pred': optimal_preds})\n",
    "results_df.to_csv(os.path.join(OUTPUT_DIR, 'clip_zeroshot_predictions.csv'), index=False)\n",
    "print(f\"Predictions saved in {OUTPUT_DIR}: clip_zeroshot_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f23ca",
   "metadata": {},
   "source": [
    "### CLIP Fine-Tuned with Contrastive Loss\n",
    "\n",
    "General idea: We pull together embeddings of persuasive (image, text), and we push apart embeddings of non-persuasive pairs. For persuasive pairs: maximize cosine similarity, for non-persuasive pairs: minimize cosine similarity. This should improve the previous CLIP Score because it's align to our specific domain (Gun Control / Abortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPPersuasivenessModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            clip_model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "            freeze_encoders: bool = False,\n",
    "            use_mlp_head: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Load CLIP\n",
    "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
    "        \n",
    "        # Optionally freeze encoders\n",
    "        if freeze_encoders:\n",
    "            for param in self.clip.vision_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.clip.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"CLIP encoders frozen\")\n",
    "        \n",
    "        self.use_mlp_head = use_mlp_head\n",
    "        \n",
    "        # Optional: Add MLP head on top of similarity\n",
    "        if use_mlp_head:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(512 + 512 + 1, 256),  # concat(img_emb, txt_emb, similarity)\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid())\n",
    "        \n",
    "        print(f\"    CLIP Persuasiveness Model initialized\")\n",
    "        print(f\"     - Freeze encoders: {freeze_encoders}\")\n",
    "        print(f\"     - Use MLP head: {use_mlp_head}\")\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "\n",
    "        # images embeddings\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = vision_outputs.pooler_output\n",
    "        \n",
    "        # text embeddings \n",
    "        text_outputs = self.clip.text_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.pooler_output\n",
    "        \n",
    "        # Project\n",
    "        image_embeds = self.clip.visual_projection(image_embeds)\n",
    "        text_embeds = self.clip.text_projection(text_embeds)\n",
    "        \n",
    "        # Normalize\n",
    "        image_embeds = F.normalize(image_embeds, p=2, dim=1)\n",
    "        text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = (image_embeds * text_embeds).sum(dim=1)\n",
    "        similarity_01 = (similarity + 1) / 2 \n",
    "        \n",
    "        # If we use MLP head, we concatenate features\n",
    "        if self.use_mlp_head:\n",
    "            features = torch.cat([image_embeds,text_embeds,similarity_01.unsqueeze(1)], dim=1)\n",
    "            persuasiveness_score = self.head(features).squeeze(1)\n",
    "            return similarity_01, persuasiveness_score\n",
    "        else:\n",
    "            return similarity_01, similarity_01\n",
    "\n",
    "def contrastive_persuasiveness_loss(similarity, labels, temperature=0.07):\n",
    "\n",
    "    # For persuasive pairs: maximize similarity (minimize negative similarity)\n",
    "    # For non-persuasive pairs: minimize similarity (maximize negative similarity)\n",
    "    \n",
    "    # Convert similarity to logits\n",
    "    logits = similarity / temperature\n",
    "    \n",
    "    # BCE-like loss\n",
    "    # If label=1 (persuasive): want similarity → 1\n",
    "    # If label=0 (not persuasive): want similarity → 0\n",
    "    \n",
    "    # Convert similarity from [-1, 1] to [0, 1]\n",
    "    similarity_01 = (similarity + 1) / 2\n",
    "    \n",
    "    # Binary cross entropy\n",
    "    loss = F.binary_cross_entropy(similarity_01, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_clip_persuasiveness(model,train_loader,dev_loader,num_epochs=10,learning_rate=5e-6,weight_decay=1e-4,\n",
    "    device=device):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=num_warmup_steps,num_training_steps=num_training_steps)\n",
    "    \n",
    "    best_auroc = 0\n",
    "    best_model_state = None\n",
    "    history = {'train_loss': [],\n",
    "                'dev_auroc': [],\n",
    "                  'dev_ap': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in pbar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['persuasiveness'].to(device)\n",
    "            \n",
    "            # Forward\n",
    "            similarity, pred_score = model(pixel_values, input_ids, attention_mask)\n",
    "            \n",
    "            # Loss\n",
    "            loss = F.binary_cross_entropy(pred_score, labels)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item() * pixel_values.size(0)\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_scores = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader, desc=\"Validation\", leave=False):\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['persuasiveness']\n",
    "                \n",
    "                _, pred_score = model(pixel_values, input_ids, attention_mask)\n",
    "                \n",
    "                all_scores.extend(pred_score.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        all_scores = np.array(all_scores)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        auroc = roc_auc_score(all_labels, all_scores)\n",
    "        ap = average_precision_score(all_labels, all_scores)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['dev_auroc'].append(auroc)\n",
    "        history['dev_ap'].append(ap)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Dev AUROC: {auroc:.4f} | Dev AP: {ap:.4f}\")\n",
    "        \n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            print(f\" New best AUROC: {best_auroc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"\\n Training complete! Best Dev AUROC: {best_auroc:.4f}\")\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a682605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "clip_train_dataset = ImageTextDataset(df_train, IMG_PATH, clip_processor, model_type='clip')\n",
    "clip_dev_dataset = ImageTextDataset(df_dev, IMG_PATH, clip_processor, model_type='clip')\n",
    "\n",
    "clip_train_loader = DataLoader(clip_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "clip_dev_loader = DataLoader(clip_dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "clip_finetuned_model = CLIPPersuasivenessModel(clip_model_name=clip_model,freeze_encoders=False)\n",
    "\n",
    "# Train\n",
    "clip_finetuned_model, clip_history = train_clip_persuasiveness(\n",
    "    clip_finetuned_model,\n",
    "    clip_train_loader,\n",
    "    clip_dev_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    device=device)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n Evaluating CLIP Fine-tuned on TEST set...\")\n",
    "clip_finetuned_model.eval()\n",
    "finetuned_scores = []\n",
    "finetuned_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(clip_test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        _, pred_score = clip_finetuned_model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        finetuned_scores.extend(pred_score.cpu().numpy())\n",
    "        finetuned_labels.extend(batch['persuasiveness'].numpy())\n",
    "\n",
    "finetuned_scores = np.array(finetuned_scores)\n",
    "finetuned_labels = np.array(finetuned_labels)\n",
    "\n",
    "# Metrics\n",
    "finetuned_auroc = roc_auc_score(finetuned_labels, finetuned_scores)\n",
    "finetuned_ap = average_precision_score(finetuned_labels, finetuned_scores)\n",
    "\n",
    "print(\"\\n\\n\\nCLIP FINE-TUNED TEST RESULTS\")\n",
    "print(f\"AUROC: {finetuned_auroc:.4f}\")\n",
    "print(f\"Average Precision: {finetuned_ap:.4f}\")\n",
    "print(f\"\\nComparison with Zero-Shot:\")\n",
    "print(f\"  Zero-Shot AUROC: {auroc:.4f}\")\n",
    "print(f\"  Fine-tuned AUROC: {finetuned_auroc:.4f}\")\n",
    "print(f\"  Improvement: {(finetuned_auroc - auroc):.4f} ({((finetuned_auroc/auroc - 1)*100):.1f}%)\")\n",
    "\n",
    "# Save model\n",
    "torch.save(clip_finetuned_model.state_dict(), \n",
    "           os.path.join(OUTPUT_DIR, 'clip_finetuned_persuasiveness.pth'))\n",
    "print(f\"model saved in {OUTPUT_DIR}: clip_finetuned_persuasiveness.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937f1c7",
   "metadata": {},
   "source": [
    "### BLIP-2 Fine-Tuned\n",
    "\n",
    "General idea: We'll fine-tune BLIP-2 for persuasiveness scoring, same as CLIP but BLIP it's more recent than CLIP (2023), uses Q-former ( with cross-Attention, so both embeddings learn from each other and from themselves) to bridge vision and language, it's supposely better at complex visual reasoning and since we have memes could perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_processor = Blip2Processor.from_pretrained(BLIP2)\n",
    "blip2_model = Blip2Model.from_pretrained(BLIP2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate our Blip-2 Model  Architecture\n",
    "class BLIP2PersuasivenessModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, blip2_model):\n",
    "        super().__init__()\n",
    "        self.blip2 = blip2_model\n",
    "        \n",
    "        # Freeze most of BLIP-2 (it's large)\n",
    "        for param in self.blip2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Small trainable head\n",
    "        hidden_dim = self.blip2.config.qformer_config.hidden_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid())\n",
    "        print(\"Only training Q-Former and classification head\")\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids=None, attention_mask=None):\n",
    "        # Get Q-Former outputs\n",
    "        outputs = self.blip2(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True)\n",
    "        \n",
    "        # Use Q-Former outputs\n",
    "        qformer_outputs = outputs.qformer_outputs\n",
    "        pooled = qformer_outputs.pooler_output  # [batch, hidden_dim]\n",
    "        \n",
    "        # Predict\n",
    "        score = self.head(pooled).squeeze(1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e104cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We train BLIP-2 for persuasiveness prediction\n",
    "\n",
    "# Create BLIP-2 datasets\n",
    "blip2_train_dataset = ImageTextDataset(df_train,IMG_PATH, blip2_processor, model_type='blip2')\n",
    "blip2_dev_dataset = ImageTextDataset(df_dev,IMG_PATH, blip2_processor, model_type='blip2')\n",
    "blip2_test_dataset = ImageTextDataset(df_test,IMG_PATH, blip2_processor, model_type='blip2')\n",
    "\n",
    "blip2_train_loader = DataLoader(blip2_train_dataset, batch_size=8, shuffle=True, num_workers=2) \n",
    "blip2_dev_loader = DataLoader(blip2_dev_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "blip2_test_loader = DataLoader(blip2_test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize\n",
    "blip2_pers_model = BLIP2PersuasivenessModel(blip2_model)\n",
    "\n",
    "# Train \n",
    "blip2_pers_model = blip2_pers_model.to(device)\n",
    "optimizer = torch.optim.AdamW(blip2_pers_model.head.parameters(), lr=1e-4)\n",
    "\n",
    "best_blip2_auroc = 0\n",
    "for epoch in range(5):\n",
    "    print(f\"\\nEpoch {epoch+1}/5\")\n",
    "    \n",
    "    blip2_pers_model.train()\n",
    "    for batch in tqdm(blip2_train_loader, desc=\"Training\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['persuasiveness'].to(device)\n",
    "        \n",
    "        pred = blip2_pers_model(pixel_values)\n",
    "        loss = F.binary_cross_entropy(pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate\n",
    "    blip2_pers_model.eval()\n",
    "    blip2_dev_scores = []\n",
    "    blip2_dev_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(blip2_dev_loader, desc=\"Validation\", leave=False):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            pred = blip2_pers_model(pixel_values)\n",
    "            \n",
    "            blip2_dev_scores.extend(pred.cpu().numpy())\n",
    "            blip2_dev_labels.extend(batch['persuasiveness'].numpy())\n",
    "    \n",
    "    dev_auroc = roc_auc_score(blip2_dev_labels, blip2_dev_scores)\n",
    "    print(f\"Dev AUROC: {dev_auroc:.4f}\")\n",
    "    \n",
    "    if dev_auroc > best_blip2_auroc:\n",
    "        best_blip2_auroc = dev_auroc\n",
    "        torch.save(blip2_pers_model.state_dict(), \n",
    "                   os.path.join(OUTPUT_DIR, 'blip2_persuasiveness.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test BLIP-2 model\n",
    "\n",
    "blip2_pers_model.eval()\n",
    "blip2_test_scores = []\n",
    "blip2_test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(blip2_test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        pred = blip2_pers_model(pixel_values)\n",
    "        \n",
    "        blip2_test_scores.extend(pred.cpu().numpy())\n",
    "        blip2_test_labels.extend(batch['persuasiveness'].numpy())\n",
    "\n",
    "blip2_test_scores = np.array(blip2_test_scores)\n",
    "blip2_test_labels = np.array(blip2_test_labels)\n",
    "\n",
    "blip2_auroc = roc_auc_score(blip2_test_labels, blip2_test_scores)\n",
    "blip2_ap = average_precision_score(blip2_test_labels, blip2_test_scores)\n",
    "\n",
    "print(\"\\n\\n\\n BLIP-2 TEST RESULTS\")\n",
    "print(f\"AUROC: {blip2_auroc:.4f}\")\n",
    "print(f\"Average Precision: {blip2_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparision between models\n",
    "\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['CLIP Zero-Shot', 'CLIP Fine-tuned', 'BLIP-2 Fine-tuned'],\n",
    "    'AUROC': [auroc, finetuned_auroc, blip2_auroc],\n",
    "    'Average Precision': [ap, finetuned_ap, blip2_ap],\n",
    "    'Training Time': ['0 (no training)', '~30 min', '~45 min'],\n",
    "    'Inference Speed': ['Fast', 'Fast', 'Slow']})\n",
    "\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUROC comparison\n",
    "axes[0].bar(comparison['Model'], comparison['AUROC'], color = [\"blue\", \"orange\", \"green\"])\n",
    "axes[0].set_title('AUROC Comparison', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('AUROC')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison.iterrows():\n",
    "    axes[0].text(i, row['AUROC'] + 0.02, f\"{row['AUROC']:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# AP comparison\n",
    "axes[1].bar(comparison['Model'], comparison['Average Precision'], color =[\"blue\", \"orange\", \"green\"])\n",
    "axes[1].set_title('Average Precision Comparison', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Average Precision')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison.iterrows():\n",
    "    axes[1].text(i, row['Average Precision'] + 0.02, f\"{row['Average Precision']:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'final_comparison.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15581739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Concluson: Best Model\n",
    "best_model = comparison.loc[comparison['AUROC'].idxmax(), 'Model']\n",
    "best_auroc = comparison['AUROC'].max()\n",
    "\n",
    "print(f\"\\n Best Model: {best_model} with AUROC={best_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810055b",
   "metadata": {},
   "source": [
    "KEY FINDINGS:\n",
    "1. CLIP zero-shot provides decent baseline (AUROC={auroc:.3f})\n",
    "2. Fine-tuning improves performance by {((finetuned_auroc/auroc - 1)*100):.1f}%\n",
    "3. {best_model} is recommended for multimodal fusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
