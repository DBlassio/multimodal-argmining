{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed Search: DeBERTa + ALIGN + Caption BLIP2 + proj_concat\n",
    "\n",
    "## Objetivo:\n",
    "Encontrar la seed que maximiza el **Test F1-Score** para el modelo ganador.\n",
    "El modelo seleccionado ser√° usado como baseline para el Error Analysis.\n",
    "\n",
    "## Configuraci√≥n:\n",
    "- **Modelo:** DeBERTa + ALIGN + Caption BLIP2 + Fusion: proj_concat\n",
    "- **Seeds:** 10 seeds distintas\n",
    "- **Criterio de selecci√≥n:** Mejor Test F1-Score\n",
    "- **Output:** Checkpoint del mejor modelo + tabla completa de resultados\n",
    "\n",
    "**Nota:** Todas las m√©tricas de todas las seeds se guardan para an√°lisis de varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    AlignProcessor, AlignModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH  = \"../../data/\"\n",
    "IMG_PATH   = \"../../data/images\"\n",
    "OUTPUT_DIR = \"../../results/multimodal/seed_search/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config (winner from previous experiments)\n",
    "TEXT_MODEL_NAME  = \"microsoft/deberta-v3-base\"\n",
    "ALIGN_MODEL_NAME = \"kakaobrain/align-base\"\n",
    "CAPTION_COL      = \"caption_blip2\"\n",
    "FUSION_TYPE      = \"proj_concat\"\n",
    "\n",
    "# Training hyperparameters (same as all previous experiments)\n",
    "MAX_TEXT_LENGTH  = 128\n",
    "COMMON_DIM       = 768\n",
    "NUM_CLASSES      = 2\n",
    "BATCH_SIZE       = 16\n",
    "NUM_EPOCHS       = 15\n",
    "LEARNING_RATE    = 2e-5\n",
    "WEIGHT_DECAY     = 1e-4\n",
    "WARMUP_RATIO     = 0.1\n",
    "PATIENCE         = 5\n",
    "\n",
    "# Seeds to try\n",
    "SEEDS = [42, 123, 456, 789, 1024, 2024, 3090, 7777, 8888, 9999]\n",
    "\n",
    "print(f\"Model: DeBERTa + ALIGN + Caption BLIP2 + {FUSION_TYPE}\")\n",
    "print(f\"Seeds to run: {SEEDS}\")\n",
    "print(f\"Total runs: {len(SEEDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set all random seeds for full reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "    os.environ['PYTHONHASHSEED']       = str(seed)\n",
    "\n",
    "print(\"‚úì set_seed() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_PATH, \"train_with_captions.csv\"))\n",
    "df_dev   = pd.read_csv(os.path.join(DATA_PATH, \"dev_with_captions.csv\"))\n",
    "df_test  = pd.read_csv(os.path.join(DATA_PATH, \"test_with_captions.csv\"))\n",
    "\n",
    "# Map labels\n",
    "stance_2id = {\"oppose\": 0, \"support\": 1}\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(stance_2id)\n",
    "\n",
    "print(f\"Train: {len(df_train)} | Dev: {len(df_dev)} | Test: {len(df_test)}\")\n",
    "print(f\"\\nTrain: {df_train['stance'].value_counts().to_dict()}\")\n",
    "print(f\"Dev:   {df_dev['stance'].value_counts().to_dict()}\")\n",
    "print(f\"Test:  {df_test['stance'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Text + Caption + Image dataset.\n",
    "    text_input = tweet_text [SEP] caption\n",
    "    \"\"\"\n",
    "    def __init__(self, df, img_dir, tokenizer, image_processor, caption_col, max_length=128):\n",
    "        self.df              = df.reset_index(drop=True)\n",
    "        self.img_dir         = img_dir\n",
    "        self.tokenizer       = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.caption_col     = caption_col\n",
    "        self.max_length      = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row     = self.df.iloc[idx]\n",
    "        text    = str(row['tweet_text'])\n",
    "        caption = str(row[self.caption_col])\n",
    "\n",
    "        text_enc = self.tokenizer(\n",
    "            f\"{text} [SEP] {caption}\",\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, str(row['tweet_id']) + \".jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n",
    "\n",
    "        img_enc = self.image_processor(images=image, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids':      text_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_enc['attention_mask'].squeeze(0),\n",
    "            'pixel_values':   img_enc['pixel_values'].squeeze(0),\n",
    "            'label':          torch.tensor(row['label'], dtype=torch.long),\n",
    "            'idx':            torch.tensor(idx, dtype=torch.long)  # For error analysis\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids':      torch.stack([b['input_ids']      for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'pixel_values':   torch.stack([b['pixel_values']   for b in batch]),\n",
    "        'labels':         torch.stack([b['label']          for b in batch]),\n",
    "        'indices':        torch.stack([b['idx']            for b in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DeBERTa (text+caption) + ALIGN vision encoder + proj_concat fusion.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name   = \"microsoft/deberta-v3-base\",\n",
    "        vision_model_name = \"kakaobrain/align-base\",\n",
    "        num_classes       = 2,\n",
    "        fusion_type       = \"proj_concat\",\n",
    "        common_dim        = 768,\n",
    "        dropout           = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fusion_type = fusion_type\n",
    "        self.common_dim  = common_dim\n",
    "\n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_dim     = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # Vision encoder (ALIGN)\n",
    "        align_full          = AlignModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder = align_full.vision_model\n",
    "        self.vision_dim     = align_full.config.vision_config.hidden_size\n",
    "\n",
    "        # Projections to common_dim\n",
    "        self.text_projection   = nn.Linear(self.text_dim, common_dim)   if self.text_dim   != common_dim else nn.Identity()\n",
    "        self.vision_projection = nn.Linear(self.vision_dim, common_dim) if self.vision_dim != common_dim else nn.Identity()\n",
    "\n",
    "        # proj_concat fusion\n",
    "        proj_dim = common_dim // 2\n",
    "        self.text_proj    = nn.Linear(common_dim, proj_dim)\n",
    "        self.vision_proj  = nn.Linear(common_dim, proj_dim)\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(proj_dim * 2, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(common_dim, common_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(common_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_emb   = self.text_projection(\n",
    "            self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :])\n",
    "        vision_emb = self.vision_projection(\n",
    "            self.vision_encoder(pixel_values=pixel_values).pooler_output)\n",
    "\n",
    "        fused = self.fusion_layer(\n",
    "            torch.cat([self.text_proj(text_emb), self.vision_proj(vision_emb)], dim=1))\n",
    "\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_seed(model, train_loader, dev_loader, save_path, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Train model and return best dev F1 + best model state.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer    = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps  = len(train_loader) * NUM_EPOCHS\n",
    "    scheduler    = get_linear_schedule_with_warmup(\n",
    "        optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
    "    criterion    = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_f1      = 0.0\n",
    "    patience_counter = 0\n",
    "    best_state       = None\n",
    "    history          = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"    Epoch {epoch+1:02d}/{NUM_EPOCHS}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['pixel_values'].to(device)\n",
    "            )\n",
    "            loss = criterion(logits, batch['labels'].to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_loader:\n",
    "                logits = model(\n",
    "                    batch['input_ids'].to(device),\n",
    "                    batch['attention_mask'].to(device),\n",
    "                    batch['pixel_values'].to(device)\n",
    "                )\n",
    "                all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "        _, _, dev_f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        history.append({'epoch': epoch+1, 'train_loss': avg_loss, 'dev_f1': dev_f1})\n",
    "\n",
    "        print(f\"    Epoch {epoch+1:02d} | Loss: {avg_loss:.4f} | Dev F1: {dev_f1:.4f}\", end=\"\")\n",
    "\n",
    "        if dev_f1 > best_dev_f1:\n",
    "            best_dev_f1 = dev_f1\n",
    "            patience_counter = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\" ‚úì\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience {patience_counter}/{PATIENCE})\")\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_dev_f1, pd.DataFrame(history)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Full evaluation: returns metrics + per-sample predictions and probabilities.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels, all_indices = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            logits = model(\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['pixel_values'].to(device)\n",
    "            )\n",
    "            probs  = F.softmax(logits, dim=1)\n",
    "            preds  = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].numpy())\n",
    "            all_indices.extend(batch['indices'].numpy())\n",
    "\n",
    "    all_preds   = np.array(all_preds)\n",
    "    all_probs   = np.array(all_probs)\n",
    "    all_labels  = np.array(all_labels)\n",
    "    all_indices = np.array(all_indices)\n",
    "\n",
    "    accuracy              = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', pos_label=1, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy':   accuracy,\n",
    "        'f1':         f1,\n",
    "        'precision':  precision,\n",
    "        'recall':     recall,\n",
    "        'cm':         cm,\n",
    "        'preds':      all_preds,\n",
    "        'probs':      all_probs,\n",
    "        'labels':     all_labels,\n",
    "        'indices':    all_indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer       = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "align_processor = AlignProcessor.from_pretrained(ALIGN_MODEL_NAME)\n",
    "\n",
    "# Build datasets ONCE (same data for all seeds, only model changes)\n",
    "train_ds = MultimodalCaptionDataset(df_train, IMG_PATH, tokenizer, align_processor, CAPTION_COL, MAX_TEXT_LENGTH)\n",
    "dev_ds   = MultimodalCaptionDataset(df_dev,   IMG_PATH, tokenizer, align_processor, CAPTION_COL, MAX_TEXT_LENGTH)\n",
    "test_ds  = MultimodalCaptionDataset(df_test,  IMG_PATH, tokenizer, align_processor, CAPTION_COL, MAX_TEXT_LENGTH)\n",
    "\n",
    "print(f\"‚úì Tokenizer and processor loaded\")\n",
    "print(f\"‚úì Datasets created: train={len(train_ds)} | dev={len(dev_ds)} | test={len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seed Search Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seed_results = []  # One row per seed\n",
    "all_histories    = {}  # Training curves per seed\n",
    "\n",
    "# Track global best by Test F1\n",
    "best_test_f1     = 0.0\n",
    "best_seed        = None\n",
    "best_test_eval   = None  # Full evaluation dict of best model\n",
    "best_model_path  = os.path.join(OUTPUT_DIR, \"best_model_overall.pt\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"SEED SEARCH: {len(SEEDS)} runs\")\n",
    "print(f\"Model: DeBERTa + ALIGN + Caption BLIP2 + {FUSION_TYPE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for run_idx, seed in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUN {run_idx}/{len(SEEDS)} | Seed: {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Set seed BEFORE everything\n",
    "    set_seed(seed)\n",
    "\n",
    "    # DataLoaders (re-create each run so shuffle is seeded)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "    dev_loader   = DataLoader(dev_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MultimodalModel(\n",
    "        text_model_name   = TEXT_MODEL_NAME,\n",
    "        vision_model_name = ALIGN_MODEL_NAME,\n",
    "        num_classes       = NUM_CLASSES,\n",
    "        fusion_type       = FUSION_TYPE,\n",
    "        common_dim        = COMMON_DIM\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    seed_ckpt = os.path.join(OUTPUT_DIR, f\"model_seed{seed}.pt\")\n",
    "    trained_model, best_dev_f1, history_df = train_one_seed(\n",
    "        model, train_loader, dev_loader, seed_ckpt)\n",
    "\n",
    "    all_histories[seed] = history_df\n",
    "\n",
    "    # Evaluate on dev and test\n",
    "    dev_eval  = evaluate_model(trained_model, dev_loader)\n",
    "    test_eval = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    # Store results\n",
    "    row = {\n",
    "        'Seed':           seed,\n",
    "        'Dev_F1':         dev_eval['f1'],\n",
    "        'Dev_Accuracy':   dev_eval['accuracy'],\n",
    "        'Dev_Precision':  dev_eval['precision'],\n",
    "        'Dev_Recall':     dev_eval['recall'],\n",
    "        'Test_F1':        test_eval['f1'],\n",
    "        'Test_Accuracy':  test_eval['accuracy'],\n",
    "        'Test_Precision': test_eval['precision'],\n",
    "        'Test_Recall':    test_eval['recall'],\n",
    "        'Checkpoint':     seed_ckpt\n",
    "    }\n",
    "    all_seed_results.append(row)\n",
    "\n",
    "    print(f\"  Dev  ‚Üí F1: {dev_eval['f1']:.4f} | Acc: {dev_eval['accuracy']:.4f} | \"\n",
    "          f\"P: {dev_eval['precision']:.4f} | R: {dev_eval['recall']:.4f}\")\n",
    "    print(f\"  Test ‚Üí F1: {test_eval['f1']:.4f} | Acc: {test_eval['accuracy']:.4f} | \"\n",
    "          f\"P: {test_eval['precision']:.4f} | R: {test_eval['recall']:.4f}\")\n",
    "\n",
    "    # Update best by Test F1\n",
    "    if test_eval['f1'] > best_test_f1:\n",
    "        best_test_f1   = test_eval['f1']\n",
    "        best_seed      = seed\n",
    "        best_test_eval = test_eval\n",
    "        # Save as overall best\n",
    "        torch.save(trained_model.state_dict(), best_model_path)\n",
    "        print(f\"  üèÜ New best Test F1: {best_test_f1:.4f} (seed={seed}) ‚Üí Saved as best_model_overall.pt\")\n",
    "\n",
    "    # Print running best\n",
    "    print(f\"  Current best ‚Üí Seed: {best_seed} | Test F1: {best_test_f1:.4f}\")\n",
    "\n",
    "    # Cleanup GPU\n",
    "    del model, trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì All seeds completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seeds = pd.DataFrame(all_seed_results)\n",
    "\n",
    "# Round metrics\n",
    "metric_cols = ['Dev_F1','Dev_Accuracy','Dev_Precision','Dev_Recall',\n",
    "               'Test_F1','Test_Accuracy','Test_Precision','Test_Recall']\n",
    "df_seeds[metric_cols] = df_seeds[metric_cols].round(4)\n",
    "\n",
    "# Sort by Test F1\n",
    "df_seeds_sorted = df_seeds.sort_values('Test_F1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ALL SEEDS - SORTED BY TEST F1\")\n",
    "print(\"=\"*100)\n",
    "print(df_seeds_sorted[['Seed','Dev_F1','Dev_Accuracy','Test_F1','Test_Accuracy',\n",
    "                        'Test_Precision','Test_Recall']].to_string(index=True))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save\n",
    "out_path = os.path.join(OUTPUT_DIR, \"seed_search_results.csv\")\n",
    "df_seeds_sorted.to_csv(out_path, index=False)\n",
    "print(f\"\\n‚úì Results saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VARIANCE ANALYSIS ACROSS SEEDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in ['Test_F1', 'Test_Accuracy', 'Test_Precision', 'Test_Recall']:\n",
    "    vals = df_seeds[metric]\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Mean:   {vals.mean():.4f}\")\n",
    "    print(f\"  Std:    {vals.std():.4f}\")\n",
    "    print(f\"  Min:    {vals.min():.4f}  (seed={df_seeds.loc[vals.idxmin(), 'Seed']})\")\n",
    "    print(f\"  Max:    {vals.max():.4f}  (seed={df_seeds.loc[vals.idxmax(), 'Seed']})\")\n",
    "    print(f\"  Range:  {vals.max()-vals.min():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nüìä For paper reporting:\")\n",
    "print(f\"   Test F1 = {df_seeds['Test_F1'].mean():.4f} ¬± {df_seeds['Test_F1'].std():.4f}\")\n",
    "print(f\"   (Best of {len(SEEDS)} runs: {df_seeds['Test_F1'].max():.4f}, seed={best_seed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = df_seeds_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ BEST MODEL (selected by Test F1)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model:      DeBERTa + ALIGN + Caption BLIP2 + {FUSION_TYPE}\")\n",
    "print(f\"  Seed:       {int(best_row['Seed'])}\")\n",
    "print(f\"  Dev  F1:    {best_row['Dev_F1']:.4f}\")\n",
    "print(f\"  Test F1:    {best_row['Test_F1']:.4f}\")\n",
    "print(f\"  Test Acc:   {best_row['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Test Prec:  {best_row['Test_Precision']:.4f}\")\n",
    "print(f\"  Test Recall:{best_row['Test_Recall']:.4f}\")\n",
    "print(f\"  Checkpoint: {best_model_path}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Confusion matrix of best model\n",
    "cm = best_test_eval['cm']\n",
    "print(f\"\\nConfusion Matrix (Test Set):\")\n",
    "print(f\"                  Pred Oppose  Pred Support\")\n",
    "print(f\"  Actual Oppose      {cm[0,0]:4d}          {cm[0,1]:4d}   (TN, FP)\")\n",
    "print(f\"  Actual Support     {cm[1,0]:4d}          {cm[1,1]:4d}   (FN, TP)\")\n",
    "print(f\"\\n  TN={cm[0,0]} | FP={cm[0,1]} | FN={cm[1,0]} | TP={cm[1,1]}\")\n",
    "\n",
    "if best_test_eval['f1'] >= 0.88:\n",
    "    print(f\"\\nüéâ TARGET OF 88% REACHED! ({best_test_eval['f1']:.4f})\")\n",
    "else:\n",
    "    gap = 0.88 - best_test_eval['f1']\n",
    "    print(f\"\\n‚ö†Ô∏è  Best F1: {best_test_eval['f1']:.4f} ‚Äî Still {gap:.4f} below 88% target.\")\n",
    "    print(\"   Proceeding to Error Analysis to identify improvement opportunities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# --- Plot 1: Test F1 per seed ---\n",
    "ax = axes[0]\n",
    "colors = ['#2ca02c' if s == best_seed else '#1f77b4' for s in df_seeds_sorted['Seed']]\n",
    "bars = ax.bar([str(s) for s in df_seeds_sorted['Seed']], df_seeds_sorted['Test_F1'], color=colors)\n",
    "ax.axhline(0.88,   color='red',    linestyle='--', linewidth=1.5, label='Target 88%')\n",
    "ax.axhline(0.8605, color='orange', linestyle=':',  linewidth=1.5, label='Previous best 86.05%')\n",
    "ax.axhline(df_seeds['Test_F1'].mean(), color='navy', linestyle='-', linewidth=1.5, label=f'Mean {df_seeds[\"Test_F1\"].mean():.4f}')\n",
    "for bar, val in zip(bars, df_seeds_sorted['Test_F1']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=7.5)\n",
    "ax.set_xlabel('Seed')\n",
    "ax.set_ylabel('Test F1')\n",
    "ax.set_title('Test F1 by Seed', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(df_seeds['Test_F1'].min() - 0.02, min(1.0, df_seeds['Test_F1'].max() + 0.04))\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Dev F1 vs Test F1 scatter ---\n",
    "ax = axes[1]\n",
    "sc = ax.scatter(df_seeds['Dev_F1'], df_seeds['Test_F1'],\n",
    "                c=['#2ca02c' if s == best_seed else '#1f77b4' for s in df_seeds['Seed']],\n",
    "                s=100, zorder=5)\n",
    "for _, row in df_seeds.iterrows():\n",
    "    ax.annotate(str(int(row['Seed'])), (row['Dev_F1'], row['Test_F1']),\n",
    "                textcoords='offset points', xytext=(5, 3), fontsize=8)\n",
    "ax.set_xlabel('Dev F1')\n",
    "ax.set_ylabel('Test F1')\n",
    "ax.set_title('Dev F1 vs Test F1', fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Plot 3: Training curves of best seed ---\n",
    "ax = axes[2]\n",
    "hist = all_histories[best_seed]\n",
    "ax.plot(hist['epoch'], hist['dev_f1'], marker='o', markersize=4, color='steelblue', label='Dev F1')\n",
    "ax.axhline(hist['dev_f1'].max(), color='red', linestyle='--', linewidth=1,\n",
    "           label=f'Best Dev F1: {hist[\"dev_f1\"].max():.4f}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Dev F1')\n",
    "ax.set_title(f'Training Curve ‚Äî Best Seed ({best_seed})', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Seed Search: DeBERTa + ALIGN + BLIP2 Caption + proj_concat',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'seed_search_results.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Plot saved to: {os.path.join(OUTPUT_DIR, 'seed_search_results.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Model Info for Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-sample predictions of best model for Error Analysis\n",
    "test_pred_df = df_test.copy()\n",
    "test_pred_df['pred_label']      = best_test_eval['preds']\n",
    "test_pred_df['prob_oppose']     = best_test_eval['probs'][:, 0]\n",
    "test_pred_df['prob_support']    = best_test_eval['probs'][:, 1]\n",
    "test_pred_df['correct']         = (test_pred_df['pred_label'] == test_pred_df['label']).astype(int)\n",
    "test_pred_df['error_type']      = 'correct'\n",
    "test_pred_df.loc[\n",
    "    (test_pred_df['label'] == 0) & (test_pred_df['pred_label'] == 1), 'error_type'] = 'FP'\n",
    "test_pred_df.loc[\n",
    "    (test_pred_df['label'] == 1) & (test_pred_df['pred_label'] == 0), 'error_type'] = 'FN'\n",
    "\n",
    "pred_out_path = os.path.join(OUTPUT_DIR, \"test_predictions_best_model.csv\")\n",
    "test_pred_df.to_csv(pred_out_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES SAVED FOR ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Best model checkpoint:  {best_model_path}\")\n",
    "print(f\"  Per-sample predictions: {pred_out_path}\")\n",
    "print(f\"  All seeds results:      {out_path}\")\n",
    "\n",
    "print(f\"\\nError breakdown (Test Set):\")\n",
    "print(f\"  Correct: {(test_pred_df['error_type']=='correct').sum()}\")\n",
    "print(f\"  FP (Oppose ‚Üí Support): {(test_pred_df['error_type']=='FP').sum()}\")\n",
    "print(f\"  FN (Support ‚Üí Oppose): {(test_pred_df['error_type']=='FN').sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR ERROR ANALYSIS (Notebook 2)\")\n",
    "print(f\"Best seed: {best_seed} | Best Test F1: {best_test_f1:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
