# ============================================================================
# CONFIGURACIÓN FINAL - MODELO TEXTUAL
# ============================================================================
# Proyecto: Stance Classification (Abortion & Gun Control)
# Modalidad: Texto (Tweets)
# Objetivo: Maximizar F1-Score
# Fecha creación: 2025-10-27
# Última actualización: 2025-10-27
# Estado: En desarrollo (HPO pendiente)
# ============================================================================

metadata:
  experiment_name: "text_stance_ensemble_final"
  project: "stance_classification_multimodal"
  task: "stance_classification"
  topics: 
    - "abortion"
    - "gun_control"
  modality: "text"
  version: "1.0.0-beta"
  author: "Tu Nombre"
  created_date: "2025-10-27"
  last_updated: "2025-10-27"
  description: "Configuración optimizada para clasificación de stance en tweets usando ensemble de modelos transformers"

# ============================================================================
# CONFIGURACIÓN DE MODELOS
# ============================================================================
model:
  # Tipo de arquitectura
  architecture: "ensemble"
  task_type: "sequence_classification"
  num_labels: 3  # favor, against, neutral
  
  # Modelos individuales del ensemble
  base_models:
    - name: "roberta-base"
      nickname: "roberta"
      reason: "Strong baseline with robust social media understanding"
      
    - name: "microsoft/deberta-v3-base"
      nickname: "deberta"
      reason: "Advanced attention for nuanced stance reasoning"
      
    - name: "cardiffnlp/twitter-roberta-base-stance"
      nickname: "twitter-stance"
      reason: "Domain-specific fine-tuned for stance detection"
      
    - name: "vinai/bertweet-base"
      nickname: "bertweet"
      reason: "Twitter-native pre-trained on 850M tweets"
  
  # Configuración del ensemble
  ensemble:
    enabled: true
    method: "hard_voting"  # BEST: hard_voting (F1: 0.8577)
    
    # Resultados de métodos probados
    methods_tested:
      hard_voting:
        accuracy: 0.8600
        precision: 0.8563
        recall: 0.8727
        f1: 0.8577
        selected: true
        
      soft_voting:
        accuracy: 0.8467
        precision: 0.8453
        recall: 0.8617
        f1: 0.8447
        selected: false
        
      weighted_voting:
        accuracy: 0.8467
        precision: 0.8453
        recall: 0.8617
        f1: 0.8447
        selected: false
        notes: "Same performance as soft voting"
    
    # Configuración para hard voting
    voting_config:
      strategy: "majority"
      tie_breaking: "first_model"  # En caso de empate, usar predicción del primer modelo

# ============================================================================
# TÉCNICAS DE OPTIMIZACIÓN
# ============================================================================
peft:
  enabled: false
  reason: "PEFT techniques did not improve performance"
  
  # Técnicas probadas (TODAS RECHAZADAS)
  techniques_tested:
    - name: "LoRA"
      eval_loss: 0.640741
      accuracy: 0.6800
      f1_score: 0.6560
      run_time_sec: 20.77
      selected: false
      notes: "Bajo rendimiento comparado con fine-tuning completo"
      
    - name: "PromptTuning"
      eval_loss: 0.690301
      accuracy: 0.5333
      f1_score: 0.5285
      run_time_sec: 19.65
      selected: false
      notes: "Peor rendimiento de las tres técnicas"
      
    - name: "AdaLoRA"
      eval_loss: 1.671759
      accuracy: 0.6333
      f1_score: 0.6158
      run_time_sec: 23.20
      selected: false
      notes: "Alto eval_loss y bajo F1"

# ============================================================================
# DATA AUGMENTATION
# ============================================================================
data_augmentation:
  enabled: true
  technique: "paraphrase"  # BEST performer
  
  # Técnicas probadas (ranking por F1)
  techniques_tested:
    - name: "paraphrase"
      accuracy: 0.8567
      precision: 0.8515
      recall: 0.8670
      f1: 0.8539
      selected: true
      notes: "Mejor técnica de augmentation"
      
    - name: "noise"
      accuracy: 0.8533
      precision: 0.8486
      recall: 0.8642
      f1: 0.8507
      selected: false
      notes: "Segunda mejor opción"
      
    - name: "synonym"
      accuracy: 0.8467
      precision: 0.8411
      recall: 0.8557
      f1: 0.8436
      selected: false
      notes: "Similar a raw data"
      
    - name: "back_translation"
      accuracy: 0.8067
      precision: 0.8194
      recall: 0.8317
      f1: 0.8060
      selected: false
      notes: "Peor rendimiento, posible pérdida de contexto"
  
  # Configuración de paraphrase
  paraphrase_config:
    model: "humarin/chatgpt_paraphraser_on_T5_base"  # o el que uses
    probability: 0.3  # Augmentar 30% del dataset
    max_length: 128
    temperature: 0.7
    preserve_labels: true

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Hiperparámetros actuales (pre-HPO)
  learning_rate: 2.0e-5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  num_train_epochs: 5
  weight_decay: 0.01
  
  # Estrategias
  eval_strategy: "epoch"
  save_strategy: "epoch"
  logging_strategy: "epoch"
  logging_steps: 10
  
  # Optimización
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 2
    threshold: 0.0001
  
  # Directorios
  output_dir: "./temp_models/{model_name}_ensemble"
  logging_dir: "./logs"
  
  # Reporting
  report_to: "none"  # Cambiar a ["wandb", "mlflow"] si usas tracking
  
  # Optimizer
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1.0e-8
  
  # Scheduler
  lr_scheduler:
    type: "linear"
    warmup_ratio: 0.1

# ============================================================================
# HYPERPARAMETER OPTIMIZATION (HPO)
# ============================================================================
hpo:
  status: "pending"  # pending | in_progress | completed
  framework: "optuna"  # optuna | ray | wandb
  
  # Espacio de búsqueda (ajustar según tus necesidades)
  search_space:
    learning_rate:
      type: "loguniform"
      low: 1.0e-5
      high: 5.0e-5
      
    per_device_train_batch_size:
      type: "categorical"
      choices: [8, 16, 32]
      
    num_train_epochs:
      type: "int"
      low: 3
      high: 7
      
    weight_decay:
      type: "uniform"
      low: 0.0
      high: 0.1
      
    warmup_ratio:
      type: "uniform"
      low: 0.0
      high: 0.2
  
  # Configuración de búsqueda
  n_trials: 50
  timeout: 86400  # 24 horas en segundos
  direction: "maximize"
  metric: "eval_f1"
  
  # Pruner (para detener trials no prometedores)
  pruner:
    type: "median"
    n_startup_trials: 5
    n_warmup_steps: 0
  
  # Sampler
  sampler:
    type: "tpe"  # Tree-structured Parzen Estimator
  
  # Resultados (llenar después de HPO)
  best_params: null
  best_value: null
  best_trial: null

# ============================================================================
# DATA PREPROCESSING
# ============================================================================
preprocessing:
  # Tokenización
  max_length: 128
  truncation: true
  padding: "max_length"
  return_tensors: "pt"
  
  # Limpieza de texto (para tweets)
  text_cleaning:
    remove_urls: true
    remove_mentions: false  # @mentions pueden tener contexto
    remove_hashtags: false  # #hashtags son importantes para stance
    lowercase: false  # Mantener mayúsculas por énfasis
    remove_emojis: false  # Emojis tienen valor semántico
    normalize_whitespace: true
    remove_retweet_prefix: true  # "RT @user:"

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  name: "stance_abortion_guncontrol"
  source: "huggingface"  # o "local"
  version: "1.0"
  
  # Splits
  splits:
    train: "train"
    validation: "dev"
    test: "test"
  
  # Balance de clases
  class_distribution:
    favor: null  # Llenar con estadísticas reales
    against: null
    neutral: null
  
  # Features
  text_column: "text"
  label_column: "label"
  image_column: "image"  # Para fase multimodal futura
  
  # Mapeo de labels
  label_mapping:
    0: "favor"
    1: "against"
    2: "neutral"

# ============================================================================
# EVALUATION METRICS
# ============================================================================
evaluation:
  primary_metric: "f1_macro"
  
  metrics:
    - "accuracy"
    - "precision_macro"
    - "recall_macro"
    - "f1_macro"
    - "f1_weighted"
    - "confusion_matrix"
  
  # Resultados actuales (llenar con mejores resultados)
  current_best:
    validation:
      accuracy: 0.8600
      precision: 0.8563
      recall: 0.8727
      f1_macro: 0.8577
      method: "hard_voting_ensemble"
    
    test:
      accuracy: null
      precision: null
      recall: null
      f1_macro: null

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
reproducibility:
  seed: 42
  deterministic: true
  
  # Environment
  python_version: "3.10"  # Ajustar
  cuda_version: "11.8"    # Ajustar
  
  # Key dependencies (llenar con versiones exactas)
  dependencies:
    transformers: "4.35.0"
    torch: "2.1.0"
    datasets: "2.14.0"
    accelerate: "0.24.0"
    optuna: "3.4.0"
    scikit-learn: "1.3.0"

# ============================================================================
# FUTURE PHASES
# ============================================================================
roadmap:
  current_phase: "text_modeling"
  
  phases:
    - phase: 1
      name: "text_modeling"
      status: "in_progress"
      description: "Optimize text-only models"
      target_f1: 0.87
      
    - phase: 2
      name: "vision_modeling"
      status: "pending"
      description: "Image-only stance detection"
      models: ["ViT", "CLIP_vision", "ResNet"]
      
    - phase: 3
      name: "multimodal_fusion"
      status: "pending"
      description: "Combine text + image"
      models: ["CLIP", "VisualBERT", "FLAVA"]
      fusion_strategy: "late_fusion"  # o early/intermediate

# ============================================================================
# NOTES & LEARNINGS
# ============================================================================
notes:
  key_findings:
    - "PEFT técnicas no mejoraron rendimiento - usar fine-tuning completo"
    - "Paraphrase augmentation es la mejor técnica (+1% F1 vs raw data)"
    - "Hard voting ensemble supera a soft/weighted voting"
    - "Modelos Twitter-específicos aportan valor al ensemble"
    
  next_steps:
    - "Completar HPO para optimizar hiperparámetros"
    - "Evaluar en test set con mejores hiperparámetros"
    - "Analizar errores por clase (favor/against/neutral)"
    - "Preparar pipeline para fase de vision"
    
  warnings:
    - "No usar PEFT en producción con estos modelos"
    - "Back translation degrada performance significativamente"
    - "Considerar class imbalance si existe en dataset"