{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6561d69",
   "metadata": {},
   "source": [
    "# Model Ensembles for Stance Classification \n",
    "\n",
    "Our goal is to improve the macro F1-score of our stance classification models by combining multiple models through ensemble techniques. We will explore approaches such as majority voting, soft voting (averaging probabilities), and weighted ensembles.\n",
    "\n",
    "We will implement:\n",
    "\n",
    "1. **Soft Voting Ensemble**: average predicted probabilities and select the class with highest average probability.\n",
    "2. **Hard Voting Ensemble**: majority vote among predicted labels.\n",
    "3. **Weighted Ensemble**: Assign different weights to each model based on their performance (F1-score) and combine predictions.\n",
    "\n",
    "\n",
    "The notebook evaluates the performance on a held-out test set and compares ensembles against individual models.\n",
    "Metrics: accuracy, precision, recall, F1-score (macro), confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f45382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data locally from: /workspace/dzuniga/\n",
      "GPU ready: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Google Colab or not\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = \"/content/drive/MyDrive/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data from Google Drive: {path}\")\n",
    "else:\n",
    "    path = \"/workspace/dzuniga/\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data locally from: {path}\")\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU ready:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected, using CPU.\")\n",
    "\n",
    "#If we use the augmented dataset:\n",
    "augmented = True\n",
    "\n",
    "# Set Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0b7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "if augmented:\n",
    "  train_path = f\"{path}/data/train_augmented_synonym.csv\"\n",
    "else:\n",
    "  train_path = f\"{path}/data/train.csv\"\n",
    "dev_path   = f\"{path}/data/dev.csv\"\n",
    "test_path  = f\"{path}/data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Map labels to ints\n",
    "label2id = {\"oppose\": 0, \"support\": 1}\n",
    "\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(label2id)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"tweet_text\",\"label\"]])\n",
    "dataset_dev   = Dataset.from_pandas(df_dev[[\"tweet_text\",\"label\"]])\n",
    "dataset_test  = Dataset.from_pandas(df_test[[\"tweet_text\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9081eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model Selection for Stance Detection\n",
    "ENSEMBLE_MODELS = [\n",
    "    \"roberta-base\",                                    # Strong baseline with robust social media understanding\n",
    "    \"microsoft/deberta-v3-base\",                       # Advanced attention for nuanced stance reasoning\n",
    "    \"cardiffnlp/twitter-roberta-base-offensive\",       # Domain-specific: fine-tuned specifically for stance detection\n",
    "    \"vinai/bertweet-base\"                              # Twitter-native: pre-trained on 850M tweets\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 105"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcb768",
   "metadata": {},
   "source": [
    "Short Justification:\n",
    "\n",
    "1. roberta-base: Provides a solid foundation with extensive pre-training on diverse text, including social media. Handles informal language, typos, and abbreviations well.\n",
    "\n",
    "2. microsoft/deberta-v3-base: Contributes sophisticated contextual understanding through attention mechanism, crucial for capturing the relationship between \n",
    "   arguments and stance targets in controversial topics.\n",
    "\n",
    "3. cardiffnlp/twitter-roberta-base-stance: Adds task-specific expertise as it's explicitly fine-tuned for stance detection, bringing specialized knowledge to the ensemble.\n",
    "\n",
    "4. vinai/bertweet-base: Captures Twitter-specific linguistic patterns (hashtags, mentions, emojis, slang) that other models may miss, having been pre-trained exclusively on tweets.\n",
    "\n",
    "This combination ensures architectural diversity (BERT-based vs RoBERTa-based), capacity variation (general vs specialized), and domain coverage (general NLP vs Twitter-specific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab08586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for roberta-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168a5f1b12704036b7301b2762fd3f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446f9b3d4ad24c508ef6b1300b0d8f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc6eb8e6624428ab26a9a7aa4c495da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c21858c863646d5af658e1f9027f3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56e1a4124d448e4b402a8745cd4a2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae5c87194e4423687ea98b5abd98d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for cardiffnlp/twitter-roberta-base-offensive...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2305bd573b7a44299c8ff58484612416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40b6db954444898b48987457a232a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a815b923781248f98d64bff8a13a0a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for vinai/bertweet-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5452722f9153415d8cd5c33c2106cf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc35b672a82b4dd0bc814dd4d52f3b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e2ab04c04f4a659d3a7d34b74782c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We tokenize the dataset for each model in the ensemble\n",
    "tokenized_datasets = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for model_name in ENSEMBLE_MODELS:\n",
    "    print(f\"Tokenizing for {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizers[model_name] = tokenizer\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"tweet_text\"],padding=\"max_length\",truncation=True,max_length=MAX_LENGTH)\n",
    "\n",
    "\n",
    "    tokenized_train = dataset_train.map(tokenize_batch, batched=True)\n",
    "    tokenized_dev = dataset_dev.map(tokenize_batch, batched=True)\n",
    "    tokenized_test = dataset_test.map(tokenize_batch, batched=True)\n",
    "\n",
    "    for ds in [tokenized_train, tokenized_dev, tokenized_test]:\n",
    "        ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    \n",
    "    tokenized_datasets[model_name] = {\n",
    "        \"train\": tokenized_train,\n",
    "        \"dev\": tokenized_dev,\n",
    "        \"test\": tokenized_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5469ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics we are going to evaluate\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    precision = precision_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    recall = recall_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    \n",
    "    return {\"accuracy\": acc,\"f1\": f1,\"precision\": precision,\"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d176075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 00:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.258812</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.258957</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.248245</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.863014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.261341</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.863014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.315810</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.879433</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 00:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.286953</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.890411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.314624</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.753425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.324910</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.863014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.466455</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.435196</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning cardiffnlp/twitter-roberta-base-offensive...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 00:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.295794</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.364161</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.698630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.397751</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.405354</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.472899</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning vinai/bertweet-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 00:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.289438</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.295307</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.282229</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.904110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.316655</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.293808</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Fine-tune Each Model\n",
    "all_model_results = {}\n",
    "\n",
    "for model_name in ENSEMBLE_MODELS:\n",
    "    print(f\"\\nFine-tuning {model_name}...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    datasets = tokenized_datasets[model_name]\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./temp_models/{model_name}_ensemble\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"dev\"],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Predictions on test set\n",
    "    predictions_output = trainer.predict(datasets[\"test\"])\n",
    "    y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(predictions_output.predictions), dim=1).numpy()\n",
    "    y_true = predictions_output.label_ids\n",
    "\n",
    "    predictions_output = trainer.predict(datasets[\"test\"])\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = compute_metrics(predictions_output)\n",
    "    all_model_results[model_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"model_probs\":probs,\n",
    "        \"model_preds\":y_pred,\n",
    "        \"y_true\": y_true\n",
    "    }\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f35927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract predictions and probabilities from each model\n",
    "model_preds = {name: results[\"model_preds\"] for name, results in all_model_results.items()}\n",
    "model_probs = {name: results[\"model_probs\"] for name, results in all_model_results.items()}\n",
    "y_true = list(all_model_results.values())[0][\"y_true\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550af68",
   "metadata": {},
   "source": [
    "### Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1973ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Voting (Majority)----\n",
    "all_preds = np.array(list(model_preds.values()))  # dimension [n_models, n_samples]\n",
    "hard_vote_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38d25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting (Average Probabilities) ---\n",
    "all_probs = np.array(list(model_probs.values()))  # dimension [n_models, n_samples, n_classes]\n",
    "soft_vote_preds = np.argmax(np.mean(all_probs, axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d9144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ensemble (Based on the F1-Score)----\n",
    "\n",
    "# We extract F1-scores from each model\n",
    "f1_scores = np.array([results[\"metrics\"][\"f1\"] for results in all_model_results.values()])\n",
    "\n",
    "# Normalize the F1-scores to use them as weights\n",
    "weights = f1_scores / np.sum(f1_scores)\n",
    "\n",
    "# Apply weights to the probabilities\n",
    "weighted_probs = np.tensordot(weights, all_probs, axes=([0], [0]))  # dimension [n_samples, n_classes]\n",
    "weighted_vote_preds = np.argmax(weighted_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a34f06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m ensemble_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ensemble_name, y_pred \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhard_voting\u001b[39m\u001b[38;5;124m\"\u001b[39m, hard_vote_preds), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft_voting\u001b[39m\u001b[38;5;124m\"\u001b[39m, soft_vote_preds),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_voting\u001b[39m\u001b[38;5;124m\"\u001b[39m, weighted_vote_preds)]:\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#Metrics\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mlabels\u001b[49m, preds)\n\u001b[1;32m      8\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     prec \u001b[38;5;241m=\u001b[39m precision_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate Ensembles and Display Results\n",
    "ensemble_results = {}\n",
    "\n",
    "for ensemble_name, y_pred in [(\"hard_voting\", hard_vote_preds), (\"soft_voting\", soft_vote_preds),(\"weighted_voting\", weighted_vote_preds)]:\n",
    "    \n",
    "    #Metrics\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    prec = precision_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    rec = recall_score(labels, preds, average=\"binary\", pos_label=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    ensemble_results[ensemble_name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"predictions\": y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{ensemble_name.upper()} Results:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    labels=[\"oppose\",\"support\"]\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix | F1 (Binary): {f1:.4f}\")\n",
    "    plt.show()\n",
    "\n",
    "# Show the weights used in weighted ensemble\n",
    "print(\"\\nWeights used in Weighted Ensemble:\")\n",
    "for model_name, weight, f1 in zip(all_model_results.keys(), weights, f1_scores):\n",
    "    print(f\"{model_name}: {weight:.4f} (F1: {f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "ensemble_data = []\n",
    "for ensemble_name, metrics in ensemble_results.items():\n",
    "    ensemble_data.append({\n",
    "        \"ensemble_method\": ensemble_name,\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1\": metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_data)\n",
    "results_df = results_df.sort_values(by=\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Guardar como CSV\n",
    "results_df.to_csv(f\"{path}/experiments/text/Ensemble/ensemble_results.csv\", index=False)\n",
    "print(f\"Ensemble results saved to {path}/experiments/text/Ensemble/ensemble_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
