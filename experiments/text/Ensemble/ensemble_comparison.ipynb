{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6561d69",
   "metadata": {},
   "source": [
    "# Model Ensembles for Stance Classification \n",
    "\n",
    "Our goal is to improve the macro F1-score of our stance classification models by combining multiple models through ensemble techniques. We will explore approaches such as majority voting, soft voting (averaging probabilities), and weighted ensembles.\n",
    "\n",
    "We will implement:\n",
    "\n",
    "1. **Soft Voting Ensemble**: average predicted probabilities and select the class with highest average probability.\n",
    "2. **Hard Voting Ensemble**: majority vote among predicted labels.\n",
    "3. **Weighted Ensemble**: Assign different weights to each model based on their performance (F1-score) and combine predictions.\n",
    "\n",
    "\n",
    "The notebook evaluates the performance on a held-out test set and compares ensembles against individual models.\n",
    "Metrics: accuracy, precision, recall, F1-score (macro), confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f45382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data locally from: C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\n",
      "No GPU detected, using CPU.\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Google Colab or not\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = \"/content/drive/MyDrive/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data from Google Drive: {path}\")\n",
    "else:\n",
    "    path = \"C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data locally from: {path}\")\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU ready:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected, using CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0b7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_path = f\"{path}/data/train.csv\"\n",
    "dev_path   = f\"{path}/data/dev.csv\"\n",
    "test_path  = f\"{path}/data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Map labels to ints\n",
    "label2id = {\"oppose\": 0, \"support\": 1}\n",
    "\n",
    "# Minimal Preprocessing\n",
    "def clean_tweet(text):\n",
    "\n",
    "    text = str(text)\n",
    "    # URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Replace symbols\n",
    "    text = text.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    # Split the words inside Hashtag (#ILoveMexico -> # I Love Mexico)\n",
    "    def separar_hashtag(match):\n",
    "        hashtag = match.group(1)\n",
    "        separado = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', hashtag)\n",
    "        return f\"# {separado}\"\n",
    "\n",
    "    text = re.sub(r\"#(\\w+)\", separar_hashtag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(clean_tweet)\n",
    "    df[\"label\"] = df[\"stance\"].map(label2id)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"tweet_text\",\"label\"]])\n",
    "dataset_dev   = Dataset.from_pandas(df_dev[[\"tweet_text\",\"label\"]])\n",
    "dataset_test  = Dataset.from_pandas(df_test[[\"tweet_text\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9081eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model Selection for Stance Detection\n",
    "ENSEMBLE_MODELS = [\n",
    "    \"roberta-base\",                                    # Strong baseline with robust social media understanding\n",
    "    \"microsoft/deberta-v3-base\",                       # Advanced attention for nuanced stance reasoning\n",
    "    \"cardiffnlp/twitter-roberta-base-stance\",          # Domain-specific: fine-tuned specifically for stance detection\n",
    "    \"vinai/bertweet-base\"                              # Twitter-native: pre-trained on 850M tweets\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 105"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcb768",
   "metadata": {},
   "source": [
    "Short Justification:\n",
    "\n",
    "1. roberta-base: Provides a solid foundation with extensive pre-training on diverse text, including social media. Handles informal language, typos, and abbreviations well.\n",
    "\n",
    "2. microsoft/deberta-v3-base: Contributes sophisticated contextual understanding through attention mechanism, crucial for capturing the relationship between \n",
    "   arguments and stance targets in controversial topics.\n",
    "\n",
    "3. cardiffnlp/twitter-roberta-base-stance: Adds task-specific expertise as it's explicitly fine-tuned for stance detection, bringing specialized knowledge to the ensemble.\n",
    "\n",
    "4. vinai/bertweet-base: Captures Twitter-specific linguistic patterns (hashtags, mentions, emojis, slang) that other models may miss, having been pre-trained exclusively on tweets.\n",
    "\n",
    "This combination ensures architectural diversity (BERT-based vs RoBERTa-based), capacity variation (general vs specialized), and domain coverage (general NLP vs Twitter-specific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab08586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1814/1814 [00:00<00:00, 3563.85 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3748.51 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 4265.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1814/1814 [00:00<00:00, 3071.81 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 2498.71 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 2577.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for distilroberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1814/1814 [00:00<00:00, 5767.40 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4281.54 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 4877.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# We tokenize the dataset for each model in the ensemble\n",
    "tokenized_datasets = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for model_name in ENSEMBLE_MODELS:\n",
    "    print(f\"Tokenizing for {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizers[model_name] = tokenizer\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"tweet_text\"],padding=\"max_length\",truncation=True,max_length=MAX_LENGTH)\n",
    "\n",
    "\n",
    "    tokenized_train = dataset_train.map(tokenize_batch, batched=True)\n",
    "    tokenized_dev = dataset_dev.map(tokenize_batch, batched=True)\n",
    "    tokenized_test = dataset_test.map(tokenize_batch, batched=True)\n",
    "\n",
    "    for ds in [tokenized_train, tokenized_dev, tokenized_test]:\n",
    "        ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    \n",
    "    tokenized_datasets[model_name] = {\n",
    "        \"train\": tokenized_train,\n",
    "        \"dev\": tokenized_dev,\n",
    "        \"test\": tokenized_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5469ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Metrics Function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tune Each Model\n",
    "all_model_results = {}\n",
    "\n",
    "for model_name in ENSEMBLE_MODELS:\n",
    "    print(f\"\\nFine-tuning {model_name}...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    datasets = tokenized_datasets[model_name]\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./temp_models/{model_name}_ensemble\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"dev\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Predictions on test set\n",
    "    predictions_output = trainer.predict(datasets[\"test\"])\n",
    "    y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(predictions_output.predictions), dim=1).numpy()\n",
    "    y_true = predictions_output.label_ids\n",
    "\n",
    "    predictions_output = trainer.predict(datasets[\"test\"])\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = compute_metrics(predictions_output)\n",
    "    all_model_results[model_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"model_probs\":probs,\n",
    "        \"model_preds\":y_pred,\n",
    "        \"y_true\": y_true\n",
    "    }\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f35927",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ensemble Methods\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# We extract predictions and probabilities from each model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model_preds \u001b[38;5;241m=\u001b[39m {name: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, results \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_model_results\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      5\u001b[0m model_probs \u001b[38;5;241m=\u001b[39m {name: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, results \u001b[38;5;129;01min\u001b[39;00m all_model_results\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      6\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_model_results\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_model_results' is not defined"
     ]
    }
   ],
   "source": [
    "# We extract predictions and probabilities from each model\n",
    "model_preds = {name: results[\"model_preds\"] for name, results in all_model_results.items()}\n",
    "model_probs = {name: results[\"model_probs\"] for name, results in all_model_results.items()}\n",
    "y_true = list(all_model_results.values())[0][\"y_true\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550af68",
   "metadata": {},
   "source": [
    "### Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Voting (Majority)----\n",
    "all_preds = np.array(list(model_preds.values()))  # dimension [n_models, n_samples]\n",
    "hard_vote_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting (Average Probabilities) ---\n",
    "all_probs = np.array(list(model_probs.values()))  # dimension [n_models, n_samples, n_classes]\n",
    "soft_vote_preds = np.argmax(np.mean(all_probs, axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ensemble (Based on the F1-Score)----\n",
    "\n",
    "# We extract F1-scores from each model\n",
    "f1_scores = np.array([results[\"metrics\"][\"f1\"] for results in all_model_results.values()])\n",
    "\n",
    "# Normalize the F1-scores to use them as weights\n",
    "weights = f1_scores / np.sum(f1_scores)\n",
    "\n",
    "# Apply weights to the probabilities\n",
    "weighted_probs = np.tensordot(weights, all_probs, axes=([0], [0]))  # dimension [n_samples, n_classes]\n",
    "weighted_vote_preds = np.argmax(weighted_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ensembles and Display Results\n",
    "ensemble_results = {}\n",
    "\n",
    "for ensemble_name, y_pred in [(\"hard_voting\", hard_vote_preds), (\"soft_voting\", soft_vote_preds),(\"weighted_voting\", weighted_vote_preds)]:\n",
    "    \n",
    "    #Metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    ensemble_results[ensemble_name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"predictions\": y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{ensemble_name.upper()} Results:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    labels=[\"oppose\",\"support\"]\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix | F1 Macro: {f1:.4f}\")\n",
    "    plt.show()\n",
    "\n",
    "# Show the weights used in weighted ensemble\n",
    "print(\"\\nWeights used in Weighted Ensemble:\")\n",
    "for model_name, weight, f1 in zip(all_model_results.keys(), weights, f1_scores):\n",
    "    print(f\"{model_name}: {weight:.4f} (F1: {f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "ensemble_data = []\n",
    "for ensemble_name, metrics in ensemble_results.items():\n",
    "    ensemble_data.append({\n",
    "        \"ensemble_method\": ensemble_name,\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1\": metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_data)\n",
    "results_df = results_df.sort_values(by=\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Guardar como CSV\n",
    "results_df.to_csv(f\"{path}/experiments/text/Ensemble/ensemble_results.csv\", index=False)\n",
    "print(f\"Ensemble results saved to {path}/experiments/text/Ensemble/ensemble_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
