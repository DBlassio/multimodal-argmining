{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a05a5b",
   "metadata": {},
   "source": [
    "### Testing 5 different transformer models with 3 seeds each\n",
    "### Task: Argument Stance Classification (Support/Oppose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5455aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data locally from: C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\n",
      "No GPU detecting, using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Colab or not\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = \"/content/drive/MyDrive/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data from Google Drive: {path}\")\n",
    "else:\n",
    "    path = \"C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data locally from: {path}\")\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU ready:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detecting, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ab8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models to test:\n",
      "  1. roberta-base\n",
      "  2. microsoft/deberta-v3-base\n",
      "  3. cardiffnlp/twitter-roberta-base\n",
      "  4. bert-base-uncased\n",
      "  5. microsoft/deberta-base\n",
      "  6. ddore14/RooseBERT-scr-uncased\n"
     ]
    }
   ],
   "source": [
    "# WE define our Models to tests and the seeds\n",
    "MODELS = [\n",
    "    \"roberta-base\",                    \n",
    "    \"microsoft/deberta-v3-base\",       \n",
    "    \"cardiffnlp/twitter-roberta-base\", \n",
    "    \"bert-base-uncased\",               \n",
    "    \"microsoft/deberta-base\",\n",
    "    \"ddore14/RooseBERT-scr-uncased\"           \n",
    "]\n",
    "\n",
    "#Seeds\n",
    "SEEDS = [42, 123, 456]  \n",
    "\n",
    "\n",
    "print(\"\\nModels to test:\")\n",
    "for i, model in enumerate(MODELS, 1):\n",
    "    print(f\"  {i}. {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "446bf029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    475\n",
      "0    448\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "train_path = f\"{path}/data/gun_control_train.csv\"\n",
    "dev_path   = f\"{path}/data/gun_control_dev.csv\"\n",
    "test_path  = f\"{path}/data/gun_control_test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Map labels to ints\n",
    "label2id = {\"oppose\": 0, \"support\": 1}\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"label\"] = df[\"stance\"].map(label2id)\n",
    "\n",
    "print(df_train[\"label\"].value_counts())\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"tweet_text\", \"label\"]])\n",
    "dataset_dev   = Dataset.from_pandas(df_dev[[\"tweet_text\", \"label\"]])\n",
    "dataset_test  = Dataset.from_pandas(df_test[[\"tweet_text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d746beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization Function for each model \n",
    "def tokenize_dataset(dataset, tokenizer, max_length=128):\n",
    "    \n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"tweet_text\"],padding=\"max_length\",truncation=True,max_length=max_length)\n",
    "\n",
    "    tokenized = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67425fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define metrics Function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_and_evaluate(model_name, seed, train_dataset, dev_dataset):\n",
    "\n",
    "    # Set seed!\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"Training: {model_name} | Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Tokenizer loaded for {model_name}...\")\n",
    "    \n",
    "\n",
    "    # Tokenize datasets with model-specific tokenizer\n",
    "    train_dataset = tokenize_dataset(train_dataset, tokenizer, 128)\n",
    "    dev_dataset = tokenize_dataset(dev_dataset, tokenizer, 128)\n",
    "    print(f\"Tokenization complete.\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    print(f\"Model Loaded: {model_name}.\")\n",
    "    \n",
    "  \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./temp_models/{model_name.replace('/', '_')}_seed{seed}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\n Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n Evaluating on dev set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Extract metrics\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"seed\": seed,\n",
    "        \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "        \"precision\": eval_results[\"eval_precision\"],\n",
    "        \"recall\": eval_results[\"eval_recall\"],\n",
    "        \"f1\": eval_results[\"eval_f1\"],\n",
    "        \"loss\": eval_results[\"eval_loss\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Results for {model_name} (seed {seed}):\")\n",
    "    print(f\"   Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {results['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {results['f1']:.4f}\")\n",
    "    \n",
    "    # Clean up to save memory\n",
    "    del model\n",
    "    del trainer\n",
    "    del train_dataset\n",
    "    del dev_dataset\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c48ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run the Experiments\n",
    "print(\"STARTING EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_results = []\n",
    "total_experiments = len(MODELS) * len(SEEDS)\n",
    "current_experiment = 0\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print(f\"# MODEL: {model_name}\")\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        current_experiment += 1\n",
    "        print(f\"\\n[Experiment {current_experiment}/{total_experiments}]\")\n",
    "        results = train_and_evaluate(\n",
    "            model_name=model_name,\n",
    "            seed=seed,\n",
    "            train_dataset=dataset_train,\n",
    "            dev_dataset=dataset_dev)\n",
    "        \n",
    "        all_results.append(results)\n",
    "           \n",
    "\n",
    "print(\"EXPERIMENT COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Per model we calculate the mean and std on each metric\n",
    "model_stats = results_df.groupby('model').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "# Format\n",
    "model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns.values]\n",
    "model_stats = model_stats.reset_index()\n",
    "model_stats.columns = [\n",
    "    'Model',\n",
    "    'Accuracy_Mean', 'Accuracy_Std',\n",
    "    'Precision_Mean', 'Precision_Std',\n",
    "    'Recall_Mean', 'Recall_Std',\n",
    "    'F1_Mean', 'F1_Std'\n",
    "]\n",
    "\n",
    "# Sort by F1 score\n",
    "model_stats = model_stats.sort_values('F1_Mean', ascending=False).reset_index(drop=True)\n",
    "print(model_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1296cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Results to CSV\n",
    "os.makedirs(f\"{path}/experiments/text/Performance/\", exist_ok=True)\n",
    "output_file = f\"{path}/experiments/text/Performance/model_comparison_results.csv\"\n",
    "model_stats.to_csv(output_file, index=False)\n",
    "print(f\"\\nSummary results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
