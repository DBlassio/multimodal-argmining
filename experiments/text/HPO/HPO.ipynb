{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d9ea24",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for Stance Classification (OPTUNA)\n",
    "\n",
    "The goal of this notebook is to optimize the macro F1-score of a stance classification model using hyperparameter optimization (HPO) techniques with Optuna.\n",
    "We fine-tune our winner model (Best Performance) on a balanced dataset (Previously Augmented) of tweets labeled as support or oppose.\n",
    "\n",
    "We incorporate:\n",
    "- Bayesian Optimization (TPE Sampler) to efficiently explore the hyperparameter space.\n",
    "- Early Stopping to prevent overfitting by stopping training.\n",
    "- Pruning (Median Pruner) to terminate unpromising trials early and save GPU time.\n",
    "- Evaluation Metrics: Accuracy, Precision, Recall, and F1-score (macro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf80464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data locally from: C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\n",
      "No GPU detecting, using CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2331174f810>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Colab or not\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = \"/content/drive/MyDrive/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data from Google Drive: {path}\")\n",
    "else:\n",
    "    path = \"C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data locally from: {path}\")\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU ready:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detecting, using CPU.\")\n",
    "\n",
    "#If we use the augmented dataset:\n",
    "augmented = True \n",
    "\n",
    "# Set Seed\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db110049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "if augmented:\n",
    "  train_path = f\"{path}/data/train_augmented_paraphrase.csv\"\n",
    "else:\n",
    "  train_path = f\"{path}/data/train.csv\"\n",
    "  \n",
    "dev_path   = f\"{path}/data/dev.csv\"\n",
    "test_path  = f\"{path}/data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Map labels to ints\n",
    "label2id = {\"oppose\": 0, \"support\": 1}\n",
    "\n",
    "#Minimal Preprocessing\n",
    "def clean_tweet(text):\n",
    "\n",
    "    text = str(text)\n",
    "    #URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Replace symbols\n",
    "    # text = text.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
    "    # # Remove multiple spaces\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    # # Split the words inside Hashtag (#ILoveMexico -> # I Love Mexico)\n",
    "    # def separar_hashtag(match):\n",
    "    #     hashtag = match.group(1)\n",
    "    #     separado = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', hashtag)\n",
    "    #     return f\"# {separado}\"\n",
    "\n",
    "    # text = re.sub(r\"#(\\w+)\", separar_hashtag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(clean_tweet)\n",
    "    df[\"label\"] = df[\"stance\"].map(label2id)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"tweet_text\",\"label\"]])\n",
    "dataset_dev   = Dataset.from_pandas(df_dev[[\"tweet_text\",\"label\"]])\n",
    "dataset_test  = Dataset.from_pandas(df_test[[\"tweet_text\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e7b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our Model\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN=105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46833962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2190/2190 [00:00<00:00, 2683.18 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3669.40 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 3131.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "\n",
    "# Tokenization Function for each model\n",
    "def tokenize_dataset(dataset, tokenizer, max_length=MAX_LEN):\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"tweet_text\"],padding=\"max_length\",truncation=True,max_length=max_length)\n",
    "\n",
    "    #Tokenization\n",
    "    tokenized = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "    #Dataset Format for PyTorch\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets with model tokenizer\n",
    "train_dataset_tok = tokenize_dataset(dataset_train, tokenizer, MAX_LEN)\n",
    "dev_dataset_tok = tokenize_dataset(dataset_dev, tokenizer, MAX_LEN)\n",
    "test_dataset_tok = tokenize_dataset(dataset_test, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8326537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    precision = precision_score(labels, preds, average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc,\"f1\": f1_macro,\"precision\": precision,\"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79afa465",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization - OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6334f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function for Hyperparameter Optimization (OPTUNA)\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 3e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 8)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "\n",
    "    # New Model for each trial\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=dropout,\n",
    "        attention_probs_dropout_prob=dropout).to(device)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./optuna-trial-{trial.number}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    # Trainer with Early Stopping\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_tok,\n",
    "        eval_dataset=dev_dataset_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # Training loop with pruning (More efficient)\n",
    "    for epoch in range(num_train_epochs):\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate(dev_dataset_tok)\n",
    "        f1_macro = eval_results[\"eval_f1\"]\n",
    "\n",
    "        # We Report to Optuna\n",
    "        trial.report(f1_macro, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Final evaluation (F1 score)\n",
    "    final_metrics = trainer.evaluate(dev_dataset_tok)\n",
    "    return final_metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create and run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            sampler=TPESampler(seed=seed),\n",
    "                            pruner=MedianPruner(n_warmup_steps=2))\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=20, timeout=None) \n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best F1:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58874888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the best model with optimal hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "print(\"\\n Retraining best model with optimal hyperparameters...\")\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=best_params[\"dropout\"],\n",
    "    attention_probs_dropout_prob=best_params[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_best\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_params[\"batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_ratio=best_params[\"warmup_ratio\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=dev_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "eval_results = final_trainer.evaluate(test_dataset_tok)\n",
    "print(\"\\n Final evaluation on test set:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Output\n",
    "os.makedirs(f\"{path}/experiments/text/HPO/\", exist_ok=True)\n",
    "output_dir = f\"{path}/experiments/text/HPO/\"\n",
    "\n",
    "# Confusion Matrix\n",
    "preds_output = final_trainer.predict(test_dataset_tok)\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = np.argmax(preds_output.predictions, axis=-1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"oppose\", \"support\"])\n",
    "\n",
    "# Export\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax)\n",
    "plt.title(f\"Confusion Matrix_Optimized RoBERTa,f1_Score={eval_results['eval_f1']:.4f}\")\n",
    "save_path = os.path.join(output_dir,f\"confusion_matrix_Model_Optimized.jpg\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics to CSV\n",
    "metrics_dict = eval_results\n",
    "metrics_dict[\"best_trial_f1_dev\"] = study.best_trial.value \n",
    "results_df = pd.DataFrame([metrics_dict])\n",
    "results_df.to_csv(output_dir + \"evaluation_results.csv\", index=False)\n",
    "print(f\"Metrics saved to {output_dir}/evaluation_results.csv\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6739df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best hyperparameters to JSON\n",
    "best_hyperparams = study.best_trial.params\n",
    "with open(f\"{output_dir}/best_hyperparameters.json\", \"w\") as f:\n",
    "    json.dump(best_hyperparams, f, indent=4)\n",
    "print(f\"Best hyperparameters saved to {output_dir}/best_hyperparameters.json\")\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
