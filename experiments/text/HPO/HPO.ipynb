{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d9ea24",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for Stance Classification (OPTUNA)\n",
    "\n",
    "The goal of this notebook is to optimize the macro F1-score of a stance classification model using hyperparameter optimization (HPO) techniques with Optuna.\n",
    "We fine-tune our winner model (Best Performance) on a balanced dataset (Previously Augmented) of tweets labeled as support or oppose.\n",
    "\n",
    "We incorporate:\n",
    "- Bayesian Optimization (TPE Sampler) to efficiently explore the hyperparameter space.\n",
    "- Early Stopping to prevent overfitting by stopping training.\n",
    "- Pruning (Median Pruner) to terminate unpromising trials early and save GPU time.\n",
    "- Evaluation Metrics: Accuracy, Precision, Recall, and F1-score (macro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bf80464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data locally from: C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\n",
      "No GPU detecting, using CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2331174f810>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Colab or not\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = \"/content/drive/MyDrive/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data from Google Drive: {path}\")\n",
    "else:\n",
    "    path = \"C:/Users/diego/Desktop/Master Neuro/M2/Intership_NLP/multimodal-argmining\"\n",
    "    os.chdir(path)\n",
    "    print(f\"Loading data locally from: {path}\")\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU ready:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detecting, using CPU.\")\n",
    "\n",
    "#If we use the augmented dataset:\n",
    "augmented = True \n",
    "\n",
    "# Set Seed\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db110049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "if augmented:\n",
    "  train_path = f\"{path}/data/train_augmented_paraphrase.csv\"\n",
    "else:\n",
    "  train_path = f\"{path}/data/train.csv\"\n",
    "  \n",
    "dev_path   = f\"{path}/data/dev.csv\"\n",
    "test_path  = f\"{path}/data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_dev   = pd.read_csv(dev_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Map labels to ints\n",
    "label2id = {\"oppose\": 0, \"support\": 1}\n",
    "\n",
    "#Minimal Preprocessing\n",
    "def clean_tweet(text):\n",
    "\n",
    "    text = str(text)\n",
    "    #URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Replace symbols\n",
    "    # text = text.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
    "    # # Remove multiple spaces\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    # # Split the words inside Hashtag (#ILoveMexico -> # I Love Mexico)\n",
    "    # def separar_hashtag(match):\n",
    "    #     hashtag = match.group(1)\n",
    "    #     separado = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', hashtag)\n",
    "    #     return f\"# {separado}\"\n",
    "\n",
    "    # text = re.sub(r\"#(\\w+)\", separar_hashtag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(clean_tweet)\n",
    "    df[\"label\"] = df[\"stance\"].map(label2id)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"tweet_text\",\"label\"]])\n",
    "dataset_dev   = Dataset.from_pandas(df_dev[[\"tweet_text\",\"label\"]])\n",
    "dataset_test  = Dataset.from_pandas(df_test[[\"tweet_text\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0e7b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our Model\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN=105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46833962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2190/2190 [00:00<00:00, 4086.16 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3979.30 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 3283.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "\n",
    "# Tokenization Function for each model\n",
    "def tokenize_dataset(dataset, tokenizer, max_length=MAX_LEN):\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"tweet_text\"],padding=\"max_length\",truncation=True,max_length=max_length)\n",
    "\n",
    "    #Tokenization\n",
    "    tokenized = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "    #Dataset Format for PyTorch\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets with model tokenizer\n",
    "train_dataset_tok = tokenize_dataset(dataset_train, tokenizer, MAX_LEN)\n",
    "dev_dataset_tok = tokenize_dataset(dataset_dev, tokenizer, MAX_LEN)\n",
    "test_dataset_tok = tokenize_dataset(dataset_test, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8326537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8446d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    precision = precision_score(labels, preds, average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc,\"f1\": f1_macro,\"precision\": precision,\"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79afa465",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization - OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6334f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function for Hyperparameter Optimization (OPTUNA)\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 3e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 8)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "\n",
    "    # New Model for each trial\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=dropout,\n",
    "        attention_probs_dropout_prob=dropout).to(device)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./optuna-trial-{trial.number}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    # Trainer with Early Stopping\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_tok,\n",
    "        eval_dataset=dev_dataset_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # Training loop with pruning (More efficient)\n",
    "    for epoch in range(num_train_epochs):\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate(dev_dataset_tok)\n",
    "        f1_macro = eval_results[\"eval_f1\"]\n",
    "\n",
    "        # We Report to Optuna\n",
    "        trial.report(f1_macro, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Final evaluation (F1 score)\n",
    "    final_metrics = trainer.evaluate(dev_dataset_tok)\n",
    "    return final_metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e993d",
   "metadata": {},
   "source": [
    "We use **TPESampler** because it efficiently explores the hyperparameter space by learning from previous trials. Unlike random search, it focuses on promising regions, increasing the chances of finding optimal hyperparameters faster and with fewer trials. It is especially useful for mixed spaces with continuous, integer, and categorical parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 12:12:00,423] A new study created in memory with name: no-name-2dfbc722-fa04-4e8c-963b-ca3acf3c1996\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/822 06:57 < 2:05:38, 0.10 it/s, Epoch 0.16/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-28 12:19:16,843] Trial 0 failed with parameters: {'learning_rate': 3.574712922600241e-06, 'weight_decay': 0.09507143064099162, 'batch_size': 8, 'num_train_epochs': 3, 'warmup_ratio': 0.011616722433639893, 'dropout': 0.2598528437324805} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_14888\\3866794121.py\", line 50, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\", line 2325, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\", line 2674, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\", line 4020, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\trainer.py\", line 4110, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 1188, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 862, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 606, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\utils\\deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 543, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\pytorch_utils.py\", line 257, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 552, in feed_forward_chunk\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 479, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\diego\\anaconda3\\envs\\multimodal\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-28 12:19:16,867] Trial 0 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "# We create and run the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            sampler=TPESampler(seed=seed),\n",
    "                            pruner=MedianPruner(n_warmup_steps=2))\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=20, timeout=None) \n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best F1:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58874888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the best model with optimal hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "print(\"\\n Retraining best model with optimal hyperparameters...\")\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=best_params[\"dropout\"],\n",
    "    attention_probs_dropout_prob=best_params[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_best\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_params[\"batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_ratio=best_params[\"warmup_ratio\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=dev_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "eval_results = final_trainer.evaluate(test_dataset_tok)\n",
    "print(\"\\n Final evaluation on test set:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Output\n",
    "os.makedirs(f\"{path}/experiments/text/HPO/\", exist_ok=True)\n",
    "output_dir = f\"{path}/experiments/text/HPO/\"\n",
    "\n",
    "# Confusion Matrix\n",
    "preds_output = final_trainer.predict(test_dataset_tok)\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = np.argmax(preds_output.predictions, axis=-1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"oppose\", \"support\"])\n",
    "\n",
    "# Export\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax)\n",
    "plt.title(f\"Confusion Matrix_Optimized RoBERTa,f1_Score={eval_results['eval_f1']:.4f}\")\n",
    "save_path = os.path.join(output_dir,f\"confusion_matrix_Model_Optimized.jpg\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics to CSV\n",
    "metrics_dict = eval_results\n",
    "metrics_dict[\"best_trial_f1_dev\"] = study.best_trial.value \n",
    "results_df = pd.DataFrame([metrics_dict])\n",
    "results_df.to_csv(output_dir + \"evaluation_results.csv\", index=False)\n",
    "print(f\"Metrics saved to {output_dir}/evaluation_results.csv\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6739df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best hyperparameters to JSON\n",
    "best_hyperparams = study.best_trial.params\n",
    "with open(f\"{output_dir}/best_hyperparameters.json\", \"w\") as f:\n",
    "    json.dump(best_hyperparams, f, indent=4)\n",
    "print(f\"Best hyperparameters saved to {output_dir}/best_hyperparameters.json\")\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
